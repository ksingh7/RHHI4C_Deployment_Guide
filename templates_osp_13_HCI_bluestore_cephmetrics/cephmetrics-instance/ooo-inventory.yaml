undercloud:
  hosts:
    localhost: {}
  vars:
    ansible_connection: local
    auth_url: http://192.168.120.1:5000/
    cacert: null
    os_auth_token: gAAAAABbn6EYMllYlm_vbVVbxtCi1JBh86WcSuh3gG8hn5pTFVS_ozipO8DeB2vNaUmxSFV-xIi-5WzpJbHNYVohCG9gx49bWKPJTUvYQsZ2MnSrx2AbB5Y_jOiaXksrJFV0SQn40B3w9jL-YalJU8dWAcFpE8VdrMFgPrjM64cGumB8d9JUDyg
    overcloud_admin_password: NGHXuH9pZCUkHCtFXnFeUvkUK
    overcloud_horizon_url: http://172.21.1.153:80/dashboard
    overcloud_keystone_url: http://172.21.1.153:5000/
    plan: overcloud
    project_name: admin
    undercloud_service_list: [openstack-nova-compute, openstack-heat-engine, openstack-ironic-conductor,
      openstack-swift-container, openstack-swift-object, openstack-mistral-engine]
    undercloud_swift_url: http://192.168.120.1:8080/v1/AUTH_d4ae8726a690413cbc62ade7e5e70763
    username: admin
controller-0:
  hosts:
    192.168.120.11: {}
  vars:
    cephmetrics_ip: 172.17.5.201
    ctlplane_ip: 192.168.120.11
    deploy_server_id: 641e1af0-6cc5-4c1b-9a3b-230233242e8b
    enabled_networks: [cephmetrics, management, storage, ctlplane, external, internal_api,
      storage_mgmt, tenant]
    external_ip: 172.21.0.160
    internal_api_ip: 172.17.1.201
    management_ip: 192.168.120.11
    storage_ip: 172.17.3.201
    storage_mgmt_ip: 192.168.120.11
    tenant_ip: 172.17.2.201
Controller:
  children:
    controller-0: {}
  vars:
    ansible_ssh_user: heat-admin
    bootstrap_server_id: 641e1af0-6cc5-4c1b-9a3b-230233242e8b
    role_data_cellv2_discovery: false
    role_data_config_settings: {}
    role_data_deploy_steps_tasks: []
    role_data_docker_config:
      step_1:
        cinder_volume_image_tag:
          command: [/bin/bash, -c, '/usr/bin/docker tag ''192.168.120.1:8787/rhosp13/openstack-cinder-volume:latest''
              ''192.168.120.1:8787/rhosp13/openstack-cinder-volume:pcmklatest''']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-volume:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/dev/shm:/dev/shm:rw', '/etc/sysconfig/docker:/etc/sysconfig/docker:ro',
            '/usr/bin:/usr/bin:ro', '/var/run/docker.sock:/var/run/docker.sock:rw']
        haproxy_image_tag:
          command: [/bin/bash, -c, '/usr/bin/docker tag ''192.168.120.1:8787/rhosp13/openstack-haproxy:latest''
              ''192.168.120.1:8787/rhosp13/openstack-haproxy:pcmklatest''']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-haproxy:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/dev/shm:/dev/shm:rw', '/etc/sysconfig/docker:/etc/sysconfig/docker:ro',
            '/usr/bin:/usr/bin:ro', '/var/run/docker.sock:/var/run/docker.sock:rw']
        memcached:
          command: [/bin/bash, -c, 'source /etc/sysconfig/memcached; /usr/bin/memcached
              -p ${PORT} -u ${USER} -m ${CACHESIZE} -c ${MAXCONN} $OPTIONS']
          image: 192.168.120.1:8787/rhosp13/openstack-memcached:latest
          net: host
          privileged: false
          restart: always
          start_order: 0
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/config-data/memcached/etc/sysconfig/memcached:/etc/sysconfig/memcached:ro']
        mysql_bootstrap:
          command: [bash, -ec, 'if [ -e /var/lib/mysql/mysql ]; then exit 0; fi

              echo -e "\n[mysqld]\nwsrep_provider=none" >> /etc/my.cnf

              kolla_set_configs

              sudo -u mysql -E kolla_extend_start

              mysqld_safe --skip-networking --wsrep-on=OFF &

              timeout ${DB_MAX_TIMEOUT} /bin/bash -c ''until mysqladmin -uroot -p"${DB_ROOT_PASSWORD}"
              ping 2>/dev/null; do sleep 1; done''

              mysql -uroot -p"${DB_ROOT_PASSWORD}" -e "CREATE USER ''clustercheck''@''localhost''
              IDENTIFIED BY ''${DB_CLUSTERCHECK_PASSWORD}'';"

              mysql -uroot -p"${DB_ROOT_PASSWORD}" -e "GRANT PROCESS ON *.* TO ''clustercheck''@''localhost''
              WITH GRANT OPTION;"

              timeout ${DB_MAX_TIMEOUT} mysqladmin -uroot -p"${DB_ROOT_PASSWORD}"
              shutdown']
          detach: false
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS, KOLLA_BOOTSTRAP=True, DB_MAX_TIMEOUT=60,
            DB_CLUSTERCHECK_PASSWORD=aGbm7XRUcTppYyaC37tr9Nzfu, DB_ROOT_PASSWORD=2FVVmdEhs4]
          image: 192.168.120.1:8787/rhosp13/openstack-mariadb:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/mysql.json:/var/lib/kolla/config_files/config.json',
            '/var/lib/config-data/puppet-generated/mysql/:/var/lib/kolla/config_files/src:ro',
            '/var/lib/mysql:/var/lib/mysql']
        mysql_data_ownership:
          command: [chown, -R, 'mysql:', /var/lib/mysql]
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-mariadb:latest
          net: host
          start_order: 0
          user: root
          volumes: ['/var/lib/mysql:/var/lib/mysql']
        mysql_image_tag:
          command: [/bin/bash, -c, '/usr/bin/docker tag ''192.168.120.1:8787/rhosp13/openstack-mariadb:latest''
              ''192.168.120.1:8787/rhosp13/openstack-mariadb:pcmklatest''']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-mariadb:latest
          net: host
          start_order: 2
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/dev/shm:/dev/shm:rw', '/etc/sysconfig/docker:/etc/sysconfig/docker:ro',
            '/usr/bin:/usr/bin:ro', '/var/run/docker.sock:/var/run/docker.sock:rw']
        rabbitmq_bootstrap:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS, KOLLA_BOOTSTRAP=True, RABBITMQ_CLUSTER_COOKIE=2adrHn7hC74ResuP6kf8]
          image: 192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest
          net: host
          privileged: false
          start_order: 0
          volumes: ['/var/lib/kolla/config_files/rabbitmq.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/rabbitmq/:/var/lib/kolla/config_files/src:ro',
            '/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro', '/var/lib/rabbitmq:/var/lib/rabbitmq']
        rabbitmq_image_tag:
          command: [/bin/bash, -c, '/usr/bin/docker tag ''192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest''
              ''192.168.120.1:8787/rhosp13/openstack-rabbitmq:pcmklatest''']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/dev/shm:/dev/shm:rw', '/etc/sysconfig/docker:/etc/sysconfig/docker:ro',
            '/usr/bin:/usr/bin:ro', '/var/run/docker.sock:/var/run/docker.sock:rw']
        redis_image_tag:
          command: [/bin/bash, -c, '/usr/bin/docker tag ''192.168.120.1:8787/rhosp13/openstack-redis:latest''
              ''192.168.120.1:8787/rhosp13/openstack-redis:pcmklatest''']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-redis:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/dev/shm:/dev/shm:rw', '/etc/sysconfig/docker:/etc/sysconfig/docker:ro',
            '/usr/bin:/usr/bin:ro', '/var/run/docker.sock:/var/run/docker.sock:rw']
      step_2:
        cinder_api_init_logs:
          command: [/bin/bash, -c, 'chown -R cinder:cinder /var/log/cinder']
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-api:latest
          privileged: false
          user: root
          volumes: ['/var/log/containers/cinder:/var/log/cinder', '/var/log/containers/httpd/cinder-api:/var/log/httpd']
        cinder_scheduler_init_logs:
          command: [/bin/bash, -c, 'chown -R cinder:cinder /var/log/cinder']
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-scheduler:latest
          privileged: false
          user: root
          volumes: ['/var/log/containers/cinder:/var/log/cinder']
        clustercheck:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-mariadb:latest
          net: host
          restart: always
          start_order: 1
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/clustercheck.json:/var/lib/kolla/config_files/config.json',
            '/var/lib/config-data/puppet-generated/clustercheck/:/var/lib/kolla/config_files/src:ro',
            '/var/lib/mysql:/var/lib/mysql']
        create_dnsmasq_wrapper:
          command: [/docker_puppet_apply.sh, '4', file, 'include ::tripleo::profile::base::neutron::dhcp_agent_wrappers']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-dhcp-agent:latest
          net: host
          pid: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/var/lib/docker-config-scripts/docker_puppet_apply.sh:/docker_puppet_apply.sh:ro',
            '/etc/puppet:/tmp/puppet-etc:ro', '/usr/share/openstack-puppet/modules:/usr/share/openstack-puppet/modules:ro',
            '/run/openvswitch:/run/openvswitch', '/var/lib/neutron:/var/lib/neutron']
        create_keepalived_wrapper:
          command: [/docker_puppet_apply.sh, '4', file, 'include ::tripleo::profile::base::neutron::l3_agent_wrappers']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-l3-agent:latest
          net: host
          pid: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/var/lib/docker-config-scripts/docker_puppet_apply.sh:/docker_puppet_apply.sh:ro',
            '/etc/puppet:/tmp/puppet-etc:ro', '/usr/share/openstack-puppet/modules:/usr/share/openstack-puppet/modules:ro',
            '/run/openvswitch:/run/openvswitch', '/var/lib/neutron:/var/lib/neutron']
        glance_init_logs:
          command: [/bin/bash, -c, 'chown -R glance:glance /var/log/glance']
          image: 192.168.120.1:8787/rhosp13/openstack-glance-api:latest
          privileged: false
          user: root
          volumes: ['/var/log/containers/glance:/var/log/glance']
        haproxy_init_bundle:
          command: [/docker_puppet_apply.sh, '2', 'file,file_line,concat,augeas,tripleo::firewall::rule,pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ip,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation',
            'include ::tripleo::profile::base::pacemaker; include ::tripleo::profile::pacemaker::haproxy_bundle',
            '']
          detach: false
          environment: [TRIPLEO_DEPLOY_IDENTIFIER=1537143398]
          image: 192.168.120.1:8787/rhosp13/openstack-haproxy:latest
          net: host
          privileged: true
          start_order: 3
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/var/lib/docker-config-scripts/docker_puppet_apply.sh:/docker_puppet_apply.sh:ro',
            '/etc/puppet:/tmp/puppet-etc:ro', '/usr/share/openstack-puppet/modules:/usr/share/openstack-puppet/modules:ro',
            '/etc/ipa/ca.crt:/etc/ipa/ca.crt:ro', '/etc/pki/tls/private/haproxy:/etc/pki/tls/private/haproxy:ro',
            '/etc/pki/tls/certs/haproxy:/etc/pki/tls/certs/haproxy:ro', '/etc/pki/tls/private/overcloud_endpoint.pem:/etc/pki/tls/private/overcloud_endpoint.pem:ro',
            '/etc/sysconfig:/etc/sysconfig:rw', '/usr/libexec/iptables:/usr/libexec/iptables:ro',
            '/usr/libexec/initscripts/legacy-actions:/usr/libexec/initscripts/legacy-actions:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw']
        haproxy_restart_bundle:
          command: [/usr/bin/bootstrap_host_exec, haproxy, if /usr/sbin/pcs resource
              show haproxy-bundle; then /usr/sbin/pcs resource restart --wait=600
              haproxy-bundle; echo "haproxy-bundle restart invoked"; fi]
          config_volume: haproxy
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-haproxy:latest
          net: host
          start_order: 2
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw',
            '/var/lib/config-data/puppet-generated/haproxy/:/var/lib/kolla/config_files/src:ro']
        heat_init_log:
          command: [/bin/bash, -c, 'chown -R heat:heat /var/log/heat']
          image: 192.168.120.1:8787/rhosp13/openstack-heat-engine:latest
          user: root
          volumes: ['/var/log/containers/heat:/var/log/heat']
        horizon_fix_perms:
          command: [/bin/bash, -c, 'touch /var/log/horizon/horizon.log && chown -R
              apache:apache /var/log/horizon && chmod -R a+rx /etc/openstack-dashboard']
          image: 192.168.120.1:8787/rhosp13/openstack-horizon:latest
          user: root
          volumes: ['/var/log/containers/horizon:/var/log/horizon', '/var/log/containers/httpd/horizon:/var/log/httpd',
            '/var/lib/config-data/puppet-generated/horizon/etc/openstack-dashboard:/etc/openstack-dashboard']
        keystone_init_log:
          command: [/bin/bash, -c, 'chown -R keystone:keystone /var/log/keystone']
          image: 192.168.120.1:8787/rhosp13/openstack-keystone:latest
          start_order: 1
          user: root
          volumes: ['/var/log/containers/keystone:/var/log/keystone', '/var/log/containers/httpd/keystone:/var/log/httpd']
        mysql_init_bundle:
          command: [/docker_puppet_apply.sh, '2', 'file,file_line,concat,augeas,pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation,galera_ready,mysql_database,mysql_grant,mysql_user',
            'include ::tripleo::profile::base::pacemaker;include ::tripleo::profile::pacemaker::database::mysql_bundle',
            '']
          detach: false
          environment: [TRIPLEO_DEPLOY_IDENTIFIER=1537143398]
          image: 192.168.120.1:8787/rhosp13/openstack-mariadb:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/var/lib/docker-config-scripts/docker_puppet_apply.sh:/docker_puppet_apply.sh:ro',
            '/etc/puppet:/tmp/puppet-etc:ro', '/usr/share/openstack-puppet/modules:/usr/share/openstack-puppet/modules:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw',
            '/var/lib/mysql:/var/lib/mysql:rw']
        mysql_restart_bundle:
          command: [/usr/bin/bootstrap_host_exec, mysql, if /usr/sbin/pcs resource
              show galera-bundle; then /usr/sbin/pcs resource restart --wait=600 galera-bundle;
              echo "galera-bundle restart invoked"; fi]
          config_volume: mysql
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-mariadb:latest
          net: host
          start_order: 0
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw',
            '/var/lib/config-data/puppet-generated/mysql/:/var/lib/kolla/config_files/src:ro']
        neutron_init_logs:
          command: [/bin/bash, -c, 'chown -R neutron:neutron /var/log/neutron']
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-server:latest
          privileged: false
          user: root
          volumes: ['/var/log/containers/neutron:/var/log/neutron', '/var/log/containers/httpd/neutron-api:/var/log/httpd']
        nova_api_init_logs:
          command: [/bin/bash, -c, 'chown -R nova:nova /var/log/nova']
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          privileged: false
          user: root
          volumes: ['/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd']
        nova_metadata_init_log:
          command: [/bin/bash, -c, 'chown -R nova:nova /var/log/nova']
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          privileged: false
          user: root
          volumes: ['/var/log/containers/nova:/var/log/nova']
        nova_placement_init_log:
          command: [/bin/bash, -c, 'chown -R nova:nova /var/log/nova']
          image: 192.168.120.1:8787/rhosp13/openstack-nova-placement-api:latest
          start_order: 1
          user: root
          volumes: ['/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-placement:/var/log/httpd']
        rabbitmq_init_bundle:
          command: [/docker_puppet_apply.sh, '2', 'file,file_line,concat,augeas,pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation,rabbitmq_policy,rabbitmq_user,rabbitmq_ready',
            'include ::tripleo::profile::base::pacemaker;include ::tripleo::profile::pacemaker::rabbitmq_bundle',
            '']
          detach: false
          environment: [TRIPLEO_DEPLOY_IDENTIFIER=1537143398]
          image: 192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/var/lib/docker-config-scripts/docker_puppet_apply.sh:/docker_puppet_apply.sh:ro',
            '/etc/puppet:/tmp/puppet-etc:ro', '/usr/share/openstack-puppet/modules:/usr/share/openstack-puppet/modules:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw',
            '/bin/true:/bin/epmd']
        rabbitmq_restart_bundle:
          command: [/usr/bin/bootstrap_host_exec, rabbitmq, if /usr/sbin/pcs resource
              show rabbitmq-bundle; then /usr/sbin/pcs resource restart --wait=600
              rabbitmq-bundle; echo "rabbitmq-bundle restart invoked"; fi]
          config_volume: rabbitmq
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest
          net: host
          start_order: 0
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw',
            '/var/lib/config-data/puppet-generated/rabbitmq/:/var/lib/kolla/config_files/src:ro']
        redis_init_bundle:
          command: [/docker_puppet_apply.sh, '2', 'file,file_line,concat,augeas,pacemaker::resource::bundle,pacemaker::property,pacemaker::resource::ocf,pacemaker::constraint::order,pacemaker::constraint::colocation',
            'include ::tripleo::profile::base::pacemaker;include ::tripleo::profile::pacemaker::database::redis_bundle',
            '']
          config_volume: redis_init_bundle
          detach: false
          environment: [TRIPLEO_DEPLOY_IDENTIFIER=1537143398]
          image: 192.168.120.1:8787/rhosp13/openstack-redis:latest
          net: host
          start_order: 2
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/var/lib/docker-config-scripts/docker_puppet_apply.sh:/docker_puppet_apply.sh:ro',
            '/etc/puppet:/tmp/puppet-etc:ro', '/usr/share/openstack-puppet/modules:/usr/share/openstack-puppet/modules:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw']
        redis_restart_bundle:
          command: [/usr/bin/bootstrap_host_exec, redis, if /usr/sbin/pcs resource
              show redis-bundle; then /usr/sbin/pcs resource restart --wait=600 redis-bundle;
              echo "redis-bundle restart invoked"; fi]
          config_volume: redis
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-redis:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw',
            '/var/lib/config-data/puppet-generated/redis/:/var/lib/kolla/config_files/src:ro']
      step_3:
        cinder_api_db_sync:
          command: [/usr/bin/bootstrap_host_exec, cinder_api, su cinder -s /bin/bash
              -c 'cinder-manage db sync --bump-versions']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-api:latest
          net: host
          privileged: false
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/config-data/cinder/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/cinder/etc/cinder/:/etc/cinder/:ro', '/var/log/containers/cinder:/var/log/cinder',
            '/var/log/containers/httpd/cinder-api:/var/log/httpd']
        cinder_volume_init_logs:
          command: [/bin/bash, -c, 'chown -R cinder:cinder /var/log/cinder']
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-volume:latest
          privileged: false
          start_order: 0
          user: root
          volumes: ['/var/log/containers/cinder:/var/log/cinder']
        glance_api_db_sync:
          command: /usr/bin/bootstrap_host_exec glance_api su glance -s /bin/bash
            -c '/usr/local/bin/kolla_start'
          detach: false
          environment: [KOLLA_BOOTSTRAP=True, KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-glance-api:latest
          net: host
          privileged: false
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/glance:/var/log/glance', '/var/lib/kolla/config_files/glance_api.json:/var/lib/kolla/config_files/config.json',
            '/var/lib/config-data/puppet-generated/glance_api/:/var/lib/kolla/config_files/src:ro',
            '/etc/ceph:/var/lib/kolla/config_files/src-ceph:ro', '', '']
        heat_engine_db_sync:
          command: /usr/bin/bootstrap_host_exec heat_engine su heat -s /bin/bash -c
            'heat-manage db_sync'
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-heat-engine:latest
          net: host
          privileged: false
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/heat:/var/log/heat', '/var/lib/config-data/heat/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/heat/etc/heat/:/etc/heat/:ro']
        horizon:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS, ENABLE_IRONIC=yes, ENABLE_MANILA=yes,
            ENABLE_MISTRAL=yes, ENABLE_OCTAVIA=yes, ENABLE_SAHARA=yes, ENABLE_CLOUDKITTY=no,
            ENABLE_FREEZER=no, ENABLE_FWAAS=no, ENABLE_KARBOR=no, ENABLE_DESIGNATE=no,
            ENABLE_MAGNUM=no, ENABLE_MURANO=no, ENABLE_NEUTRON_LBAAS=no, ENABLE_SEARCHLIGHT=no,
            ENABLE_SENLIN=no, ENABLE_SOLUM=no, ENABLE_TACKER=no, ENABLE_TROVE=no,
            ENABLE_WATCHER=no, ENABLE_ZAQAR=no, ENABLE_ZUN=no]
          image: 192.168.120.1:8787/rhosp13/openstack-horizon:latest
          net: host
          privileged: false
          restart: always
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/horizon.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/horizon/:/var/lib/kolla/config_files/src:ro',
            '/var/log/containers/horizon:/var/log/horizon', '/var/log/containers/httpd/horizon:/var/log/httpd',
            '', '']
        iscsid:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-iscsid:latest
          net: host
          privileged: true
          restart: always
          start_order: 2
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/iscsid.json:/var/lib/kolla/config_files/config.json:ro',
            '/dev/:/dev/', '/run/:/run/', '/sys:/sys', '/lib/modules:/lib/modules:ro',
            '/etc/iscsi:/var/lib/kolla/config_files/src-iscsid:ro']
        keystone:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-keystone:latest
          net: host
          privileged: false
          restart: always
          start_order: 2
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/keystone:/var/log/keystone', '/var/log/containers/httpd/keystone:/var/log/httpd',
            '/var/lib/kolla/config_files/keystone.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/keystone/:/var/lib/kolla/config_files/src:ro',
            '', '']
        keystone_bootstrap:
          action: exec
          command: [keystone, /usr/bin/bootstrap_host_exec, keystone, keystone-manage,
            bootstrap, --bootstrap-password, NGHXuH9pZCUkHCtFXnFeUvkUK]
          start_order: 3
          user: root
        keystone_cron:
          command: [/bin/bash, -c, /usr/local/bin/kolla_set_configs && /usr/sbin/crond
              -n]
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-keystone:latest
          net: host
          privileged: false
          restart: always
          start_order: 4
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/keystone:/var/log/keystone', '/var/log/containers/httpd/keystone:/var/log/httpd',
            '/var/lib/kolla/config_files/keystone_cron.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/keystone/:/var/lib/kolla/config_files/src:ro']
        keystone_db_sync:
          command: [/usr/bin/bootstrap_host_exec, keystone, /usr/local/bin/kolla_start]
          detach: false
          environment: [KOLLA_BOOTSTRAP=True, KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-keystone:latest
          net: host
          privileged: false
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/keystone:/var/log/keystone', '/var/log/containers/httpd/keystone:/var/log/httpd',
            '/var/lib/kolla/config_files/keystone.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/keystone/:/var/lib/kolla/config_files/src:ro',
            '', '']
        neutron_db_sync:
          command: [/usr/bin/bootstrap_host_exec, neutron_api, neutron-db-manage,
            upgrade, heads]
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-server:latest
          net: host
          privileged: false
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/neutron:/var/log/neutron', '/var/log/containers/httpd/neutron-api:/var/log/httpd',
            '/var/lib/config-data/neutron/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/neutron/etc/neutron:/etc/neutron:ro', '/var/lib/config-data/neutron/usr/share/neutron:/usr/share/neutron:ro']
        neutron_ovs_bridge:
          command: [puppet, apply, --modulepath, '/etc/puppet/modules:/usr/share/openstack-puppet/modules',
            --tags, 'file,file_line,concat,augeas,neutron::plugins::ovs::bridge,vs_config',
            -v, -e, 'include neutron::agents::ml2::ovs']
          detach: false
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-server:latest
          net: host
          pid: host
          privileged: true
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/neutron_ovs_agent.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro',
            '/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch', '/etc/puppet:/etc/puppet:ro',
            '/usr/share/openstack-puppet/modules/:/usr/share/openstack-puppet/modules/:ro',
            '/var/run/openvswitch/:/var/run/openvswitch/']
        nova_api_db_sync:
          command: /usr/bin/bootstrap_host_exec nova_api su nova -s /bin/bash -c '/usr/bin/nova-manage
            api_db sync'
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          start_order: 0
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd',
            '/var/lib/config-data/nova/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/nova/etc/nova/:/etc/nova/:ro']
        nova_api_ensure_default_cell:
          command: /usr/bin/bootstrap_host_exec nova_api /nova_api_ensure_default_cell.sh
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          start_order: 2
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd',
            '/var/lib/config-data/nova/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/nova/etc/nova/:/etc/nova/:ro', '/var/lib/config-data/nova/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/nova/etc/nova/:/etc/nova/:ro', '/var/log/containers/nova:/var/log/nova',
            '/var/lib/docker-config-scripts/nova_api_ensure_default_cell.sh:/nova_api_ensure_default_cell.sh:ro']
        nova_api_map_cell0:
          command: /usr/bin/bootstrap_host_exec nova_api su nova -s /bin/bash -c '/usr/bin/nova-manage
            cell_v2 map_cell0'
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd',
            '/var/lib/config-data/nova/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/nova/etc/nova/:/etc/nova/:ro']
        nova_db_sync:
          command: /usr/bin/bootstrap_host_exec nova_api su nova -s /bin/bash -c '/usr/bin/nova-manage
            db sync'
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          start_order: 3
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd',
            '/var/lib/config-data/nova/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/nova/etc/nova/:/etc/nova/:ro']
        nova_placement:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-nova-placement-api:latest
          net: host
          restart: always
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-placement:/var/log/httpd',
            '/var/lib/kolla/config_files/nova_placement.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova_placement/:/var/lib/kolla/config_files/src:ro',
            '', '']
        swift_copy_rings:
          command: [/bin/bash, -c, cp -v -a -t /etc/swift /swift_ringbuilder/etc/swift/*.gz
              /swift_ringbuilder/etc/swift/*.builder /swift_ringbuilder/etc/swift/backups]
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-swift-proxy-server:latest
          user: root
          volumes: ['/var/lib/config-data/puppet-generated/swift/etc/swift:/etc/swift:rw',
            '/var/lib/config-data/swift_ringbuilder:/swift_ringbuilder:ro']
        swift_setup_srv:
          command: [chown, -R, 'swift:', /srv/node]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-account:latest
          user: root
          volumes: ['/srv/node:/srv/node']
      step_4:
        cinder_api:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-api:latest
          net: host
          privileged: false
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/cinder_api.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/cinder/:/var/lib/kolla/config_files/src:ro',
            '/var/log/containers/cinder:/var/log/cinder', '/var/log/containers/httpd/cinder-api:/var/log/httpd',
            '', '']
        cinder_api_cron:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-api:latest
          net: host
          privileged: false
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/cinder_api_cron.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/cinder/:/var/lib/kolla/config_files/src:ro',
            '/var/log/containers/cinder:/var/log/cinder', '/var/log/containers/httpd/cinder-api:/var/log/httpd']
        cinder_scheduler:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-scheduler:latest
          net: host
          privileged: false
          restart: always
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/cinder_scheduler.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/cinder/:/var/lib/kolla/config_files/src:ro',
            '/var/log/containers/cinder:/var/log/cinder']
        glance_api:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-glance-api:latest
          net: host
          privileged: false
          restart: always
          start_order: 2
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/glance:/var/log/glance', '/var/lib/kolla/config_files/glance_api.json:/var/lib/kolla/config_files/config.json',
            '/var/lib/config-data/puppet-generated/glance_api/:/var/lib/kolla/config_files/src:ro',
            '/etc/ceph:/var/lib/kolla/config_files/src-ceph:ro', '', '']
        heat_api:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-heat-api:latest
          net: host
          privileged: false
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/heat:/var/log/heat', '/var/log/containers/httpd/heat-api:/var/log/httpd',
            '/var/lib/kolla/config_files/heat_api.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/heat_api/:/var/lib/kolla/config_files/src:ro',
            '', '']
        heat_api_cfn:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-heat-api-cfn:latest
          net: host
          privileged: false
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/heat:/var/log/heat', '/var/log/containers/httpd/heat-api-cfn:/var/log/httpd',
            '/var/lib/kolla/config_files/heat_api_cfn.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/heat_api_cfn/:/var/lib/kolla/config_files/src:ro',
            '', '']
        heat_api_cron:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-heat-api:latest
          net: host
          privileged: false
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/heat:/var/log/heat', '/var/log/containers/httpd/heat-api:/var/log/httpd',
            '/var/lib/kolla/config_files/heat_api_cron.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/heat_api/:/var/lib/kolla/config_files/src:ro']
        heat_engine:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-heat-engine:latest
          net: host
          privileged: false
          restart: always
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/heat:/var/log/heat', '/var/lib/kolla/config_files/heat_engine.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/heat/:/var/lib/kolla/config_files/src:ro']
        keystone_refresh:
          action: exec
          command: [keystone, pkill, --signal, USR1, httpd]
          start_order: 1
          user: root
        logrotate_crond:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-cron:latest
          net: none
          pid: host
          privileged: true
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/logrotate-crond.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/crond/:/var/lib/kolla/config_files/src:ro',
            '/var/log/containers:/var/log/containers']
        neutron_api:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-server:latest
          net: host
          privileged: false
          restart: always
          start_order: 0
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/neutron:/var/log/neutron', '/var/log/containers/httpd/neutron-api:/var/log/httpd',
            '/var/lib/kolla/config_files/neutron_api.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro']
        neutron_dhcp:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-dhcp-agent:latest
          net: host
          pid: host
          privileged: true
          restart: always
          start_order: 10
          ulimit: [nofile=1024]
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/neutron:/var/log/neutron', '/var/lib/kolla/config_files/neutron_dhcp.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro',
            '/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch', '/var/lib/neutron:/var/lib/neutron',
            '/run/netns:/run/netns:shared', '/var/lib/openstack:/var/lib/openstack',
            '/var/lib/neutron/dnsmasq_wrapper:/usr/local/bin/dnsmasq:ro', '/var/lib/neutron/dhcp_haproxy_wrapper:/usr/local/bin/haproxy:ro']
        neutron_l3_agent:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-l3-agent:latest
          net: host
          pid: host
          privileged: true
          restart: always
          start_order: 10
          ulimit: [nofile=1024]
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/neutron:/var/log/neutron', '/var/lib/kolla/config_files/neutron_l3_agent.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro',
            '/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch', '/var/lib/neutron:/var/lib/neutron',
            '/run/netns:/run/netns:shared', '/var/lib/openstack:/var/lib/openstack',
            '/var/lib/neutron/keepalived_wrapper:/usr/local/bin/keepalived:ro', '/var/lib/neutron/l3_haproxy_wrapper:/usr/local/bin/haproxy:ro',
            '/var/lib/neutron/dibbler_wrapper:/usr/local/bin/dibbler_client:ro']
        neutron_metadata_agent:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-metadata-agent:latest
          net: host
          pid: host
          privileged: true
          restart: always
          start_order: 10
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/neutron:/var/log/neutron', '/var/lib/kolla/config_files/neutron_metadata_agent.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro',
            '/lib/modules:/lib/modules:ro', '/var/lib/neutron:/var/lib/neutron']
        neutron_ovs_agent:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-openvswitch-agent:latest
          net: host
          pid: host
          privileged: true
          restart: always
          start_order: 10
          ulimit: [nofile=1024]
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/neutron:/var/log/neutron', '/var/lib/kolla/config_files/neutron_ovs_agent.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro',
            '/var/lib/docker-config-scripts/neutron_ovs_agent_launcher.sh:/neutron_ovs_agent_launcher.sh:ro',
            '/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch']
        nova_api:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          privileged: true
          restart: always
          start_order: 2
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd',
            '/var/lib/kolla/config_files/nova_api.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova/:/var/lib/kolla/config_files/src:ro',
            '', '']
        nova_api_cron:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          privileged: false
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd',
            '/var/lib/kolla/config_files/nova_api_cron.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova/:/var/lib/kolla/config_files/src:ro']
        nova_conductor:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-nova-conductor:latest
          net: host
          privileged: false
          restart: always
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/lib/kolla/config_files/nova_conductor.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova/:/var/lib/kolla/config_files/src:ro']
        nova_consoleauth:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-nova-consoleauth:latest
          net: host
          privileged: false
          restart: always
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/lib/kolla/config_files/nova_consoleauth.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova/:/var/lib/kolla/config_files/src:ro']
        nova_metadata:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          privileged: true
          restart: always
          start_order: 2
          user: nova
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/lib/kolla/config_files/nova_metadata.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova/:/var/lib/kolla/config_files/src:ro']
        nova_scheduler:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-nova-scheduler:latest
          net: host
          privileged: false
          restart: always
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/lib/kolla/config_files/nova_scheduler.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova/:/var/lib/kolla/config_files/src:ro',
            '/run:/run']
        nova_vnc_proxy:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-nova-novncproxy:latest
          net: host
          privileged: false
          restart: always
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/lib/kolla/config_files/nova_vnc_proxy.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova/:/var/lib/kolla/config_files/src:ro']
        swift_account_auditor:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-account:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_account_auditor.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_account_reaper:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-account:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_account_reaper.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_account_replicator:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-account:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_account_replicator.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_account_server:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-swift-account:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_account_server.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_container_auditor:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-container:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_container_auditor.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_container_replicator:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-container:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_container_replicator.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_container_server:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-swift-container:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_container_server.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_container_updater:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-container:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_container_updater.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_object_auditor:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-object:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_object_auditor.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_object_expirer:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-proxy-server:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_object_expirer.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_object_replicator:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-object:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_object_replicator.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_object_server:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-swift-object:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_object_server.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_object_updater:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-object:latest
          net: host
          restart: always
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_object_updater.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev', '/var/cache/swift:/var/cache/swift']
        swift_proxy:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-swift-proxy-server:latest
          net: host
          restart: always
          start_order: 2
          user: swift
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_proxy.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/run:/run', '/srv/node:/srv/node', '/dev:/dev']
        swift_rsync:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-swift-object:latest
          net: host
          privileged: true
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/swift_rsync.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/swift/:/var/lib/kolla/config_files/src:ro',
            '/srv/node:/srv/node', '/dev:/dev']
      step_5:
        cinder_volume_init_bundle:
          command: [/docker_puppet_apply.sh, '5', 'file,file_line,concat,augeas,pacemaker::resource::bundle,pacemaker::property,pacemaker::constraint::location',
            'include ::tripleo::profile::base::pacemaker;include ::tripleo::profile::pacemaker::cinder::volume_bundle',
            '']
          detach: false
          environment: [TRIPLEO_DEPLOY_IDENTIFIER=1537143398]
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-volume:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/var/lib/docker-config-scripts/docker_puppet_apply.sh:/docker_puppet_apply.sh:ro',
            '/etc/puppet:/tmp/puppet-etc:ro', '/usr/share/openstack-puppet/modules:/usr/share/openstack-puppet/modules:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw']
        cinder_volume_restart_bundle:
          command: [/usr/bin/bootstrap_host_exec, cinder_volume, if /usr/sbin/pcs
              resource show openstack-cinder-volume; then /usr/sbin/pcs resource restart
              --wait=600 openstack-cinder-volume; echo "openstack-cinder-volume restart
              invoked"; fi]
          config_volume: cinder
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-cinder-volume:latest
          net: host
          start_order: 0
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/etc/corosync/corosync.conf:/etc/corosync/corosync.conf:ro', '/dev/shm:/dev/shm:rw',
            '/var/lib/config-data/puppet-generated/cinder/:/var/lib/kolla/config_files/src:ro']
        nova_api_discover_hosts:
          command: /usr/bin/bootstrap_host_exec nova_api /nova_api_discover_hosts.sh
          detach: false
          environment: [TRIPLEO_DEPLOY_IDENTIFIER=1537143398]
          image: 192.168.120.1:8787/rhosp13/openstack-nova-api:latest
          net: host
          start_order: 1
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/log/containers/httpd/nova-api:/var/log/httpd',
            '/var/lib/config-data/nova/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/nova/etc/nova/:/etc/nova/:ro', '/var/lib/config-data/nova/etc/my.cnf.d/tripleo.cnf:/etc/my.cnf.d/tripleo.cnf:ro',
            '/var/lib/config-data/nova/etc/nova/:/etc/nova/:ro', '/var/log/containers/nova:/var/log/nova',
            '/var/lib/docker-config-scripts/nova_api_discover_hosts.sh:/nova_api_discover_hosts.sh:ro']
    role_data_docker_config_scripts:
      create_swift_secret.sh: {content: "#!/bin/bash\nexport OS_PROJECT_DOMAIN_ID=$(crudini\
          \ --get /etc/swift/keymaster.conf kms_keymaster project_domain_id)\nexport\
          \ OS_USER_DOMAIN_ID=$(crudini --get /etc/swift/keymaster.conf kms_keymaster\
          \ user_domain_id)\nexport OS_PROJECT_NAME=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster project_name)\nexport OS_USERNAME=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster username)\nexport OS_PASSWORD=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster password)\nexport OS_AUTH_URL=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster auth_endpoint)\nexport OS_AUTH_TYPE=password\nexport OS_IDENTITY_API_VERSION=3\n\
          \necho \"Check if secret already exists\"\nsecret_href=$(openstack secret\
          \ list --name swift_root_secret_uuid)\nrc=$?\nif [[ $rc != 0 ]]; then\n\
          \  echo \"Failed to check secrets, check if Barbican in enabled and responding\
          \ properly\"\n  exit $rc;\nfi\nif [ -z \"$secret_href\" ]; then\n  echo\
          \ \"Create new secret\"\n  order_href=$(openstack secret order create --name\
          \ swift_root_secret_uuid --payload-content-type=\"application/octet-stream\"\
          \ --algorithm aes --bit-length 256 --mode ctr key -f value -c \"Order href\"\
          )\nfi\n", mode: '0700'}
      docker_puppet_apply.sh: {content: "#!/bin/bash\nset -eux\nSTEP=$1\nTAGS=$2\n\
          CONFIG=$3\nEXTRA_ARGS=${4:-''}\nif [ -d /tmp/puppet-etc ]; then\n  # ignore\
          \ copy failures as these may be the same file depending on docker mounts\n\
          \  cp -a /tmp/puppet-etc/* /etc/puppet || true\nfi\necho \"{\\\"step\\\"\
          : ${STEP}}\" > /etc/puppet/hieradata/docker.json\nexport FACTER_uuid=docker\n\
          set +e\npuppet apply $EXTRA_ARGS \\\n    --verbose \\\n    --detailed-exitcodes\
          \ \\\n    --summarize \\\n    --color=false \\\n    --modulepath /etc/puppet/modules:/opt/stack/puppet-modules:/usr/share/openstack-puppet/modules\
          \ \\\n    --tags $TAGS \\\n    -e \"${CONFIG}\"\nrc=$?\nset -e\nset +ux\n\
          if [ $rc -eq 2 -o $rc -eq 0 ]; then\n    exit 0\nfi\nexit $rc\n", mode: '0700'}
      neutron_ovs_agent_launcher.sh: {content: '#!/bin/bash

          set -xe

          /usr/bin/python -m neutron.cmd.destroy_patch_ports --config-file /usr/share/neutron/neutron-dist.conf
          --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/openvswitch_agent.ini
          --config-dir /etc/neutron/conf.d/common --config-dir /etc/neutron/conf.d/neutron-openvswitch-agent

          /usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf
          --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/openvswitch_agent.ini
          --config-file /etc/neutron/plugins/ml2/ml2_conf.ini --config-dir /etc/neutron/conf.d/common
          --log-file=/var/log/neutron/openvswitch-agent.log

          ', mode: '0755'}
      nova_api_discover_hosts.sh: {content: "#!/bin/bash\nexport OS_PROJECT_DOMAIN_NAME=$(crudini\
          \ --get /etc/nova/nova.conf keystone_authtoken project_domain_name)\nexport\
          \ OS_USER_DOMAIN_NAME=$(crudini --get /etc/nova/nova.conf keystone_authtoken\
          \ user_domain_name)\nexport OS_PROJECT_NAME=$(crudini --get /etc/nova/nova.conf\
          \ keystone_authtoken project_name)\nexport OS_USERNAME=$(crudini --get /etc/nova/nova.conf\
          \ keystone_authtoken username)\nexport OS_PASSWORD=$(crudini --get /etc/nova/nova.conf\
          \ keystone_authtoken password)\nexport OS_AUTH_URL=$(crudini --get /etc/nova/nova.conf\
          \ keystone_authtoken auth_url)\nexport OS_AUTH_TYPE=password\nexport OS_IDENTITY_API_VERSION=3\n\
          \necho \"(cellv2) Running cell_v2 host discovery\"\ntimeout=600\nloop_wait=30\n\
          declare -A discoverable_hosts\nfor host in $(hiera -c /etc/puppet/hiera.yaml\
          \ cellv2_discovery_hosts | sed -e '/^nil$/d' |  tr \",\" \" \"); do discoverable_hosts[$host]=1;\
          \ done\ntimeout_at=$(( $(date +\"%s\") + ${timeout} ))\necho \"(cellv2)\
          \ Waiting ${timeout} seconds for hosts to register\"\nfinished=0\nwhile\
          \ : ; do\n  for host in $(openstack -q compute service list -c 'Host' -c\
          \ 'Zone' -f value | awk '$2 != \"internal\" { print $1 }'); do\n    if ((\
          \ discoverable_hosts[$host] == 1 )); then\n      echo \"(cellv2) compute\
          \ node $host has registered\"\n      unset discoverable_hosts[$host]\n \
          \   fi\n  done\n  finished=1\n  for host in \"${!discoverable_hosts[@]}\"\
          ; do\n    if (( ${discoverable_hosts[$host]} == 1 )); then\n      echo \"\
          (cellv2) compute node $host has not registered\"\n      finished=0\n   \
          \ fi\n  done\n  remaining=$(( $timeout_at - $(date +\"%s\") ))\n  if ((\
          \ $finished == 1 )); then\n    echo \"(cellv2) All nodes registered\"\n\
          \    break\n  elif (( $remaining <= 0 )); then\n    echo \"(cellv2) WARNING:\
          \ timeout waiting for nodes to register, running host discovery regardless\"\
          \n    echo \"(cellv2) Expected host list:\" $(hiera -c /etc/puppet/hiera.yaml\
          \ cellv2_discovery_hosts | sed -e '/^nil$/d' | sort -u |  tr ',' ' ')\n\
          \    echo \"(cellv2) Detected host list:\" $(openstack -q compute service\
          \ list -c 'Host' -c 'Zone' -f value | awk '$2 != \"internal\" { print $1\
          \ }' | sort -u | tr '\\n', ' ')\n    break\n  else\n    echo \"(cellv2)\
          \ Waiting ${remaining} seconds for hosts to register\"\n    sleep $loop_wait\n\
          \  fi\ndone\necho \"(cellv2) Running host discovery...\"\nsu nova -s /bin/bash\
          \ -c \"/usr/bin/nova-manage cell_v2 discover_hosts --by-service --verbose\"\
          \n", mode: '0700'}
      nova_api_ensure_default_cell.sh: {content: "#!/bin/bash\nDEFID=$(nova-manage\
          \ cell_v2 list_cells | sed -e '1,3d' -e '$d' | awk -F ' *| *' '$2 == \"\
          default\" {print $4}')\nif [ \"$DEFID\" ]; then\n  echo \"(cellv2) Updating\
          \ default cell_v2 cell $DEFID\"\n  su nova -s /bin/bash -c \"/usr/bin/nova-manage\
          \ cell_v2 update_cell --cell_uuid $DEFID --name=default\"\nelse\n  echo\
          \ \"(cellv2) Creating default cell_v2 cell\"\n  su nova -s /bin/bash -c\
          \ \"/usr/bin/nova-manage cell_v2 create_cell --name=default\"\nfi\n", mode: '0700'}
      set_swift_keymaster_key_id.sh: {content: "#!/bin/bash\nexport OS_PROJECT_DOMAIN_ID=$(crudini\
          \ --get /etc/swift/keymaster.conf kms_keymaster project_domain_id)\nexport\
          \ OS_USER_DOMAIN_ID=$(crudini --get /etc/swift/keymaster.conf kms_keymaster\
          \ user_domain_id)\nexport OS_PROJECT_NAME=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster project_name)\nexport OS_USERNAME=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster username)\nexport OS_PASSWORD=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster password)\nexport OS_AUTH_URL=$(crudini --get /etc/swift/keymaster.conf\
          \ kms_keymaster auth_endpoint)\nexport OS_AUTH_TYPE=password\nexport OS_IDENTITY_API_VERSION=3\n\
          echo \"retrieve key_id\"\nloop_wait=2\nfor i in {0..5}; do\n  #TODO update\
          \ uuid from mistral here too\n  secret_href=$(openstack secret list --name\
          \ swift_root_secret_uuid)\n  if [ \"$secret_href\" ]; then\n    echo \"\
          set key_id in keymaster.conf\"\n    secret_href=$(openstack secret list\
          \ --name swift_root_secret_uuid -f value -c \"Secret href\")\n    crudini\
          \ --set /etc/swift/keymaster.conf kms_keymaster key_id ${secret_href##*/}\n\
          \    exit 0\n  else\n    echo \"no key, wait for $loop_wait and check again\"\
          \n    sleep $loop_wait\n    ((loop_wait++))\n  fi\ndone\necho \"Failed to\
          \ set secret in keymaster.conf, check if Barbican is enabled and responding\
          \ properly\"\nexit 1\n", mode: '0700'}
    role_data_docker_puppet_tasks:
      step_3:
      - {config_image: '192.168.120.1:8787/rhosp13/openstack-keystone:latest', config_volume: keystone_init_tasks,
        puppet_tags: 'keystone_config,keystone_domain_config,keystone_endpoint,keystone_identity_provider,keystone_paste_ini,keystone_role,keystone_service,keystone_tenant,keystone_user,keystone_user_role,keystone_domain',
        step_config: 'include ::tripleo::profile::base::keystone'}
    role_data_external_deploy_tasks: []
    role_data_external_post_deploy_tasks: []
    role_data_fast_forward_post_upgrade_tasks:
    - name: Register repo type and args
      set_fact:
        fast_forward_repo_args:
          tripleo_repos: {ocata: -b ocata current, pike: -b pike current, queens: -b
              queens current}
        fast_forward_repo_type: custom-script
    - debug: {msg: 'fast_forward_repo_type: {{ fast_forward_repo_type }} fast_forward_repo_args:
          {{ fast_forward_repo_args }}'}
    - block:
      - git: {dest: /home/stack/tripleo-repos/, repo: 'https://github.com/openstack/tripleo-repos.git'}
        name: clone tripleo-repos
      - args: {chdir: /home/stack/tripleo-repos/}
        command: python setup.py install
        name: install tripleo-repos
      - {command: 'tripleo-repos {{ fast_forward_repo_args.tripleo_repos[release]
          }}', name: Enable tripleo-repos}
      when: [ffu_packages_apply|bool, fast_forward_repo_type == 'tripleo-repos']
    - block:
      - copy: {content: "#!/bin/bash\nset -e\necho \"If you use FastForwardRepoType\
            \ 'custom-script' you have to provide the upgrade repo script content.\"\
            \necho \"It will be installed as /root/ffu_upgrade_repo.sh on the node\"\
            \necho \"and passed the upstream name (ocata, pike, queens) of the release\
            \ as first argument\"\ncase $1 in\n  ocata)\n    subscription-manager\
            \ repos --disable=rhel-7-server-openstack-10-rpms\n    subscription-manager\
            \ repos --enable=rhel-7-server-openstack-11-rpms\n    ;;\n  pike)\n  \
            \  subscription-manager repos --disable=rhel-7-server-openstack-11-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-12-rpms\n\
            \    ;;\n  queens)\n    subscription-manager repos --disable=rhel-7-server-openstack-12-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-13-rpms\n\
            \    ;;\n  *)\n    echo \"unknown release $1\" >&2\n    exit 1\nesac\n",
          dest: /root/ffu_update_repo.sh, mode: 448}
        name: Create custom Script for upgrading repo.
      - {name: Execute custom script for upgrading repo., shell: '/root/ffu_update_repo.sh
          {{release}}'}
      when: [ffu_packages_apply|bool, fast_forward_repo_type == 'custom-script']
    role_data_fast_forward_upgrade_tasks:
    - file: path=/etc/httpd/conf.d/10-ceilometer_wsgi.conf state=absent
      name: Purge Ceilometer apache config files
      when: [step|int == 1, release == 'ocata']
    - lineinfile: dest=/etc/httpd/conf/ports.conf state=absent regexp="8777$"
      name: Clean up ceilometer port from ports.conf
      when: [step|int == 1, release == 'ocata']
    - command: systemctl is-enabled --quiet openstack-ceilometer-collector
      ignore_errors: true
      name: FFU check if openstack-ceilometer-collector is deployed
      register: ceilometer_agent_collector_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact ceilometer_agent_collector_enabled
      set_fact: {ceilometer_agent_collector_enabled: '{{ ceilometer_agent_collector_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable ceilometer_collector service on upgrade
      service: name=openstack-ceilometer-collector state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', ceilometer_agent_collector_enabled|bool]
    - changed_when: [step|int == 1, release == 'ocata', remove_ceilometer_expirer_crontab.stderr
          != "no crontab for ceilometer"]
      failed_when: [step|int == 1, release == 'ocata', remove_ceilometer_expirer_crontab.rc
          != 0, remove_ceilometer_expirer_crontab.stderr != "no crontab for ceilometer"]
      name: Remove ceilometer expirer cron tab on upgrade
      register: remove_ceilometer_expirer_crontab
      shell: /usr/bin/crontab -u ceilometer -r
    - command: systemctl is-enabled --quiet openstack-cinder-api
      ignore_errors: true
      name: Check is cinder_api is deployed
      register: cinder_api_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact cinder_api_enabled
      set_fact: {cinder_api_enabled: '{{ cinder_api_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop openstack-cinder-api
      service: name=openstack-cinder-api state=stopped
      when: [step|int == 1, release == 'ocata', cinder_api_enabled|bool]
    - name: Extra removal of services for cinder
      shell: 'cinder-manage service list |\

        grep -v Binary | tr ''@'' '' '' |\

        awk ''{print $1 " " $2}'' |\

        while read i ; do cinder-manage service remove $i ; done

        '
      when: [step|int == 5, release == 'pike', is_bootstrap_node|bool]
    - command: cinder-manage db online_data_migrations
      name: Extra migration for cinder
      when: [step|int == 5, release == 'pike', is_bootstrap_node|bool]
    - name: Cinder package update
      shell: yum -y update openstack-cinder*
      when: [step|int == 6, is_bootstrap_node|bool]
    - command: cinder-manage db sync
      name: Cinder db sync
      when: [step|int == 8, is_bootstrap_node|bool]
    - command: systemctl is-enabled --quiet openstack-cinder-scheduler
      ignore_errors: true
      name: Check if cinder_scheduler is deployed
      register: cinder_scheduler_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact cinder_scheduler_enabled
      set_fact: {cinder_scheduler_enabled: '{{ cinder_scheduler_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop openstack-cinder-scheduler
      service: name=openstack-cinder-scheduler state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', cinder_scheduler_enabled|bool]
    - ignore_errors: true
      name: Check cluster resource status
      pacemaker_resource: {check_mode: false, resource: openstack-cinder-volume, state: show}
      register: cinder_volume_res_result
      when: [step|int == 0, release == 'ocata', is_bootstrap_node|bool]
    - name: Set fact cinder_volume_res
      set_fact: {cinder_volume_res: '{{ cinder_volume_res_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata', is_bootstrap_node|bool]
    - name: Disable the openstack-cinder-volume cluster resource
      pacemaker_resource: {resource: openstack-cinder-volume, state: disable, wait_for_resource: true}
      register: cinder_volume_output
      retries: 5
      until: cinder_volume_output.rc == 0
      when: [step|int == 2, release == 'ocata', is_bootstrap_node|bool, cinder_volume_res|bool]
    - command: systemctl is-enabled --quiet openstack-glance-api
      ignore_errors: true
      name: Check if glance_api is deployed
      register: glance_api_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact glance_api_enabled
      set_fact: {glance_api_enabled: '{{ glance_api_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop openstack-glance-api
      service: name=openstack-glance-api state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', glance_api_enabled|bool]
    - name: glance package update
      when: [step|int == 6, is_bootstrap_node|bool]
      yum: name=openstack-glance state=latest
    - command: glance-manage db_sync
      name: glance db sync
      when: [step|int == 8, is_bootstrap_node|bool]
    - command: systemctl is-enabled --quiet openstack-glance-registry
      ignore_errors: true
      name: Check if glance_registry is deployed
      register: glance_registry_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact glance_registry_enabled
      set_fact: {glance_registry_enabled: '{{ glance_registry_enabled_result.rc ==
          0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop openstack-glance-registry
      service: name=openstack-glance-registry state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', glance_registry_enabled|bool]
    - command: systemctl is-enabled openstack-heat-api
      ignore_errors: true
      name: FFU check openstack-heat-api is enabled
      register: heat_api_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact heat_api_enabled
      set_fact: {heat_api_enabled: '{{ heat_api_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: FFU stop and disable openstack-heat-api
      service: name=openstack-heat-api state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', heat_api_enabled|bool]
    - name: FFU Heat package update
      shell: yum -y update openstack-heat*
      when: [step|int == 6, is_bootstrap_node|bool]
    - command: heat-manage db_sync
      name: FFU Heat db-sync
      when: [step|int == 8, is_bootstrap_node|bool]
    - command: systemctl is-enabled openstack-heat-api-cloudwatch
      ignore_errors: true
      name: FFU check if heat_api_cloudwatch is deployed
      register: heat_api_cloudwatch_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact heat_api_cloudwatch_enabled
      set_fact: {heat_api_cloudwatch_enabled: '{{ heat_api_cloudwatch_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: FFU stop and disable the heat-api-cloudwatch service.
      service: name=openstack-heat-api-cloudwatch state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', heat_api_cloudwatch_enabled|bool]
    - ignore_errors: true
      name: Remove heat_api_cloudwatch package
      when: [step|int == 2, release == 'ocata']
      yum: name=openstack-heat-api-cloudwatch state=removed
    - command: systemctl is-enabled openstack-heat-api-cfn
      ignore_errors: true
      name: FFU check if openstack-heat-api-cfn service is enabled
      register: heat_api_cfn_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact heat_api_cfn_enabled
      set_fact: {heat_api_cfn_enabled: '{{ heat_api_cfn_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: FFU stop and disable openstack-heat-api-cfn service
      service: name=openstack-heat-api-cfn state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', heat_api_cfn_enabled|bool]
    - command: systemctl is-enabled --quiet openstack-heat-engine
      ignore_errors: true
      name: FFU check if openstack-heat-engine is enabled
      register: heat_engine_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact heat_engine_enabled
      set_fact: {heat_engine_enabled: '{{ heat_engine_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: FFU stop and disable openstack-heat-engine service
      service: name=openstack-heat-engine state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', heat_engine_enabled|bool]
    - ignore_errors: true
      name: Check for keystone running under apache
      register: keystone_httpd_enabled_result
      shell: httpd -t -D DUMP_VHOSTS | grep -q keystone_wsgi
      tags: common
      when: [step|int == 0, release == 'ocata']
    - name: Set fact keystone_httpd_enabled
      set_fact: {keystone_httpd_enabled: '{{ keystone_httpd_enabled_result.rc == 0
          }}'}
      when: [step|int == 0, release == 'ocata']
    - command: systemctl is-active --quiet httpd
      ignore_errors: true
      name: Check if httpd is running
      register: httpd_running_result
      when: [step|int == 0, release == 'ocata', httpd_running is undefined]
    - name: Set fact httpd_running if undefined
      set_fact: {httpd_running: '{{ httpd_running_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata', httpd_running is undefined]
    - name: Stop and disable keystone (under httpd)
      service: name=httpd state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', keystone_httpd_enabled|bool, httpd_running|bool]
    - name: Keystone package update
      shell: yum -y update openstack-keystone*
      when: [step|int == 6, is_bootstrap_node|bool]
    - command: keystone-manage db_sync
      name: keystone db sync
      when: [step|int == 8, is_bootstrap_node|bool]
    - command: systemctl is-enabled --quiet memcached
      ignore_errors: true
      name: Check if memcached is deployed
      register: memcached_enabled_result
      tags: common
      when: [step|int == 0, release == 'ocata']
    - name: memcached_enabled
      set_fact: {memcached_enabled: '{{ memcached_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable memcached service
      service: name=memcached state=stopped enabled=no
      when: [step|int == 2, release == 'ocata', memcached_enabled|bool]
    - command: systemctl is-enabled --quiet neutron-server
      ignore_errors: true
      name: Check if neutron_server is deployed
      register: neutron_server_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact neutron_server_enabled
      set_fact: {neutron_server_enabled: '{{ neutron_server_enabled_result.rc == 0
          }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop neutron_server
      service: name=neutron-server state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', neutron_server_enabled|bool]
    - name: Neutron package update
      shell: yum -y update openstack-neutron*
      when: [step|int == 6, is_bootstrap_node|bool]
    - name: Neutron package update workaround
      when: [step|int == 6, is_bootstrap_node|bool]
      yum: name=python-networking-odl state=latest
    - command: neutron-db-manage upgrade head
      name: Neutron db sync
      when: [step|int == 8, is_bootstrap_node|bool]
    - command: systemctl is-enabled --quiet neutron-dhcp-agent
      ignore_errors: true
      name: Check if neutron_dhcp_agent is deployed
      register: neutron_dhcp_agent_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact neutron_dhcp_agent_enabled
      set_fact: {neutron_dhcp_agent_enabled: '{{ neutron_dhcp_agent_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop neutron_dhcp_agent
      service: name=neutron-dhcp-agent state=stopped enabled=no
      when: [step|int == 2, release == 'ocata', neutron_dhcp_agent_enabled|bool]
    - command: systemctl is-enabled --quiet neutron-l3-agent
      ignore_errors: true
      name: Check if neutron_l3_agent is deployed
      register: neutron_l3_agent_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact neutron_l3_agent_enabled
      set_fact: {neutron_l3_agent_enabled: '{{ neutron_l3_agent_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop neutron_l3_agent
      service: name=neutron-l3-agent state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', neutron_l3_agent_enabled|bool]
    - command: systemctl is-enabled --quiet neutron-metadata-agent
      ignore_errors: true
      name: Check if neutron_metadata_agent is deployed
      register: neutron_metadata_agent_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact neutron_metadata_agent_enabled
      set_fact: {neutron_metadata_agent_enabled: '{{ neutron_metadata_agent_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop neutron_metadata_agent
      service: name=neutron-metadata-agent state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', neutron_metadata_agent_enabled|bool]
    - command: systemctl is-enabled --quiet neutron-openvswitch-agent
      ignore_errors: true
      name: Check if neutron_ovs_agent is deployed
      register: neutron_ovs_agent_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact neutron_ovs_agent_enabled
      set_fact: {neutron_ovs_agent_enabled: '{{ neutron_ovs_agent_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop neutron_openvswitch_agent
      service: name=neutron-openvswitch-agent state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', neutron_ovs_agent_enabled|bool]
    - command: systemctl is-enabled --quiet openstack-nova-api
      ignore_errors: true
      name: Check if nova-api is deployed
      register: nova_api_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact nova_api_enabled
      set_fact: {nova_api_enabled: '{{ nova_api_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop openstack-nova-api service
      service: name=openstack-nova-api state=stopped
      when: [step|int == 1, nova_api_enabled|bool, release == 'ocata']
    - command: nova-manage db online_data_migrations
      name: Extra migration for nova tripleo/+bug/1656791
      when: [step|int == 5, release == 'ocata', is_bootstrap_node|bool]
    - command: yum update -y *nova*
      name: Update nova packages
      when: [step|int == 6, is_bootstrap_node|bool]
    - block:
      - mysql_db: {name: nova_cell0, state: present}
        name: Create cell0 db
      - mysql_user: {host_all: true, name: nova, priv: '*.*:ALL', state: present}
        name: Grant access to cell0 db
      - copy: {content: "$transport_url = os_transport_url({\n  'transport' => hiera('messaging_service_name',\
            \ 'rabbit'),\n  'hosts'     => any2array(hiera('rabbitmq_node_names',\
            \ undef)),\n  'port'      => sprintf('%s',hiera('nova::rabbit_port', '5672')\
            \ ),\n  'username'  => hiera('nova::rabbit_userid', 'guest'),\n  'password'\
            \  => hiera('nova::rabbit_password'),\n  'ssl'       => sprintf('%s',\
            \ bool2num(str2bool(hiera('nova::rabbit_use_ssl', '0'))))\n}) oslo::messaging::default\
            \ { 'nova_config':\n  transport_url => $transport_url\n}\n", dest: /root/nova-api_upgrade_manifest.pp,
          mode: 384}
        name: Create puppet manifest to set transport_url in nova.conf
      - {changed_when: puppet_apply_nova_api_upgrade.rc == 2, command: 'puppet apply
          --modulepath /etc/puppet/modules:/opt/stack/puppet-modules:/usr/share/openstack-puppet/modules
          --detailed-exitcodes /root/nova-api_upgrade_manifest.pp', failed_when: 'puppet_apply_nova_api_upgrade.rc
          not in [0,2]', name: Run puppet apply to set tranport_url in nova.conf,
        register: puppet_apply_nova_api_upgrade}
      - {name: Setup cell_v2 (map cell0), shell: 'nova-manage cell_v2 map_cell0 --database_connection=mysql+pymysql://nova:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova_cell0'}
      - {changed_when: nova_api_create_cell.rc == 0, failed_when: 'nova_api_create_cell.rc
          not in [0,2]', name: Setup cell_v2 (create default cell), register: nova_api_create_cell,
        shell: 'nova-manage cell_v2 create_cell --name=''default'' --database_connection=$(hiera
          nova::database_connection)'}
      - {async: 300, command: nova-manage db sync, name: Setup cell_v2 (sync nova/cell
          DB), poll: 10}
      - {name: Setup cell_v2 (get cell uuid), register: nova_api_cell_uuid, shell: 'nova-manage
          cell_v2 list_cells | sed -e ''1,3d'' -e ''$d'' | awk -F '' *| *'' ''$2 ==
          "default" {print $4}'''}
      - {command: 'nova-manage cell_v2 discover_hosts --cell_uuid {{nova_api_cell_uuid.stdout}}
          --verbose', name: Setup cell_v2 (migrate hosts)}
      - {command: 'nova-manage cell_v2 map_instances --cell_uuid {{nova_api_cell_uuid.stdout}}',
        name: Setup cell_v2 (migrate instances)}
      when: [step|int == 7, release == 'ocata', is_bootstrap_node|bool]
    - command: nova-manage api_db sync
      name: Sync nova_api DB
      when: [step|int == 8, is_bootstrap_node|bool]
    - command: nova-manage db online_data_migrations
      name: Online data migration for nova
      when: [step|int == 8, is_bootstrap_node|bool]
    - command: systemctl is-enabled --quiet openstack-nova-conductor
      ignore_errors: true
      name: Check if nova_conductor is deployed
      register: nova_conductor_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact nova_conductor_enabled
      set_fact: {nova_conductor_enabled: '{{ nova_conductor_enabled_result.rc == 0
          }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable nova_conductor service
      service: name=openstack-nova-conductor state=stopped
      when: [step|int == 1, release == 'ocata', nova_conductor_enabled|bool]
    - command: systemctl is-active --quiet openstack-nova-consoleauth
      ignore_errors: true
      name: Check if nova_consoleauth is deployed
      register: nova_consoleauth_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact nova_consoleauth_enabled
      set_fact: {nova_consoleauth_enabled: '{{ nova_consoleauth_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable nova-consoleauth service
      service: name=openstack-nova-consoleauth state=stopped
      when: [step|int == 1, release == 'ocata', nova_consoleauth_enabled|bool]
    - command: systemctl is-enabled --quiet openstack-nova-api
      ignore_errors: true
      name: Check if nova_api_metadata is deployed
      register: nova_metadata_enabled_result
      tags: common
      when: [step|int == 0, release == 'ocata']
    - name: Set fact nova_metadata_enabled
      set_fact: {nova_metadata_enabled: '{{ nova_metadata_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable nova_api service
      service: name=openstack-nova-api state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', nova_metadata_enabled|bool]
    - command: systemctl is-enabled --quiet openstack-nova-scheduler
      ignore_errors: true
      name: Check if nova_scheduler is deployed
      register: nova_scheduler_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact nova_scheduler_enabled
      set_fact: {nova_scheduler_enabled: '{{ nova_scheduler_enabled_result.rc == 0
          }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable nova-scheduler service
      service: name=openstack-nova-scheduler state=stopped
      when: [step|int == 1, release == 'ocata', nova_scheduler_enabled|bool]
    - command: systemctl is-enabled --quiet openstack-nova-novncproxy
      ignore_errors: true
      name: Check if nova vncproxy is deployed
      register: nova_vncproxy_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact nova_vncproxy_enabled
      set_fact: {nova_vncproxy_enabled: '{{ nova_vncproxy_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable nova-novncproxy service
      service: name=openstack-nova-novncproxy state=stopped
      when: [step|int == 1, release == 'ocata', nova_vncproxy_enabled|bool]
    - ignore_errors: true
      name: Check cluster resource status of rabbitmq
      pacemaker_resource: {check_mode: false, resource: rabbitmq, state: show}
      register: rabbitmq_res_result
      when: [step|int == 0, release == 'ocata', is_bootstrap_node|bool]
    - name: Set fact rabbitmq_res
      set_fact: {rabbitmq_res: '{{ rabbitmq_res_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata', is_bootstrap_node|bool]
    - name: Disable the rabitmq cluster resource
      pacemaker_resource: {resource: rabbitmq, state: disable, wait_for_resource: true}
      register: rabbitmq_output
      retries: 5
      until: rabbitmq_output.rc == 0
      when: [step|int == 2, release == 'ocata', is_bootstrap_node|bool, rabbitmq_res|bool]
    - ignore_errors: true
      name: Check cluster resource status of redis
      pacemaker_resource: {check_mode: false, resource: redis, state: show}
      register: redis_res_result
      when: [step|int == 0, release == 'ocata', is_bootstrap_node|bool]
    - name: Set fact redis_res
      set_fact: {redis_res: '{{ redis_res_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata', is_bootstrap_node|bool]
    - name: Disable the redis cluster resource
      pacemaker_resource: {resource: redis, state: disable, wait_for_resource: true}
      register: redis_output
      retries: 5
      until: redis_output.rc == 0
      when: [step|int == 2, release == 'ocata', is_bootstrap_node|bool, redis_res|bool]
    - command: systemctl is-enabled --quiet "{{ item }}"
      ignore_errors: true
      name: Check if swift-proxy or swift-object-expirer are deployed
      register: swift_proxy_services_enabled_result
      when: [step|int == 0, release == 'ocata']
      with_items: [openstack-swift-proxy, openstack-swift-object-expirer]
    - name: Set fact swift_proxy_services_enabled
      set_fact: {swift_proxy_services_enabled: '{{ swift_proxy_services_enabled_result
          }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop swift-proxy and swift-object-expirer services
      service: name={{ item.item }} state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', item.rc == 0]
      with_items: '{{ swift_proxy_services_enabled.results }}'
    - command: systemctl is-enabled --quiet "{{ item }}"
      ignore_errors: true
      name: Check if swift storage services are deployed
      register: swift_services_enabled_result
      when: [step|int == 0, release == 'ocata']
      with_items: [openstack-swift-account-auditor, openstack-swift-account-reaper,
        openstack-swift-account-replicator, openstack-swift-account, openstack-swift-container-auditor,
        openstack-swift-container-replicator, openstack-swift-container-updater, openstack-swift-container,
        openstack-swift-object-auditor, openstack-swift-object-replicator, openstack-swift-object-updater,
        openstack-swift-object]
    - name: Set fact swift_services_enabled
      set_fact: {swift_services_enabled: '{{ swift_services_enabled_result }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop swift storage services
      service: name={{ item.item }} state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', item.rc == 0]
      with_items: '{{ swift_services_enabled.results }}'
    - name: Register repo type and args
      set_fact:
        fast_forward_repo_args:
          tripleo_repos: {ocata: -b ocata current, pike: -b pike current, queens: -b
              queens current}
        fast_forward_repo_type: custom-script
      when: step|int == 3
    - debug: {msg: 'fast_forward_repo_type: {{ fast_forward_repo_type }} fast_forward_repo_args:
          {{ fast_forward_repo_args }}'}
      when: step|int == 3
    - block:
      - git: {dest: /home/stack/tripleo-repos/, repo: 'https://github.com/openstack/tripleo-repos.git'}
        name: clone tripleo-repos
      - args: {chdir: /home/stack/tripleo-repos/}
        command: python setup.py install
        name: install tripleo-repos
      - {command: 'tripleo-repos {{ fast_forward_repo_args.tripleo_repos[release]
          }}', name: Enable tripleo-repos}
      when: [step|int == 3, ffu_packages_apply|bool, fast_forward_repo_type == 'tripleo-repos']
    - block:
      - copy: {content: "#!/bin/bash\nset -e\necho \"If you use FastForwardRepoType\
            \ 'custom-script' you have to provide the upgrade repo script content.\"\
            \necho \"It will be installed as /root/ffu_upgrade_repo.sh on the node\"\
            \necho \"and passed the upstream name (ocata, pike, queens) of the release\
            \ as first argument\"\ncase $1 in\n  ocata)\n    subscription-manager\
            \ repos --disable=rhel-7-server-openstack-10-rpms\n    subscription-manager\
            \ repos --enable=rhel-7-server-openstack-11-rpms\n    ;;\n  pike)\n  \
            \  subscription-manager repos --disable=rhel-7-server-openstack-11-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-12-rpms\n\
            \    ;;\n  queens)\n    subscription-manager repos --disable=rhel-7-server-openstack-12-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-13-rpms\n\
            \    ;;\n  *)\n    echo \"unknown release $1\" >&2\n    exit 1\nesac\n",
          dest: /root/ffu_update_repo.sh, mode: 448}
        name: Create custom Script for upgrading repo.
      - {name: Execute custom script for upgrading repo., shell: '/root/ffu_update_repo.sh
          {{release}}'}
      when: [step|int == 3, ffu_packages_apply|bool, fast_forward_repo_type == 'custom-script']
    role_data_global_config_settings: {}
    role_data_host_prep_tasks:
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/cinder, /var/log/containers/httpd/cinder-api]
    - copy: {content: 'Log files from cinder containers can be found under

          /var/log/containers/cinder and /var/log/containers/httpd/cinder-api.

          ', dest: /var/log/cinder/readme.txt}
      ignore_errors: true
      name: cinder logs readme
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/var/log/containers/cinder]
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/var/log/containers/cinder, /var/lib/cinder]
    - file: {path: /etc/ceph, state: directory}
      name: ensure ceph configurations exist
    - name: cinder_enable_iscsi_backend fact
      set_fact: {cinder_enable_iscsi_backend: false}
    - args: {creates: /var/lib/cinder/cinder-volumes}
      command: dd if=/dev/zero of=/var/lib/cinder/cinder-volumes bs=1 count=0 seek=10280M
      name: cinder create LVM volume group dd
      when: cinder_enable_iscsi_backend
    - args: {creates: /dev/loop2, executable: /bin/bash}
      name: cinder create LVM volume group
      shell: "if ! losetup /dev/loop2; then\n  losetup /dev/loop2 /var/lib/cinder/cinder-volumes\n\
        fi\nif ! pvdisplay | grep cinder-volumes; then\n  pvcreate /dev/loop2\nfi\n\
        if ! vgdisplay | grep cinder-volumes; then\n  vgcreate cinder-volumes /dev/loop2\n\
        fi\n"
      when: cinder_enable_iscsi_backend
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/glance]
    - copy: {content: 'Log files from glance containers can be found under

          /var/log/containers/glance.

          ', dest: /var/log/glance/readme.txt}
      ignore_errors: true
      name: glance logs readme
    - block:
      - name: null
        set_fact: {remote_file_path: /etc/glance/glance-metadata-file.conf}
      - file: {path: '{{ remote_file_path }}', state: touch}
        name: null
      - {register: file_path, stat: 'path="{{ remote_file_path }}"'}
      - copy:
          content: {mount_point: /var/lib/glance/images, share_location: '{{item.NETAPP_SHARE}}',
            type: nfs}
          dest: '{{ remote_file_path }}'
        when: [file_path.stat.exists == true]
        with_items:
        - {NETAPP_SHARE: ''}
      - mount: name=/var/lib/glance/images src="{{item.NETAPP_SHARE}}" fstype=nfs4
          opts="{{item.NFS_OPTIONS}}" state=mounted
        name: null
        with_items:
        - {NETAPP_SHARE: '', NFS_OPTIONS: '_netdev,bg,intr,context=system_u:object_r:glance_var_lib_t:s0'}
      name: Mount Netapp NFS
      vars: {netapp_nfs_backend_enable: false}
      when: netapp_nfs_backend_enable
    - mount: name=/var/lib/glance/images src="{{item.NFS_SHARE}}" fstype=nfs4 opts="{{item.NFS_OPTIONS}}"
        state=mounted
      name: Mount NFS on host
      vars: {nfs_backend_enable: false}
      when: [nfs_backend_enable]
      with_items:
      - {NFS_OPTIONS: '_netdev,bg,intr,context=system_u:object_r:glance_var_lib_t:s0',
        NFS_SHARE: ''}
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/var/lib/haproxy]
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/heat, /var/log/containers/httpd/heat-api]
    - copy: {content: 'Log files from heat containers can be found under

          /var/log/containers/heat and /var/log/containers/httpd/heat-api*.

          ', dest: /var/log/heat/readme.txt}
      ignore_errors: true
      name: heat logs readme
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/heat, /var/log/containers/httpd/heat-api-cfn]
    - file: {path: /var/log/containers/heat, state: directory}
      name: create persistent logs directory
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/horizon, /var/log/containers/httpd/horizon]
    - copy: {content: 'Log files from horizon containers can be found under

          /var/log/containers/horizon and /var/log/containers/httpd/horizon.

          ', dest: /var/log/horizon/readme.txt}
      ignore_errors: true
      name: horizon logs readme
    - {name: stat /lib/systemd/system/iscsid.socket, register: stat_iscsid_socket,
      stat: path=/lib/systemd/system/iscsid.socket}
    - {name: Stop and disable iscsid.socket service, service: name=iscsid.socket state=stopped
        enabled=no, when: stat_iscsid_socket.stat.exists}
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/keystone, /var/log/containers/httpd/keystone]
    - copy: {content: 'Log files from keystone containers can be found under

          /var/log/containers/keystone and /var/log/containers/httpd/keystone.

          ', dest: /var/log/keystone/readme.txt}
      ignore_errors: true
      name: keystone logs readme
    - copy: {content: 'Memcached container logs to stdout/stderr only.

          ', dest: /var/log/memcached-readme.txt}
      ignore_errors: true
      name: memcached logs readme
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/var/log/containers/mysql, /var/lib/mysql]
    - copy: {content: 'Log files from mysql containers can be found under

          /var/log/containers/mysql.

          ', dest: /var/log/mariadb/readme.txt}
      ignore_errors: true
      name: mysql logs readme
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/neutron, /var/log/containers/httpd/neutron-api]
    - copy: {content: 'Log files from neutron containers can be found under

          /var/log/containers/neutron and /var/log/containers/httpd/neutron-api.

          ', dest: /var/log/neutron/readme.txt}
      ignore_errors: true
      name: neutron logs readme
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/neutron]
    - file: {path: /var/lib/neutron, state: directory}
      name: create /var/lib/neutron
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/nova, /var/log/containers/httpd/nova-api]
    - copy: {content: 'Log files from nova containers can be found under

          /var/log/containers/nova and /var/log/containers/httpd/nova-*.

          ', dest: /var/log/nova/readme.txt}
      ignore_errors: true
      name: nova logs readme
    - file: {path: /var/log/containers/nova, state: directory}
      name: create persistent logs directory
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/nova, /var/log/containers/httpd/nova-placement]
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/var/lib/rabbitmq, /var/log/containers/rabbitmq]
    - copy: {content: 'Log files from rabbitmq containers can be found under

          /var/log/containers/rabbitmq.

          ', dest: /var/log/rabbitmq/readme.txt}
      ignore_errors: true
      name: rabbitmq logs readme
    - {name: stop the Erlang port mapper on the host and make sure it cannot bind
        to the port used by container, shell: 'echo ''export ERL_EPMD_ADDRESS=127.0.0.1''
        > /etc/rabbitmq/rabbitmq-env.conf

        echo ''export ERL_EPMD_PORT=4370'' >> /etc/rabbitmq/rabbitmq-env.conf

        for pid in $(pgrep epmd --ns 1 --nslist pid); do kill $pid; done

        '}
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/var/lib/redis, /var/log/containers/redis, /var/run/redis]
    - copy: {content: 'Log files from redis containers can be found under

          /var/log/containers/redis.

          ', dest: /var/log/redis/readme.txt}
      ignore_errors: true
      name: redis logs readme
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/srv/node, /var/log/swift]
    - file: {dest: /var/log/containers/swift, src: /var/log/swift, state: link}
      name: Create swift logging symlink
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/srv/node, /var/log/swift, /var/log/containers]
    - name: Set swift_use_local_disks fact
      set_fact: {swift_use_local_disks: true}
    - file: {path: /srv/node/d1, state: directory}
      name: Create Swift d1 directory if needed
      when: swift_use_local_disks
    - copy: {content: 'Log files from swift containers can be found under

          /var/log/containers/swift and /var/log/containers/httpd/swift-*.

          ', dest: /var/log/swift/readme.txt}
      ignore_errors: true
      name: swift logs readme
    - filesystem: {dev: '/dev/{{ item }}', fstype: xfs, opts: -f -i size=1024}
      name: Format SwiftRawDisks
      with_items:
      - []
    - mount: {fstype: xfs, name: '/srv/node/{{ item }}', opts: noatime, src: '/dev/{{
          item }}', state: mounted}
      name: Mount devices defined in SwiftRawDisks
      with_items:
      - []
    role_data_kolla_config:
      /var/lib/kolla/config_files/cinder_api.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'cinder:cinder', path: /var/log/cinder, recurse: true}
      /var/lib/kolla/config_files/cinder_api_cron.json:
        command: /usr/sbin/crond -n
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'cinder:cinder', path: /var/log/cinder, recurse: true}
      /var/lib/kolla/config_files/cinder_scheduler.json:
        command: /usr/bin/cinder-scheduler --config-file /usr/share/cinder/cinder-dist.conf
          --config-file /etc/cinder/cinder.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'cinder:cinder', path: /var/log/cinder, recurse: true}
      /var/lib/kolla/config_files/cinder_volume.json:
        command: /usr/bin/cinder-volume --config-file /usr/share/cinder/cinder-dist.conf
          --config-file /etc/cinder/cinder.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /etc/ceph/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-ceph/}
        - {dest: /etc/iscsi/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-iscsid/*}
        permissions:
        - {owner: 'cinder:cinder', path: /var/log/cinder, recurse: true}
      /var/lib/kolla/config_files/clustercheck.json:
        command: /usr/sbin/xinetd -dontfork
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/glance_api.json:
        command: /usr/bin/glance-api --config-file /usr/share/glance/glance-api-dist.conf
          --config-file /etc/glance/glance-api.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /etc/ceph/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-ceph/}
        permissions:
        - {owner: 'glance:glance', path: /var/lib/glance, recurse: true}
        - {owner: 'glance:glance', path: /etc/ceph/ceph.client.openstack.keyring,
          perm: '0600'}
      /var/lib/kolla/config_files/glance_api_tls_proxy.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/haproxy.json:
        command: /usr/sbin/haproxy-systemd-wrapper -f /etc/haproxy/haproxy.cfg
        config_files:
        - {dest: /, merge: true, optional: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /, merge: true, optional: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-tls/*}
        permissions:
        - {owner: 'haproxy:haproxy', path: /var/lib/haproxy, recurse: true}
        - {optional: true, owner: 'haproxy:haproxy', path: /etc/pki/tls/certs/haproxy/*,
          perm: '0600'}
        - {optional: true, owner: 'haproxy:haproxy', path: /etc/pki/tls/private/haproxy/*,
          perm: '0600'}
      /var/lib/kolla/config_files/heat_api.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'heat:heat', path: /var/log/heat, recurse: true}
      /var/lib/kolla/config_files/heat_api_cfn.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'heat:heat', path: /var/log/heat, recurse: true}
      /var/lib/kolla/config_files/heat_api_cron.json:
        command: /usr/sbin/crond -n
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'heat:heat', path: /var/log/heat, recurse: true}
      /var/lib/kolla/config_files/heat_engine.json:
        command: '/usr/bin/heat-engine --config-file /usr/share/heat/heat-dist.conf
          --config-file /etc/heat/heat.conf '
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'heat:heat', path: /var/log/heat, recurse: true}
      /var/lib/kolla/config_files/horizon.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'apache:apache', path: /var/log/horizon/, recurse: true}
        - {owner: 'apache:apache', path: /etc/openstack-dashboard/, recurse: true}
        - {owner: 'apache:apache', path: /usr/share/openstack-dashboard/openstack_dashboard/local/,
          recurse: false}
        - {owner: 'apache:apache', path: /usr/share/openstack-dashboard/openstack_dashboard/local/local_settings.d/,
          recurse: false}
      /var/lib/kolla/config_files/iscsid.json:
        command: /usr/sbin/iscsid -f
        config_files:
        - {dest: /etc/iscsi/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-iscsid/*}
      /var/lib/kolla/config_files/keystone.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/keystone_cron.json:
        command: /usr/sbin/crond -n
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'keystone:keystone', path: /var/log/keystone, recurse: true}
      /var/lib/kolla/config_files/logrotate-crond.json:
        command: /usr/sbin/crond -s -n
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/mysql.json:
        command: /usr/sbin/pacemaker_remoted
        config_files:
        - {dest: /etc/libqb/force-filesystem-sockets, owner: root, perm: '0644', source: /dev/null}
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /, merge: true, optional: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-tls/*}
        permissions:
        - {owner: 'mysql:mysql', path: /var/log/mysql, recurse: true}
        - {optional: true, owner: 'mysql:mysql', path: /etc/pki/tls/certs/mysql.crt,
          perm: '0600'}
        - {optional: true, owner: 'mysql:mysql', path: /etc/pki/tls/private/mysql.key,
          perm: '0600'}
      /var/lib/kolla/config_files/neutron_api.json:
        command: /usr/bin/neutron-server --config-file /usr/share/neutron/neutron-dist.conf
          --config-dir /usr/share/neutron/server --config-file /etc/neutron/neutron.conf
          --config-file /etc/neutron/plugin.ini --config-dir /etc/neutron/conf.d/common
          --config-dir /etc/neutron/conf.d/neutron-server --log-file=/var/log/neutron/server.log
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'neutron:neutron', path: /var/log/neutron, recurse: true}
      /var/lib/kolla/config_files/neutron_dhcp.json:
        command: /usr/bin/neutron-dhcp-agent --config-file /usr/share/neutron/neutron-dist.conf
          --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent.ini
          --config-dir /etc/neutron/conf.d/common --config-dir /etc/neutron/conf.d/neutron-dhcp-agent
          --log-file=/var/log/neutron/dhcp-agent.log
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'neutron:neutron', path: /var/log/neutron, recurse: true}
        - {owner: 'neutron:neutron', path: /var/lib/neutron, recurse: true}
        - {owner: 'neutron:neutron', path: /etc/pki/tls/certs/neutron.crt}
        - {owner: 'neutron:neutron', path: /etc/pki/tls/private/neutron.key}
      /var/lib/kolla/config_files/neutron_l3_agent.json:
        command: /usr/bin/neutron-l3-agent --config-file /usr/share/neutron/neutron-dist.conf
          --config-dir /usr/share/neutron/l3_agent --config-file /etc/neutron/neutron.conf
          --config-file /etc/neutron/l3_agent.ini --config-dir /etc/neutron/conf.d/common
          --config-dir /etc/neutron/conf.d/neutron-l3-agent --log-file=/var/log/neutron/l3-agent.log
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'neutron:neutron', path: /var/log/neutron, recurse: true}
        - {owner: 'neutron:neutron', path: /var/lib/neutron, recurse: true}
      /var/lib/kolla/config_files/neutron_metadata_agent.json:
        command: /usr/bin/neutron-metadata-agent --config-file /usr/share/neutron/neutron-dist.conf
          --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/metadata_agent.ini
          --config-dir /etc/neutron/conf.d/common --config-dir /etc/neutron/conf.d/neutron-metadata-agent
          --log-file=/var/log/neutron/metadata-agent.log
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'neutron:neutron', path: /var/log/neutron, recurse: true}
        - {owner: 'neutron:neutron', path: /var/lib/neutron, recurse: true}
      /var/lib/kolla/config_files/neutron_ovs_agent.json:
        command: /neutron_ovs_agent_launcher.sh
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'neutron:neutron', path: /var/log/neutron, recurse: true}
      /var/lib/kolla/config_files/neutron_server_tls_proxy.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/nova_api.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/nova_api_cron.json:
        command: /usr/sbin/crond -n
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/nova_conductor.json:
        command: '/usr/bin/nova-conductor '
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/nova_consoleauth.json:
        command: '/usr/bin/nova-consoleauth '
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/nova_metadata.json:
        command: '/usr/bin/nova-api-metadata '
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/nova_placement.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/nova_scheduler.json:
        command: '/usr/bin/nova-scheduler '
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/nova_vnc_proxy.json:
        command: '/usr/bin/nova-novncproxy --web /usr/share/novnc/ '
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
      /var/lib/kolla/config_files/rabbitmq.json:
        command: /usr/sbin/pacemaker_remoted
        config_files:
        - {dest: /etc/libqb/force-filesystem-sockets, owner: root, perm: '0644', source: /dev/null}
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /, merge: true, optional: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-tls/*}
        permissions:
        - {owner: 'rabbitmq:rabbitmq', path: /var/lib/rabbitmq, recurse: true}
        - {owner: 'rabbitmq:rabbitmq', path: /var/log/rabbitmq, recurse: true}
        - {optional: true, owner: 'rabbitmq:rabbitmq', path: /etc/pki/tls/certs/rabbitmq.crt,
          perm: '0600'}
        - {optional: true, owner: 'rabbitmq:rabbitmq', path: /etc/pki/tls/private/rabbitmq.key,
          perm: '0600'}
      /var/lib/kolla/config_files/redis.json:
        command: /usr/sbin/pacemaker_remoted
        config_files:
        - {dest: /etc/libqb/force-filesystem-sockets, owner: root, perm: '0644', source: /dev/null}
        - {dest: /, merge: true, optional: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /, merge: true, optional: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-tls/*}
        permissions:
        - {owner: 'redis:redis', path: /var/run/redis, recurse: true}
        - {owner: 'redis:redis', path: /var/lib/redis, recurse: true}
        - {owner: 'redis:redis', path: /var/log/redis, recurse: true}
        - {optional: true, owner: 'redis:redis', path: /etc/pki/tls/certs/redis.crt,
          perm: '0600'}
        - {optional: true, owner: 'redis:redis', path: /etc/pki/tls/private/redis.key,
          perm: '0600'}
      /var/lib/kolla/config_files/redis_tls_proxy.json:
        command: stunnel /etc/stunnel/stunnel.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_account_auditor.json:
        command: /usr/bin/swift-account-auditor /etc/swift/account-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_account_reaper.json:
        command: /usr/bin/swift-account-reaper /etc/swift/account-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_account_replicator.json:
        command: /usr/bin/swift-account-replicator /etc/swift/account-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_account_server.json:
        command: /usr/bin/swift-account-server /etc/swift/account-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_container_auditor.json:
        command: /usr/bin/swift-container-auditor /etc/swift/container-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_container_replicator.json:
        command: /usr/bin/swift-container-replicator /etc/swift/container-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_container_server.json:
        command: /usr/bin/swift-container-server /etc/swift/container-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_container_updater.json:
        command: /usr/bin/swift-container-updater /etc/swift/container-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_object_auditor.json:
        command: /usr/bin/swift-object-auditor /etc/swift/object-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_object_expirer.json:
        command: /usr/bin/swift-object-expirer /etc/swift/object-expirer.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_object_replicator.json:
        command: /usr/bin/swift-object-replicator /etc/swift/object-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_object_server.json:
        command: /usr/bin/swift-object-server /etc/swift/object-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'swift:swift', path: /var/cache/swift, recurse: true}
      /var/lib/kolla/config_files/swift_object_updater.json:
        command: /usr/bin/swift-object-updater /etc/swift/object-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_proxy.json:
        command: /usr/bin/swift-proxy-server /etc/swift/proxy-server.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_proxy_tls_proxy.json:
        command: /usr/sbin/httpd -DFOREGROUND
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/swift_rsync.json:
        command: /usr/bin/rsync --daemon --no-detach --config=/etc/rsyncd.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
    role_data_logging_groups: [root]
    role_data_logging_sources: []
    role_data_merged_config_settings:
      apache::default_vhost: false
      apache::ip: internal_api
      apache::mod::prefork::maxclients: 256
      apache::mod::prefork::serverlimit: 256
      apache::mod::remoteip::proxy_ips: ['%{hiera(''apache_remote_proxy_ips_network'')}']
      apache::server_signature: 'Off'
      apache::server_tokens: Prod
      apache_remote_proxy_ips_network: internal_api_subnet
      ceph_mgr_ansible_vars:
        ceph_conf_overrides:
          global: {max_open_files: 131072, mon_max_pg_per_osd: 1000, mon_osd_full_ratio: 90,
            osd_journal_size: 10240, osd_pool_default_pg_num: 32, osd_pool_default_pgp_num: 32,
            osd_pool_default_size: 3, rgw_keystone_accepted_roles: 'Member, admin',
            rgw_keystone_admin_domain: default, rgw_keystone_admin_password: pFavNu2xcWbJFKHgBatfFpn9Z,
            rgw_keystone_admin_project: service, rgw_keystone_admin_user: swift, rgw_keystone_api_version: 3,
            rgw_keystone_implicit_tenants: 'true', rgw_keystone_revocation_interval: '0',
            rgw_keystone_url: 'http://172.17.1.13:5000', rgw_s3_auth_use_keystone: 'true'}
        ceph_docker_image: rhceph/rhceph-3-rhel7
        ceph_docker_image_tag: latest
        ceph_docker_registry: 192.168.120.1:8787
        ceph_mgr_docker_extra_env: -e MGR_DASHBOARD=0
        ceph_origin: distro
        ceph_stable: true
        cluster: ceph
        cluster_network: 172.17.4.0/24
        containerized_deployment: true
        docker: true
        fsid: 1ed62898-b2ad-11e8-916e-2047478ccfaa
        generate_fsid: false
        ip_version: ipv4
        ireallymeanit: 'yes'
        keys:
        - {key: AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==, mgr_cap: allow *, mode: '0600',
          mon_cap: profile rbd, name: client.openstack, osd_cap: 'profile rbd pool=volumes,
            profile rbd pool=backups, profile rbd pool=vms, profile rbd pool=images,
            profile rbd pool=metrics'}
        - {key: AQDNj5JbAAAAABAAkqBgz7ALcbRkZuO5m5jr2A==, mds_cap: allow *, mgr_cap: allow
            *, mode: '0600', mon_cap: 'allow r, allow command \"auth del\", allow
            command \"auth caps\", allow command \"auth get\", allow command \"auth
            get-or-create\"', name: client.manila, osd_cap: allow rw}
        - {key: AQDNj5JbAAAAABAAZXB7q7DHSABFHtBxlhIvWw==, mgr_cap: allow *, mode: '0600',
          mon_cap: allow rw, name: client.radosgw, osd_cap: allow rwx}
        monitor_address_block: 172.17.3.0/24
        ntp_service_enabled: false
        openstack_config: true
        openstack_keys:
        - {key: AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==, mgr_cap: allow *, mode: '0600',
          mon_cap: profile rbd, name: client.openstack, osd_cap: 'profile rbd pool=volumes,
            profile rbd pool=backups, profile rbd pool=vms, profile rbd pool=images,
            profile rbd pool=metrics'}
        - {key: AQDNj5JbAAAAABAAkqBgz7ALcbRkZuO5m5jr2A==, mds_cap: allow *, mgr_cap: allow
            *, mode: '0600', mon_cap: 'allow r, allow command \"auth del\", allow
            command \"auth caps\", allow command \"auth get\", allow command \"auth
            get-or-create\"', name: client.manila, osd_cap: allow rw}
        - {key: AQDNj5JbAAAAABAAZXB7q7DHSABFHtBxlhIvWw==, mgr_cap: allow *, mode: '0600',
          mon_cap: allow rw, name: client.radosgw, osd_cap: allow rwx}
        openstack_pools:
        - {name: images, pg_num: 32, rule_name: ''}
        - {name: metrics, pg_num: 32, rule_name: ''}
        - {name: backups, pg_num: 32, rule_name: ''}
        - {name: vms, pg_num: 32, rule_name: ''}
        - {name: volumes, pg_num: 32, rule_name: ''}
        pools: []
        public_network: 172.17.3.0/24
        user_config: true
      ceph_mon_ansible_vars:
        admin_secret: AQDNj5JbAAAAABAA4hKBb1guIjOHrt9pcpBpgQ==
        ceph_conf_overrides:
          global: {max_open_files: 131072, mon_max_pg_per_osd: 1000, mon_osd_full_ratio: 90,
            osd_journal_size: 10240, osd_pool_default_pg_num: 32, osd_pool_default_pgp_num: 32,
            osd_pool_default_size: 3, rgw_keystone_accepted_roles: 'Member, admin',
            rgw_keystone_admin_domain: default, rgw_keystone_admin_password: pFavNu2xcWbJFKHgBatfFpn9Z,
            rgw_keystone_admin_project: service, rgw_keystone_admin_user: swift, rgw_keystone_api_version: 3,
            rgw_keystone_implicit_tenants: 'true', rgw_keystone_revocation_interval: '0',
            rgw_keystone_url: 'http://172.17.1.13:5000', rgw_s3_auth_use_keystone: 'true'}
        ceph_docker_image: rhceph/rhceph-3-rhel7
        ceph_docker_image_tag: latest
        ceph_docker_registry: 192.168.120.1:8787
        ceph_origin: distro
        ceph_stable: true
        cluster: ceph
        cluster_network: 172.17.4.0/24
        containerized_deployment: true
        docker: true
        fsid: 1ed62898-b2ad-11e8-916e-2047478ccfaa
        generate_fsid: false
        ip_version: ipv4
        ireallymeanit: 'yes'
        keys:
        - {key: AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==, mgr_cap: allow *, mode: '0600',
          mon_cap: profile rbd, name: client.openstack, osd_cap: 'profile rbd pool=volumes,
            profile rbd pool=backups, profile rbd pool=vms, profile rbd pool=images,
            profile rbd pool=metrics'}
        - {key: AQDNj5JbAAAAABAAkqBgz7ALcbRkZuO5m5jr2A==, mds_cap: allow *, mgr_cap: allow
            *, mode: '0600', mon_cap: 'allow r, allow command \"auth del\", allow
            command \"auth caps\", allow command \"auth get\", allow command \"auth
            get-or-create\"', name: client.manila, osd_cap: allow rw}
        - {key: AQDNj5JbAAAAABAAZXB7q7DHSABFHtBxlhIvWw==, mgr_cap: allow *, mode: '0600',
          mon_cap: allow rw, name: client.radosgw, osd_cap: allow rwx}
        monitor_address_block: 172.17.3.0/24
        monitor_secret: AQDNj5JbAAAAABAAnWz6FTOVAhl1hyoFF7LVSA==
        ntp_service_enabled: false
        openstack_config: true
        openstack_keys:
        - {key: AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==, mgr_cap: allow *, mode: '0600',
          mon_cap: profile rbd, name: client.openstack, osd_cap: 'profile rbd pool=volumes,
            profile rbd pool=backups, profile rbd pool=vms, profile rbd pool=images,
            profile rbd pool=metrics'}
        - {key: AQDNj5JbAAAAABAAkqBgz7ALcbRkZuO5m5jr2A==, mds_cap: allow *, mgr_cap: allow
            *, mode: '0600', mon_cap: 'allow r, allow command \"auth del\", allow
            command \"auth caps\", allow command \"auth get\", allow command \"auth
            get-or-create\"', name: client.manila, osd_cap: allow rw}
        - {key: AQDNj5JbAAAAABAAZXB7q7DHSABFHtBxlhIvWw==, mgr_cap: allow *, mode: '0600',
          mon_cap: allow rw, name: client.radosgw, osd_cap: allow rwx}
        openstack_pools:
        - {name: images, pg_num: 32, rule_name: ''}
        - {name: metrics, pg_num: 32, rule_name: ''}
        - {name: backups, pg_num: 32, rule_name: ''}
        - {name: vms, pg_num: 32, rule_name: ''}
        - {name: volumes, pg_num: 32, rule_name: ''}
        pools: []
        public_network: 172.17.3.0/24
        user_config: true
      cinder::api::bind_host: '%{hiera(''fqdn_internal_api'')}'
      cinder::api::enable_proxy_headers_parsing: true
      cinder::api::nova_catalog_admin_info: compute:nova:adminURL
      cinder::api::nova_catalog_info: compute:nova:internalURL
      cinder::api::service_name: httpd
      cinder::backend_host: hostgroup
      cinder::ceilometer::notification_driver: noop
      cinder::config:
        DEFAULT/swift_catalog_info: {value: 'object-store:swift:internalURL'}
      cinder::cron::db_purge::age: '30'
      cinder::cron::db_purge::destination: /var/log/cinder/cinder-rowsflush.log
      cinder::cron::db_purge::hour: '0'
      cinder::cron::db_purge::minute: '1'
      cinder::cron::db_purge::month: '*'
      cinder::cron::db_purge::monthday: '*'
      cinder::cron::db_purge::user: cinder
      cinder::cron::db_purge::weekday: '*'
      cinder::database_connection: mysql+pymysql://cinder:VY8DpgEdM2R9VHVkTRveuzmtp@172.17.1.13/cinder?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      cinder::db::database_db_max_retries: -1
      cinder::db::database_max_retries: -1
      cinder::db::mysql::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      cinder::db::mysql::dbname: cinder
      cinder::db::mysql::host: 172.17.1.13
      cinder::db::mysql::password: VY8DpgEdM2R9VHVkTRveuzmtp
      cinder::db::mysql::user: cinder
      cinder::debug: true
      cinder::glance::glance_api_servers: http://172.17.1.13:9292
      cinder::keystone::auth::admin_url: http://172.17.1.13:8776/v1/%(tenant_id)s
      cinder::keystone::auth::admin_url_v2: http://172.17.1.13:8776/v2/%(tenant_id)s
      cinder::keystone::auth::admin_url_v3: http://172.17.1.13:8776/v3/%(tenant_id)s
      cinder::keystone::auth::internal_url: http://172.17.1.13:8776/v1/%(tenant_id)s
      cinder::keystone::auth::internal_url_v2: http://172.17.1.13:8776/v2/%(tenant_id)s
      cinder::keystone::auth::internal_url_v3: http://172.17.1.13:8776/v3/%(tenant_id)s
      cinder::keystone::auth::password: VY8DpgEdM2R9VHVkTRveuzmtp
      cinder::keystone::auth::public_url: http://172.21.1.153:8776/v1/%(tenant_id)s
      cinder::keystone::auth::public_url_v2: http://172.21.1.153:8776/v2/%(tenant_id)s
      cinder::keystone::auth::public_url_v3: http://172.21.1.153:8776/v3/%(tenant_id)s
      cinder::keystone::auth::region: regionOne
      cinder::keystone::auth::tenant: service
      cinder::keystone::authtoken::auth_uri: http://172.17.1.13:5000
      cinder::keystone::authtoken::auth_url: http://172.17.1.13:5000
      cinder::keystone::authtoken::password: VY8DpgEdM2R9VHVkTRveuzmtp
      cinder::keystone::authtoken::project_domain_name: Default
      cinder::keystone::authtoken::project_name: service
      cinder::keystone::authtoken::user_domain_name: Default
      cinder::policy::policies: {}
      cinder::rabbit_heartbeat_timeout_threshold: 60
      cinder::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      cinder::rabbit_port: 5672
      cinder::rabbit_use_ssl: 'False'
      cinder::rabbit_userid: guest
      cinder::scheduler::scheduler_driver: cinder.scheduler.filter_scheduler.FilterScheduler
      cinder::volume::enabled: false
      cinder::volume::manage_service: false
      cinder::wsgi::apache::bind_host: internal_api
      cinder::wsgi::apache::servername: '%{hiera(''fqdn_internal_api'')}'
      cinder::wsgi::apache::ssl: false
      cinder::wsgi::apache::workers: '%{::os_workers}'
      corosync_ipv6: false
      corosync_token_timeout: 10000
      enable_fencing: false
      enable_galera: true
      enable_load_balancer: true
      glance::api::authtoken::auth_uri: http://172.17.1.13:5000
      glance::api::authtoken::auth_url: http://172.17.1.13:5000
      glance::api::authtoken::password: MKtQX8YpUm8dgE4nPbfAQypuQ
      glance::api::authtoken::project_name: service
      glance::api::bind_host: internal_api
      glance::api::bind_port: '9292'
      glance::api::database_connection: mysql+pymysql://glance:MKtQX8YpUm8dgE4nPbfAQypuQ@172.17.1.13/glance?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      glance::api::debug: true
      glance::api::enable_proxy_headers_parsing: true
      glance::api::enable_v1_api: false
      glance::api::enable_v2_api: true
      glance::api::enabled_import_methods: [web-download]
      glance::api::image_member_quota: 128
      glance::api::os_region_name: regionOne
      glance::api::pipeline: keystone
      glance::api::show_image_direct_url: true
      glance::api::show_multiple_locations: true
      glance::api::sync_db: false
      glance::backend::rbd::rbd_store_ceph_conf: /etc/ceph/ceph.conf
      glance::backend::rbd::rbd_store_pool: images
      glance::backend::rbd::rbd_store_user: openstack
      glance::backend::swift::swift_store_auth_address: http://172.17.1.13:5000/v3
      glance::backend::swift::swift_store_auth_version: 3
      glance::backend::swift::swift_store_create_container_on_put: true
      glance::backend::swift::swift_store_key: MKtQX8YpUm8dgE4nPbfAQypuQ
      glance::backend::swift::swift_store_user: service:glance
      glance::db::mysql::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      glance::db::mysql::dbname: glance
      glance::db::mysql::host: 172.17.1.13
      glance::db::mysql::password: MKtQX8YpUm8dgE4nPbfAQypuQ
      glance::db::mysql::user: glance
      glance::keystone::auth::admin_url: http://172.17.1.13:9292
      glance::keystone::auth::internal_url: http://172.17.1.13:9292
      glance::keystone::auth::password: MKtQX8YpUm8dgE4nPbfAQypuQ
      glance::keystone::auth::public_url: http://172.21.1.153:9292
      glance::keystone::auth::region: regionOne
      glance::keystone::auth::tenant: service
      glance::keystone::authtoken::project_domain_name: Default
      glance::keystone::authtoken::user_domain_name: Default
      glance::notify::rabbitmq::notification_driver: noop
      glance::notify::rabbitmq::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      glance::notify::rabbitmq::rabbit_port: 5672
      glance::notify::rabbitmq::rabbit_use_ssl: 'False'
      glance::notify::rabbitmq::rabbit_userid: guest
      glance::policy::policies: {}
      glance_backend: rbd
      glance_log_file: ''
      glance_notifier_strategy: noop
      hacluster_pwd: yRcCt49xfHY8vQq4
      haproxy_docker: true
      heat::api::bind_host: internal_api
      heat::api::service_name: httpd
      heat::api_cfn::bind_host: internal_api
      heat::api_cfn::service_name: httpd
      heat::cron::purge_deleted::age: '30'
      heat::cron::purge_deleted::age_type: days
      heat::cron::purge_deleted::destination: /dev/null
      heat::cron::purge_deleted::ensure: present
      heat::cron::purge_deleted::hour: '0'
      heat::cron::purge_deleted::maxdelay: '3600'
      heat::cron::purge_deleted::minute: '1'
      heat::cron::purge_deleted::month: '*'
      heat::cron::purge_deleted::monthday: '*'
      heat::cron::purge_deleted::user: heat
      heat::cron::purge_deleted::weekday: '*'
      heat::database_connection: mysql+pymysql://heat:GUg4YQVpEyX3Br4mgCsGdyrZG@172.17.1.13/heat?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      heat::db::database_db_max_retries: -1
      heat::db::database_max_retries: -1
      heat::db::mysql::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      heat::db::mysql::dbname: heat
      heat::db::mysql::host: 172.17.1.13
      heat::db::mysql::password: GUg4YQVpEyX3Br4mgCsGdyrZG
      heat::db::mysql::user: heat
      heat::debug: true
      heat::enable_proxy_headers_parsing: true
      heat::engine::auth_encryption_key: 4xUw9JNTndnUKm4fRhbCZkMm29jVsDPt
      heat::engine::configure_delegated_roles: false
      heat::engine::convergence_engine: true
      heat::engine::heat_metadata_server_url: http://172.21.1.153:8000
      heat::engine::heat_waitcondition_server_url: http://172.21.1.153:8000/v1/waitcondition
      heat::engine::max_nested_stack_depth: 6
      heat::engine::max_resources_per_stack: 1000
      heat::engine::plugin_dirs: []
      heat::engine::trusts_delegated_roles: []
      heat::heat_keystone_clients_url: http://172.21.1.153:5000
      heat::keystone::auth::admin_url: http://172.17.1.13:8004/v1/%(tenant_id)s
      heat::keystone::auth::internal_url: http://172.17.1.13:8004/v1/%(tenant_id)s
      heat::keystone::auth::password: GUg4YQVpEyX3Br4mgCsGdyrZG
      heat::keystone::auth::public_url: http://172.21.1.153:8004/v1/%(tenant_id)s
      heat::keystone::auth::region: regionOne
      heat::keystone::auth::tenant: service
      heat::keystone::auth_cfn::admin_url: http://172.17.1.13:8000/v1
      heat::keystone::auth_cfn::internal_url: http://172.17.1.13:8000/v1
      heat::keystone::auth_cfn::password: GUg4YQVpEyX3Br4mgCsGdyrZG
      heat::keystone::auth_cfn::public_url: http://172.21.1.153:8000/v1
      heat::keystone::auth_cfn::region: regionOne
      heat::keystone::auth_cfn::tenant: service
      heat::keystone::authtoken::auth_uri: http://172.17.1.13:5000
      heat::keystone::authtoken::auth_url: http://172.17.1.13:5000
      heat::keystone::authtoken::password: GUg4YQVpEyX3Br4mgCsGdyrZG
      heat::keystone::authtoken::project_domain_name: Default
      heat::keystone::authtoken::project_name: service
      heat::keystone::authtoken::user_domain_name: Default
      heat::keystone::domain::domain_admin: heat_stack_domain_admin
      heat::keystone::domain::domain_admin_email: heat_stack_domain_admin@localhost
      heat::keystone::domain::domain_name: heat_stack
      heat::keystone::domain::domain_password: jZrkjqAKg9TkPRfdZbv4pUwse
      heat::keystone_ec2_uri: http://172.17.1.13:5000/v3/ec2tokens
      heat::max_json_body_size: 4194304
      heat::notification_driver: noop
      heat::policy::policies: {}
      heat::rabbit_heartbeat_timeout_threshold: 60
      heat::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      heat::rabbit_port: 5672
      heat::rabbit_use_ssl: 'False'
      heat::rabbit_userid: guest
      heat::rpc_response_timeout: 600
      heat::wsgi::apache_api::bind_host: internal_api
      heat::wsgi::apache_api::servername: '%{hiera(''fqdn_internal_api'')}'
      heat::wsgi::apache_api::ssl: false
      heat::wsgi::apache_api_cfn::bind_host: internal_api
      heat::wsgi::apache_api_cfn::servername: '%{hiera(''fqdn_internal_api'')}'
      heat::wsgi::apache_api_cfn::ssl: false
      heat::yaql_limit_iterators: 1000
      heat::yaql_memory_quota: 100000
      horizon::allowed_hosts: ['*']
      horizon::bind_address: internal_api
      horizon::cache_backend: django.core.cache.backends.memcached.MemcachedCache
      horizon::customization_module: ''
      horizon::disable_password_reveal: true
      horizon::disallow_iframe_embed: true
      horizon::django_debug: true
      horizon::django_session_engine: django.contrib.sessions.backends.cache
      horizon::enable_secure_proxy_ssl_header: true
      horizon::enforce_password_check: true
      horizon::horizon_ca: /etc/ipa/ca.crt
      horizon::keystone_url: http://172.17.1.13:5000
      horizon::listen_ssl: false
      horizon::password_validator: ''
      horizon::password_validator_help: ''
      horizon::secret_key: j7KmAePBFT
      horizon::secure_cookies: false
      horizon::servername: '%{hiera(''fqdn_internal_api'')}'
      horizon::vhost_extra_params:
        access_log_format: '%a %l %u %t \"%r\" %>s %b \"%%{}{Referer}i\" \"%%{}{User-Agent}i\"'
        add_listen: true
        options: [FollowSymLinks, MultiViews]
        priority: 10
      kernel_modules:
        nf_conntrack: {}
        nf_conntrack_proto_sctp: {}
      keystone::admin_bind_host: '%{hiera(''fqdn_ctlplane'')}'
      keystone::admin_password: NGHXuH9pZCUkHCtFXnFeUvkUK
      keystone::admin_port: '35357'
      keystone::admin_token: Bf23pPhVcJGxKQ9ckkRM8d7my
      keystone::config::keystone_config:
        ec2/driver: {value: keystone.contrib.ec2.backends.sql.Ec2}
      keystone::credential_keys:
        /etc/keystone/credential-keys/0: {content: zdwI5DYGYVyKTTJgZ-rRy61r4IHpp0VBud3TS-_uCUg=}
        /etc/keystone/credential-keys/1: {content: C--K7T7Gx3JpYaIDmt1DWVoP59S1zslqLyryaM2c0xw=}
      keystone::cron::token_flush::destination: /var/log/keystone/keystone-tokenflush.log
      keystone::cron::token_flush::ensure: present
      keystone::cron::token_flush::hour: ['*']
      keystone::cron::token_flush::maxdelay: 0
      keystone::cron::token_flush::minute: ['1']
      keystone::cron::token_flush::month: ['*']
      keystone::cron::token_flush::monthday: ['*']
      keystone::cron::token_flush::user: keystone
      keystone::cron::token_flush::weekday: ['*']
      keystone::database_connection: mysql+pymysql://keystone:Bf23pPhVcJGxKQ9ckkRM8d7my@172.17.1.13/keystone?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      keystone::db::database_db_max_retries: -1
      keystone::db::database_max_retries: -1
      keystone::db::mysql::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      keystone::db::mysql::dbname: keystone
      keystone::db::mysql::host: 172.17.1.13
      keystone::db::mysql::password: Bf23pPhVcJGxKQ9ckkRM8d7my
      keystone::db::mysql::user: keystone
      keystone::debug: true
      keystone::enable_credential_setup: true
      keystone::enable_fernet_setup: true
      keystone::enable_proxy_headers_parsing: true
      keystone::enable_ssl: false
      keystone::endpoint::admin_url: http://192.168.120.10:35357
      keystone::endpoint::internal_url: http://172.17.1.13:5000
      keystone::endpoint::public_url: http://172.21.1.153:5000
      keystone::endpoint::region: regionOne
      keystone::endpoint::version: ''
      keystone::fernet_keys:
        /etc/keystone/fernet-keys/0: {content: IFBupcIpzoxgbrhrNXhM7m1Cl41DLrJTAiwf35L-5NI=}
        /etc/keystone/fernet-keys/1: {content: cha0Uyv0njr2ibm5HiVPKIEoiqNNtAm9g8FH9IfP5bU=}
      keystone::fernet_max_active_keys: 5
      keystone::fernet_replace_keys: true
      keystone::notification_driver: noop
      keystone::notification_format: basic
      keystone::policy::policies: {}
      keystone::public_bind_host: '%{hiera(''fqdn_internal_api'')}'
      keystone::rabbit_heartbeat_timeout_threshold: 60
      keystone::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      keystone::rabbit_port: 5672
      keystone::rabbit_use_ssl: 'False'
      keystone::rabbit_userid: guest
      keystone::roles::admin::admin_tenant: admin
      keystone::roles::admin::email: admin@example.com
      keystone::roles::admin::password: NGHXuH9pZCUkHCtFXnFeUvkUK
      keystone::roles::admin::service_tenant: service
      keystone::service_name: httpd
      keystone::token_provider: fernet
      keystone::wsgi::apache::admin_bind_host: ctlplane
      keystone::wsgi::apache::admin_port: '35357'
      keystone::wsgi::apache::bind_host: internal_api
      keystone::wsgi::apache::servername: '%{hiera(''fqdn_internal_api'')}'
      keystone::wsgi::apache::servername_admin: '%{hiera(''fqdn_ctlplane'')}'
      keystone::wsgi::apache::ssl: false
      keystone::wsgi::apache::threads: 1
      keystone::wsgi::apache::workers: '%{::os_workers}'
      keystone_enable_db_purge: true
      keystone_enable_member: true
      keystone_ssl_certificate: ''
      keystone_ssl_certificate_key: ''
      memcached::listen_ip: internal_api
      memcached::max_memory: 50%
      memcached::udp_port: 0
      memcached::verbosity: v
      memcached_ipv6: false
      memcached_network: internal_api_subnet
      mysql::server::manage_config_file: true
      mysql::server::package_name: mariadb-galera-server
      mysql::server::root_password: 2FVVmdEhs4
      mysql_bind_host: internal_api
      mysql_clustercheck_password: aGbm7XRUcTppYyaC37tr9Nzfu
      mysql_ipv6: false
      mysql_max_connections: 4096
      neutron::agents::dhcp::debug: true
      neutron::agents::dhcp::dnsmasq_dns_servers: []
      neutron::agents::dhcp::enable_force_metadata: false
      neutron::agents::dhcp::enable_isolated_metadata: false
      neutron::agents::dhcp::enable_metadata_network: false
      neutron::agents::dhcp::interface_driver: neutron.agent.linux.interface.OVSInterfaceDriver
      neutron::agents::l3::agent_mode: legacy
      neutron::agents::l3::debug: true
      neutron::agents::metadata::auth_password: G8Hfuesb4Zp2TBAwDxAdyrQyY
      neutron::agents::metadata::auth_tenant: service
      neutron::agents::metadata::auth_url: http://172.17.1.13:5000
      neutron::agents::metadata::debug: true
      neutron::agents::metadata::metadata_host: '%{hiera(''cloud_name_internal_api'')}'
      neutron::agents::metadata::metadata_ip: '%{hiera(''nova_metadata_vip'')}'
      neutron::agents::metadata::metadata_protocol: http
      neutron::agents::metadata::shared_secret: DtPZugrrjdGPC86rQTnxRW2Hp
      neutron::agents::ml2::ovs::arp_responder: false
      neutron::agents::ml2::ovs::bridge_mappings: ['datacentre:br-ex', 'floating:br-floating']
      neutron::agents::ml2::ovs::enable_distributed_routing: false
      neutron::agents::ml2::ovs::extensions: [qos]
      neutron::agents::ml2::ovs::l2_population: 'False'
      neutron::agents::ml2::ovs::local_ip: tenant
      neutron::agents::ml2::ovs::tunnel_types: [vxlan]
      neutron::allow_overlapping_ips: true
      neutron::bind_host: internal_api
      neutron::core_plugin: ml2
      neutron::db::database_db_max_retries: -1
      neutron::db::database_max_retries: -1
      neutron::db::mysql::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      neutron::db::mysql::dbname: ovs_neutron
      neutron::db::mysql::host: 172.17.1.13
      neutron::db::mysql::password: G8Hfuesb4Zp2TBAwDxAdyrQyY
      neutron::db::mysql::user: neutron
      neutron::db::sync::db_sync_timeout: 300
      neutron::db::sync::extra_params: ''
      neutron::debug: true
      neutron::dhcp_agent_notification: true
      neutron::dns_domain: openstacklocal
      neutron::global_physnet_mtu: 1500
      neutron::host: '%{::fqdn}'
      neutron::keystone::auth::admin_url: http://172.17.1.13:9696
      neutron::keystone::auth::internal_url: http://172.17.1.13:9696
      neutron::keystone::auth::password: G8Hfuesb4Zp2TBAwDxAdyrQyY
      neutron::keystone::auth::public_url: http://172.21.1.153:9696
      neutron::keystone::auth::region: regionOne
      neutron::keystone::auth::tenant: service
      neutron::keystone::authtoken::auth_uri: http://172.17.1.13:5000
      neutron::keystone::authtoken::auth_url: http://172.17.1.13:5000
      neutron::keystone::authtoken::password: G8Hfuesb4Zp2TBAwDxAdyrQyY
      neutron::keystone::authtoken::project_domain_name: Default
      neutron::keystone::authtoken::project_name: service
      neutron::keystone::authtoken::user_domain_name: Default
      neutron::notification_driver: noop
      neutron::plugins::ml2::extension_drivers: [qos, port_security]
      neutron::plugins::ml2::firewall_driver: iptables_hybrid
      neutron::plugins::ml2::flat_networks: [datacentre, floating]
      neutron::plugins::ml2::mechanism_drivers: [openvswitch]
      neutron::plugins::ml2::network_vlan_ranges: ['datacentre:1:1000']
      neutron::plugins::ml2::overlay_ip_version: 4
      neutron::plugins::ml2::tenant_network_types: [vxlan]
      neutron::plugins::ml2::tunnel_id_ranges: ['1:4094']
      neutron::plugins::ml2::type_drivers: [vxlan, vlan, flat, gre]
      neutron::plugins::ml2::vni_ranges: ['1:4094']
      neutron::policy::policies: {}
      neutron::purge_config: false
      neutron::rabbit_heartbeat_timeout_threshold: 60
      neutron::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      neutron::rabbit_port: 5672
      neutron::rabbit_use_ssl: 'False'
      neutron::rabbit_user: guest
      neutron::server::allow_automatic_l3agent_failover: 'True'
      neutron::server::database_connection: mysql+pymysql://neutron:G8Hfuesb4Zp2TBAwDxAdyrQyY@172.17.1.13/ovs_neutron?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      neutron::server::enable_dvr: false
      neutron::server::enable_proxy_headers_parsing: true
      neutron::server::notifications::auth_url: http://172.17.1.13:5000
      neutron::server::notifications::endpoint_type: internal
      neutron::server::notifications::password: dcYx3yVe7heXKRR9QnAEDRA9z
      neutron::server::notifications::project_name: service
      neutron::server::notifications::tenant_name: service
      neutron::server::router_distributed: false
      neutron::server::sync_db: true
      neutron::service_plugins: [router, qos, trunk]
      nova::api::api_bind_address: '%{hiera(''fqdn_internal_api'')}'
      nova::api::default_floating_pool: public
      nova::api::enable_proxy_headers_parsing: true
      nova::api::enabled: true
      nova::api::instance_name_template: instance-%08x
      nova::api::metadata_listen: internal_api
      nova::api::neutron_metadata_proxy_shared_secret: DtPZugrrjdGPC86rQTnxRW2Hp
      nova::api::service_name: httpd
      nova::api::sync_db_api: true
      nova::api_database_connection: mysql+pymysql://nova_api:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova_api?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::cell0_database_connection: mysql+pymysql://nova:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova_cell0?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::cinder_catalog_info: volumev3:cinderv3:internalURL
      nova::cron::archive_deleted_rows::destination: /var/log/nova/nova-rowsflush.log
      nova::cron::archive_deleted_rows::hour: '0'
      nova::cron::archive_deleted_rows::max_rows: '100'
      nova::cron::archive_deleted_rows::minute: '1'
      nova::cron::archive_deleted_rows::month: '*'
      nova::cron::archive_deleted_rows::monthday: '*'
      nova::cron::archive_deleted_rows::until_complete: false
      nova::cron::archive_deleted_rows::user: nova
      nova::cron::archive_deleted_rows::weekday: '*'
      nova::database_connection: mysql+pymysql://nova:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::db::database_db_max_retries: -1
      nova::db::database_max_retries: -1
      nova::db::mysql::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      nova::db::mysql::dbname: nova
      nova::db::mysql::host: 172.17.1.13
      nova::db::mysql::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::db::mysql::user: nova
      nova::db::mysql_api::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      nova::db::mysql_api::dbname: nova_api
      nova::db::mysql_api::host: 172.17.1.13
      nova::db::mysql_api::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::db::mysql_api::setup_cell0: true
      nova::db::mysql_api::user: nova_api
      nova::db::mysql_placement::allowed_hosts: ['%', '%{hiera(''mysql_bind_host'')}']
      nova::db::mysql_placement::dbname: nova_placement
      nova::db::mysql_placement::host: 172.17.1.13
      nova::db::mysql_placement::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::db::mysql_placement::user: nova_placement
      nova::db::sync::db_sync_timeout: 300
      nova::db::sync_api::db_sync_timeout: 300
      nova::debug: true
      nova::glance_api_servers: http://172.17.1.13:9292
      nova::host: '%{::fqdn}'
      nova::keystone::auth::admin_url: http://172.17.1.13:8774/v2.1
      nova::keystone::auth::internal_url: http://172.17.1.13:8774/v2.1
      nova::keystone::auth::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::keystone::auth::public_url: http://172.21.1.153:8774/v2.1
      nova::keystone::auth::region: regionOne
      nova::keystone::auth::tenant: service
      nova::keystone::auth_placement::admin_url: http://172.17.1.13:8778/placement
      nova::keystone::auth_placement::internal_url: http://172.17.1.13:8778/placement
      nova::keystone::auth_placement::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::keystone::auth_placement::public_url: http://172.21.1.153:8778/placement
      nova::keystone::auth_placement::region: regionOne
      nova::keystone::auth_placement::tenant: service
      nova::keystone::authtoken::auth_uri: http://172.17.1.13:5000
      nova::keystone::authtoken::auth_url: http://192.168.120.10:35357
      nova::keystone::authtoken::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::keystone::authtoken::project_domain_name: Default
      nova::keystone::authtoken::project_name: service
      nova::keystone::authtoken::user_domain_name: Default
      nova::my_ip: internal_api
      nova::network::neutron::dhcp_domain: ''
      nova::network::neutron::neutron_auth_type: v3password
      nova::network::neutron::neutron_auth_url: http://192.168.120.10:35357/v3
      nova::network::neutron::neutron_ovs_bridge: br-int
      nova::network::neutron::neutron_password: G8Hfuesb4Zp2TBAwDxAdyrQyY
      nova::network::neutron::neutron_project_name: service
      nova::network::neutron::neutron_region_name: regionOne
      nova::network::neutron::neutron_url: http://172.17.1.13:9696
      nova::network::neutron::neutron_username: neutron
      nova::notification_driver: noop
      nova::notification_format: unversioned
      nova::notify_on_state_change: vm_and_task_state
      nova::placement::auth_url: http://172.17.1.13:5000
      nova::placement::os_interface: internal
      nova::placement::os_region_name: regionOne
      nova::placement::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::placement::project_name: service
      nova::placement_database_connection: mysql+pymysql://nova_placement:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova_placement?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::policy::policies: {}
      nova::purge_config: false
      nova::rabbit_heartbeat_timeout_threshold: 60
      nova::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      nova::rabbit_port: 5672
      nova::rabbit_use_ssl: 'False'
      nova::rabbit_userid: guest
      nova::ram_allocation_ratio: '1.0'
      nova::scheduler::discover_hosts_in_cells_interval: -1
      nova::scheduler::filter::scheduler_available_filters: []
      nova::scheduler::filter::scheduler_default_filters: []
      nova::scheduler::filter::scheduler_max_attempts: 3
      nova::use_ipv6: false
      nova::vncproxy::common::vncproxy_host: 172.21.1.153
      nova::vncproxy::common::vncproxy_port: '6080'
      nova::vncproxy::common::vncproxy_protocol: http
      nova::vncproxy::enabled: true
      nova::vncproxy::host: internal_api
      nova::wsgi::apache_api::bind_host: internal_api
      nova::wsgi::apache_api::servername: '%{hiera(''fqdn_internal_api'')}'
      nova::wsgi::apache_api::ssl: false
      nova::wsgi::apache_placement::api_port: '8778'
      nova::wsgi::apache_placement::bind_host: internal_api
      nova::wsgi::apache_placement::servername: '%{hiera(''fqdn_internal_api'')}'
      nova::wsgi::apache_placement::ssl: false
      nova_enable_db_purge: true
      nova_wsgi_enabled: true
      ntp::iburst_enable: true
      'ntp::maxpoll:': 10
      'ntp::minpoll:': 6
      ntp::servers: [pool.ntp.org]
      pacemaker::corosync::cluster_name: tripleo_cluster
      pacemaker::corosync::manage_fw: false
      pacemaker::corosync::settle_tries: 360
      pacemaker::resource_defaults::defaults:
        resource-stickiness: {value: INFINITY}
      rabbit_ipv6: false
      rabbitmq::default_pass: BXxHGhfV3KNrVQDkDKYDzhvr2
      rabbitmq::default_user: guest
      rabbitmq::delete_guest_user: false
      rabbitmq::erlang_cookie: 2adrHn7hC74ResuP6kf8
      rabbitmq::file_limit: 65536
      rabbitmq::interface: internal_api
      rabbitmq::nr_ha_queues: -1
      rabbitmq::package_provider: yum
      rabbitmq::package_source: undef
      rabbitmq::port: 5672
      rabbitmq::repos_ensure: false
      rabbitmq::service_manage: false
      rabbitmq::ssl: false
      rabbitmq::ssl_depth: 1
      rabbitmq::ssl_erl_dist: false
      rabbitmq::ssl_interface: internal_api
      rabbitmq::ssl_only: false
      rabbitmq::ssl_port: 5672
      rabbitmq::tcp_keepalive: true
      rabbitmq::wipe_db_on_cookie_change: true
      rabbitmq_config_variables: {cluster_partition_handling: ignore, loopback_users: '[]',
        queue_master_locator: <<"min-masters">>}
      rabbitmq_environment: {NODE_IP_ADDRESS: '', NODE_PORT: '', RABBITMQ_NODENAME: 'rabbit@%{::hostname}',
        RABBITMQ_SERVER_ERL_ARGS: '"+K true +P 1048576 -kernel inet_default_connect_options
          [{nodelay,true}]"', export ERL_EPMD_ADDRESS: '%{hiera(''rabbitmq::interface'')}'}
      rabbitmq_kernel_variables: {inet_dist_listen_max: '25672', inet_dist_listen_min: '25672',
        net_ticktime: 15}
      redis::bind: internal_api
      redis::managed_by_cluster_manager: true
      redis::masterauth: GXqvs3zEFkWbF7Jsu4cBR96kB
      redis::notify_service: false
      redis::port: 6379
      redis::requirepass: GXqvs3zEFkWbF7Jsu4cBR96kB
      redis::sentinel::master_name: '%{hiera(''bootstrap_nodeid'')}'
      redis::sentinel::notification_script: /usr/local/bin/redis-notifications.sh
      redis::sentinel::redis_host: '%{hiera(''bootstrap_nodeid_ip'')}'
      redis::sentinel::sentinel_bind: internal_api
      redis::sentinel_auth_pass: GXqvs3zEFkWbF7Jsu4cBR96kB
      redis::service_manage: false
      redis::ulimit: '10240'
      redis_ipv6: false
      snmp::agentaddress: ['udp:161', 'udp6:[::1]:161']
      snmp::snmpd_options: -LS0-5d
      snmpd_network: internal_api_subnet
      swift::keystone::auth::admin_url: http://172.17.3.12:8080
      swift::keystone::auth::admin_url_s3: http://172.17.3.12:8080
      swift::keystone::auth::configure_s3_endpoint: false
      swift::keystone::auth::internal_url: http://172.17.3.12:8080/v1/AUTH_%(tenant_id)s
      swift::keystone::auth::internal_url_s3: http://172.17.3.12:8080
      swift::keystone::auth::operator_roles: [admin, swiftoperator, ResellerAdmin]
      swift::keystone::auth::password: pFavNu2xcWbJFKHgBatfFpn9Z
      swift::keystone::auth::public_url: http://172.21.1.153:8080/v1/AUTH_%(tenant_id)s
      swift::keystone::auth::public_url_s3: http://172.21.1.153:8080
      swift::keystone::auth::region: regionOne
      swift::keystone::auth::tenant: service
      swift::proxy::account_autocreate: true
      swift::proxy::authtoken::auth_uri: http://172.17.1.13:5000
      swift::proxy::authtoken::auth_url: http://172.17.1.13:5000
      swift::proxy::authtoken::password: pFavNu2xcWbJFKHgBatfFpn9Z
      swift::proxy::authtoken::project_name: service
      swift::proxy::keystone::operator_roles: [admin, swiftoperator, ResellerAdmin]
      swift::proxy::node_timeout: 60
      swift::proxy::pipeline: [catch_errors, healthcheck, proxy-logging, cache, ratelimit,
        bulk, tempurl, formpost, authtoken, keystone, staticweb, copy, container_quotas,
        account_quotas, slo, dlo, versioned_writes, proxy-logging, proxy-server]
      swift::proxy::port: '8080'
      swift::proxy::proxy_local_net_ip: storage
      swift::proxy::staticweb::url_base: http://172.21.1.153:8080
      swift::proxy::versioned_writes::allow_versioned_writes: true
      swift::proxy::workers: auto
      swift::storage::all::account_pipeline: [healthcheck, account-server]
      swift::storage::all::account_server_workers: auto
      swift::storage::all::container_pipeline: [healthcheck, container-server]
      swift::storage::all::container_server_workers: auto
      swift::storage::all::incoming_chmod: Du=rwx,g=rx,o=rx,Fu=rw,g=r,o=r
      swift::storage::all::mount_check: false
      swift::storage::all::object_pipeline: [healthcheck, recon, object-server]
      swift::storage::all::object_server_workers: auto
      swift::storage::all::outgoing_chmod: Du=rwx,g=rx,o=rx,Fu=rw,g=r,o=r
      swift::storage::all::storage_local_net_ip: storage_mgmt
      swift::storage::disks::args: {}
      swift::swift_hash_path_suffix: H7YmuFxUYsvWYR236nQytm2Yd
      sysctl_settings:
        fs.inotify.max_user_instances: {value: 1024}
        fs.suid_dumpable: {value: 0}
        kernel.dmesg_restrict: {value: 1}
        kernel.pid_max: {value: 1048576}
        net.core.netdev_max_backlog: {value: 10000}
        net.ipv4.conf.all.arp_accept: {value: 1}
        net.ipv4.conf.all.log_martians: {value: 1}
        net.ipv4.conf.all.secure_redirects: {value: 0}
        net.ipv4.conf.all.send_redirects: {value: 0}
        net.ipv4.conf.default.accept_redirects: {value: 0}
        net.ipv4.conf.default.log_martians: {value: 1}
        net.ipv4.conf.default.secure_redirects: {value: 0}
        net.ipv4.conf.default.send_redirects: {value: 0}
        net.ipv4.ip_forward: {value: 1}
        net.ipv4.neigh.default.gc_thresh1: {value: 1024}
        net.ipv4.neigh.default.gc_thresh2: {value: 2048}
        net.ipv4.neigh.default.gc_thresh3: {value: 4096}
        net.ipv4.tcp_keepalive_intvl: {value: 1}
        net.ipv4.tcp_keepalive_probes: {value: 5}
        net.ipv4.tcp_keepalive_time: {value: 5}
        net.ipv6.conf.all.accept_ra: {value: 0}
        net.ipv6.conf.all.accept_redirects: {value: 0}
        net.ipv6.conf.all.autoconf: {value: 0}
        net.ipv6.conf.all.disable_ipv6: {value: 0}
        net.ipv6.conf.default.accept_ra: {value: 0}
        net.ipv6.conf.default.accept_redirects: {value: 0}
        net.ipv6.conf.default.autoconf: {value: 0}
        net.ipv6.conf.default.disable_ipv6: {value: 0}
        net.netfilter.nf_conntrack_max: {value: 500000}
        net.nf_conntrack_max: {value: 500000}
      timezone::timezone: UTC
      tripleo.ceph_mgr.firewall_rules:
        113 ceph_mgr:
          dport: [6800-7300]
      tripleo.ceph_mon.firewall_rules:
        110 ceph_mon:
          dport: [6789]
      tripleo.cinder_api.firewall_rules:
        119 cinder:
          dport: [8776, 13776]
      tripleo.cinder_volume.firewall_rules:
        120 iscsi initiator: {dport: 3260}
      tripleo.glance_api.firewall_rules:
        112 glance_api:
          dport: [9292, 13292]
      tripleo.haproxy.firewall_rules:
        107 haproxy stats: {dport: 1993}
      tripleo.heat_api.firewall_rules:
        125 heat_api:
          dport: [8004, 13004]
      tripleo.heat_api_cfn.firewall_rules:
        125 heat_cfn:
          dport: [8000, 13800]
      tripleo.horizon.firewall_rules:
        127 horizon:
          dport: [80, 443]
      tripleo.keystone.firewall_rules:
        111 keystone:
          dport: [5000, 13000, '35357']
      tripleo.memcached.firewall_rules:
        121 memcached: {dport: 11211, proto: tcp, source: '%{hiera(''memcached_network'')}'}
      tripleo.mysql.firewall_rules:
        104 mysql galera-bundle:
          dport: [873, 3123, 3306, 4444, 4567, 4568, 9200]
      tripleo.neutron_api.firewall_rules:
        114 neutron api:
          dport: [9696, 13696]
      tripleo.neutron_dhcp.firewall_rules:
        115 neutron dhcp input: {dport: 67, proto: udp}
        116 neutron dhcp output: {chain: OUTPUT, dport: 68, proto: udp}
      tripleo.neutron_l3.firewall_rules:
        106 neutron_l3 vrrp: {proto: vrrp}
      tripleo.neutron_ovs_agent.firewall_rules:
        118 neutron vxlan networks: {dport: 4789, proto: udp}
        136 neutron gre networks: {proto: gre}
      tripleo.nova_api.firewall_rules:
        113 nova_api:
          dport: [8774, 13774, 8775]
      tripleo.nova_placement.firewall_rules:
        138 nova_placement:
          dport: [8778, 13778]
      tripleo.nova_vnc_proxy.firewall_rules:
        137 nova_vnc_proxy:
          dport: [6080, 13080]
      tripleo.ntp.firewall_rules:
        105 ntp: {dport: 123, proto: udp}
      tripleo.pacemaker.firewall_rules:
        130 pacemaker tcp:
          dport: [2224, 3121, 21064]
          proto: tcp
        131 pacemaker udp: {dport: 5405, proto: udp}
      tripleo.rabbitmq.firewall_rules:
        109 rabbitmq-bundle:
          dport: [3122, 4369, 5672, 25672]
      tripleo.redis.firewall_rules:
        108 redis-bundle:
          dport: [3124, 6379, 26379]
      tripleo.snmp.firewall_rules:
        124 snmp: {dport: 161, proto: udp, source: '%{hiera(''snmpd_network'')}'}
      tripleo.swift_proxy.firewall_rules:
        122 swift proxy:
          dport: [8080, 13808]
      tripleo.swift_storage.firewall_rules:
        123 swift storage:
          dport: [873, 6000, 6001, 6002]
      tripleo::fencing::config: {}
      tripleo::firewall::firewall_rules:
        300 allow ceph-mgr prometheus module:
          dport: [9283]
          proto: tcp
        301 allow prometheus node exporter:
          dport: [9100]
          proto: tcp
      tripleo::firewall::manage_firewall: true
      tripleo::firewall::purge_firewall_rules: false
      tripleo::glance::nfs_mount::edit_fstab: false
      tripleo::glance::nfs_mount::options: _netdev,bg,intr,context=system_u:object_r:glance_var_lib_t:s0
      tripleo::glance::nfs_mount::share: ''
      tripleo::haproxy::ca_bundle: /etc/ipa/ca.crt
      tripleo::haproxy::crl_file: null
      tripleo::haproxy::haproxy_log_address: /dev/log
      tripleo::haproxy::haproxy_service_manage: false
      tripleo::haproxy::haproxy_stats: true
      tripleo::haproxy::haproxy_stats_password: mWmkKyDW6fswqrsRMNF4gPFrp
      tripleo::haproxy::haproxy_stats_user: admin
      tripleo::haproxy::mysql_clustercheck: true
      tripleo::haproxy::redis_password: GXqvs3zEFkWbF7Jsu4cBR96kB
      tripleo::packages::enable_install: false
      tripleo::profile::base::cinder::cinder_enable_db_purge: true
      tripleo::profile::base::cinder::volume::cinder_enable_iscsi_backend: false
      tripleo::profile::base::cinder::volume::cinder_enable_nfs_backend: false
      tripleo::profile::base::cinder::volume::cinder_enable_rbd_backend: true
      tripleo::profile::base::cinder::volume::iscsi::cinder_iscsi_address: storage
      tripleo::profile::base::cinder::volume::iscsi::cinder_iscsi_helper: lioadm
      tripleo::profile::base::cinder::volume::iscsi::cinder_iscsi_protocol: iscsi
      tripleo::profile::base::cinder::volume::iscsi::cinder_lvm_loop_device_size: 10280
      tripleo::profile::base::cinder::volume::nfs::cinder_nas_secure_file_operations: 'False'
      tripleo::profile::base::cinder::volume::nfs::cinder_nas_secure_file_permissions: 'False'
      tripleo::profile::base::cinder::volume::nfs::cinder_nfs_mount_options: ''
      tripleo::profile::base::cinder::volume::nfs::cinder_nfs_servers: []
      tripleo::profile::base::cinder::volume::rbd::cinder_rbd_ceph_conf: /etc/ceph/ceph.conf
      tripleo::profile::base::cinder::volume::rbd::cinder_rbd_extra_pools: []
      tripleo::profile::base::cinder::volume::rbd::cinder_rbd_pool_name: volumes
      tripleo::profile::base::cinder::volume::rbd::cinder_rbd_user_name: openstack
      tripleo::profile::base::database::mysql::bind_address: '%{hiera(''fqdn_internal_api'')}'
      tripleo::profile::base::database::mysql::client::enable_ssl: false
      tripleo::profile::base::database::mysql::client::mysql_client_bind_address: internal_api
      tripleo::profile::base::database::mysql::client::ssl_ca: /etc/ipa/ca.crt
      tripleo::profile::base::database::mysql::client_bind_address: internal_api
      tripleo::profile::base::database::mysql::generate_dropin_file_limit: true
      tripleo::profile::base::database::redis::tls_proxy_bind_ip: internal_api
      tripleo::profile::base::database::redis::tls_proxy_fqdn: '%{hiera(''fqdn_internal_api'')}'
      tripleo::profile::base::database::redis::tls_proxy_port: 6379
      tripleo::profile::base::docker::additional_sockets: [/var/lib/openstack/docker.sock]
      tripleo::profile::base::docker::configure_network: true
      tripleo::profile::base::docker::debug: true
      tripleo::profile::base::docker::docker_options: --log-driver=journald --signature-verification=false
        --iptables=false --live-restore
      tripleo::profile::base::docker::insecure_registries: ['192.168.120.1:8787']
      tripleo::profile::base::docker::network_options: --bip=172.31.0.1/24
      tripleo::profile::base::glance::api::glance_nfs_enabled: false
      tripleo::profile::base::glance::api::tls_proxy_bind_ip: internal_api
      tripleo::profile::base::glance::api::tls_proxy_fqdn: '%{hiera(''fqdn_internal_api'')}'
      tripleo::profile::base::glance::api::tls_proxy_port: '9292'
      tripleo::profile::base::haproxy::certificates_specs: {}
      tripleo::profile::base::heat::manage_db_purge: true
      tripleo::profile::base::keystone::extra_notification_topics: []
      tripleo::profile::base::keystone::heat_admin_domain: heat_stack
      tripleo::profile::base::keystone::heat_admin_email: heat_stack_domain_admin@localhost
      tripleo::profile::base::keystone::heat_admin_password: jZrkjqAKg9TkPRfdZbv4pUwse
      tripleo::profile::base::keystone::heat_admin_user: heat_stack_domain_admin
      tripleo::profile::base::lvm::enable_udev: false
      tripleo::profile::base::neutron::dhcp_agent_wrappers::dnsmasq_image: 192.168.120.1:8787/rhosp13/openstack-neutron-dhcp-agent:latest
      tripleo::profile::base::neutron::dhcp_agent_wrappers::dnsmasq_process_wrapper: /var/lib/neutron/dnsmasq_wrapper
      tripleo::profile::base::neutron::dhcp_agent_wrappers::enable_dnsmasq_wrapper: true
      tripleo::profile::base::neutron::dhcp_agent_wrappers::enable_haproxy_wrapper: true
      tripleo::profile::base::neutron::dhcp_agent_wrappers::haproxy_image: 192.168.120.1:8787/rhosp13/openstack-neutron-dhcp-agent:latest
      tripleo::profile::base::neutron::dhcp_agent_wrappers::haproxy_process_wrapper: /var/lib/neutron/dhcp_haproxy_wrapper
      tripleo::profile::base::neutron::l3_agent_wrappers::dibbler_image: 192.168.120.1:8787/rhosp13/openstack-neutron-l3-agent:latest
      tripleo::profile::base::neutron::l3_agent_wrappers::dibbler_process_wrapper: /var/lib/neutron/dibbler_wrapper
      tripleo::profile::base::neutron::l3_agent_wrappers::enable_dibbler_wrapper: true
      tripleo::profile::base::neutron::l3_agent_wrappers::enable_haproxy_wrapper: true
      tripleo::profile::base::neutron::l3_agent_wrappers::enable_keepalived_wrapper: true
      tripleo::profile::base::neutron::l3_agent_wrappers::enable_radvd_wrapper: false
      tripleo::profile::base::neutron::l3_agent_wrappers::haproxy_image: 192.168.120.1:8787/rhosp13/openstack-neutron-l3-agent:latest
      tripleo::profile::base::neutron::l3_agent_wrappers::haproxy_process_wrapper: /var/lib/neutron/l3_haproxy_wrapper
      tripleo::profile::base::neutron::l3_agent_wrappers::keepalived_image: 192.168.120.1:8787/rhosp13/openstack-neutron-l3-agent:latest
      tripleo::profile::base::neutron::l3_agent_wrappers::keepalived_process_wrapper: /var/lib/neutron/keepalived_wrapper
      tripleo::profile::base::neutron::l3_agent_wrappers::keepalived_state_change_wrapper: /var/lib/neutron/keepalived_state_change_wrapper
      tripleo::profile::base::neutron::l3_agent_wrappers::radvd_image: 192.168.120.1:8787/rhosp13/openstack-neutron-l3-agent:latest
      tripleo::profile::base::neutron::l3_agent_wrappers::radvd_process_wrapper: /var/lib/neutron/radvd_wrapper
      tripleo::profile::base::neutron::server::l3_ha_override: ''
      tripleo::profile::base::neutron::server::tls_proxy_bind_ip: internal_api
      tripleo::profile::base::neutron::server::tls_proxy_fqdn: '%{hiera(''fqdn_internal_api'')}'
      tripleo::profile::base::neutron::server::tls_proxy_port: '9696'
      tripleo::profile::base::pacemaker::remote_authkey: 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
      tripleo::profile::base::rabbitmq::enable_internal_tls: false
      tripleo::profile::base::snmp::snmpd_password: dabc299cc94cdf8efe32e6f57c20a0da685fc850
      tripleo::profile::base::snmp::snmpd_user: ro_snmp_user
      tripleo::profile::base::sshd::bannertext: ''
      tripleo::profile::base::sshd::motd: ''
      tripleo::profile::base::sshd::options:
        AcceptEnv: [LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES,
          LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT, LC_IDENTIFICATION
            LC_ALL LANGUAGE, XMODIFIERS]
        AuthorizedKeysFile: .ssh/authorized_keys
        ChallengeResponseAuthentication: 'no'
        GSSAPIAuthentication: 'yes'
        GSSAPICleanupCredentials: 'no'
        HostKey: [/etc/ssh/ssh_host_rsa_key, /etc/ssh/ssh_host_ecdsa_key, /etc/ssh/ssh_host_ed25519_key]
        PasswordAuthentication: 'no'
        Subsystem: sftp  /usr/libexec/openssh/sftp-server
        SyslogFacility: AUTHPRIV
        UseDNS: 'no'
        UsePAM: 'yes'
        UsePrivilegeSeparation: sandbox
        X11Forwarding: 'yes'
      tripleo::profile::base::swift::proxy::ceilometer_enabled: false
      tripleo::profile::base::swift::proxy::ceilometer_messaging_use_ssl: 'False'
      tripleo::profile::base::swift::proxy::rabbit_port: 5672
      tripleo::profile::base::swift::proxy::tls_proxy_bind_ip: storage
      tripleo::profile::base::swift::proxy::tls_proxy_fqdn: '%{hiera(''fqdn_storage'')}'
      tripleo::profile::base::swift::proxy::tls_proxy_port: '8080'
      tripleo::profile::base::swift::ringbuilder::build_ring: true
      tripleo::profile::base::swift::ringbuilder::min_part_hours: 1
      tripleo::profile::base::swift::ringbuilder::part_power: 10
      tripleo::profile::base::swift::ringbuilder::raw_disk_prefix: r1z1-
      tripleo::profile::base::swift::ringbuilder::raw_disks: [':%PORT%/d1']
      tripleo::profile::base::swift::ringbuilder::replicas: 3
      tripleo::profile::base::swift::ringbuilder::swift_ring_get_tempurl: http://192.168.120.1:8080/v1/AUTH_d4ae8726a690413cbc62ade7e5e70763/overcloud-swift-rings/swift-rings.tar.gz?temp_url_sig=e40b72d23b006ea5722af2b5b413c2064af9b279&temp_url_expires=1537229708
      tripleo::profile::base::swift::ringbuilder::swift_ring_put_tempurl: http://192.168.120.1:8080/v1/AUTH_d4ae8726a690413cbc62ade7e5e70763/overcloud-swift-rings/swift-rings.tar.gz?temp_url_sig=776a40899cd99dad0ad7fca2860e9ec97b9003fb&temp_url_expires=1537229748
      tripleo::profile::base::swift::ringbuilder:skip_consistency_check: true
      tripleo::profile::base::swift::storage::enable_swift_storage: true
      tripleo::profile::base::swift::storage::use_local_dir: true
      tripleo::profile::base::tuned::profile: ''
      tripleo::profile::pacemaker::cinder::volume_bundle::cinder_volume_docker_image: 192.168.120.1:8787/rhosp13/openstack-cinder-volume:pcmklatest
      tripleo::profile::pacemaker::cinder::volume_bundle::docker_environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
      tripleo::profile::pacemaker::cinder::volume_bundle::docker_volumes: ['/etc/hosts:/etc/hosts:ro',
        '/etc/localtime:/etc/localtime:ro', '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro',
        '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro', '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
        '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log', '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro',
        '/etc/puppet:/etc/puppet:ro', '/var/lib/kolla/config_files/cinder_volume.json:/var/lib/kolla/config_files/config.json:ro',
        '/var/lib/config-data/puppet-generated/cinder/:/var/lib/kolla/config_files/src:ro',
        '/etc/iscsi:/var/lib/kolla/config_files/src-iscsid:ro', '/etc/ceph:/var/lib/kolla/config_files/src-ceph:ro',
        '/lib/modules:/lib/modules:ro', '/dev/:/dev/', '/run/:/run/', '/sys:/sys',
        '/var/lib/cinder:/var/lib/cinder', '/var/log/containers/cinder:/var/log/cinder']
      tripleo::profile::pacemaker::database::mysql::bind_address: '%{hiera(''fqdn_internal_api'')}'
      tripleo::profile::pacemaker::database::mysql::ca_file: /etc/ipa/ca.crt
      tripleo::profile::pacemaker::database::mysql::gmcast_listen_addr: internal_api
      tripleo::profile::pacemaker::database::mysql_bundle::bind_address: '%{hiera(''fqdn_internal_api'')}'
      tripleo::profile::pacemaker::database::mysql_bundle::control_port: 3123
      tripleo::profile::pacemaker::database::mysql_bundle::mysql_docker_image: 192.168.120.1:8787/rhosp13/openstack-mariadb:pcmklatest
      tripleo::profile::pacemaker::database::redis_bundle::control_port: 3124
      tripleo::profile::pacemaker::database::redis_bundle::redis_docker_image: 192.168.120.1:8787/rhosp13/openstack-redis:pcmklatest
      tripleo::profile::pacemaker::database::redis_bundle::tls_proxy_bind_ip: internal_api
      tripleo::profile::pacemaker::database::redis_bundle::tls_proxy_fqdn: '%{hiera(''fqdn_internal_api'')}'
      tripleo::profile::pacemaker::database::redis_bundle::tls_proxy_port: 6379
      tripleo::profile::pacemaker::haproxy_bundle::haproxy_docker_image: 192.168.120.1:8787/rhosp13/openstack-haproxy:pcmklatest
      tripleo::profile::pacemaker::haproxy_bundle::internal_certs_directory: /etc/pki/tls/certs/haproxy
      tripleo::profile::pacemaker::haproxy_bundle::internal_keys_directory: /etc/pki/tls/private/haproxy
      tripleo::profile::pacemaker::haproxy_bundle::tls_mapping: [/etc/ipa/ca.crt,
        /etc/pki/tls/private/haproxy, /etc/pki/tls/certs/haproxy, /etc/pki/tls/private/overcloud_endpoint.pem]
      tripleo::profile::pacemaker::rabbitmq_bundle::control_port: 3122
      tripleo::profile::pacemaker::rabbitmq_bundle::rabbitmq_docker_image: 192.168.120.1:8787/rhosp13/openstack-rabbitmq:pcmklatest
      tripleo::stunnel::foreground: 'yes'
      tripleo::stunnel::manage_service: false
      tripleo::trusted_cas::ca_map: {}
      vswitch::ovs::enable_hw_offload: false
    role_data_monitoring_subscriptions: [overcloud-pacemaker]
    role_data_post_update_tasks: []
    role_data_post_upgrade_tasks:
    - getent: {database: passwd, key: neutron}
      ignore_errors: true
      name: Check for neutron user
    - name: Set neutron_user_avail
      set_fact: {neutron_user_avail: '{{ getent_passwd is defined }}'}
    - block:
      - {become: true, name: Ensure read/write access for files created after upgrade,
        shell: 'umask 0002

          setfacl -d -R -m u:neutron:rwx /var/lib/neutron

          setfacl -R -m u:neutron:rw /var/lib/neutron

          find /var/lib/neutron -type d -exec setfacl -m u:neutron:rwx ''{}'' \;

          '}
      - become: true
        ignore_errors: true
        name: Provide access for domain sockets
        shell: 'umask 0002

          setfacl -m u:neutron:rwx "{{ item }}"

          '
        with_items: [/var/lib/neutron/metadata_proxy, /var/lib/neutron]
      when: [step|int == 2, neutron_user_avail|bool]
    - block:
      - {become: true, name: Ensure r/w access for existing files after upgrade, shell: 'umask
          0002

          setfacl -d -R -m u:neutron:rwx /var/lib/neutron

          setfacl -R -m u:neutron:rw /var/lib/neutron

          find /var/lib/neutron -type d -exec setfacl -m u:neutron:rwx ''{}'' \;

          '}
      - become: true
        ignore_errors: true
        name: Provide access to domain sockets
        shell: 'umask 0002

          setfacl -m u:neutron:rwx "{{ item }}"

          '
        with_items: [/var/lib/neutron/metadata_proxy, /var/lib/neutron/keepalived-state-change,
          /var/lib/neutron]
      when: [step|int == 2, neutron_user_avail|bool]
    role_data_pre_upgrade_rolling_tasks: []
    role_data_puppet_config:
    - {config_image: '', config_volume: '', step_config: ''}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-cinder-api:latest', config_volume: cinder,
      puppet_tags: 'cinder_config,file,concat,file_line', step_config: 'include ::tripleo::profile::base::cinder::api


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-cinder-api:latest', config_volume: cinder,
      puppet_tags: 'cinder_config,file,concat,file_line', step_config: 'include ::tripleo::profile::base::cinder::scheduler


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-cinder-api:latest', config_volume: cinder,
      puppet_tags: 'cinder_config,file,concat,file_line', step_config: 'include ::tripleo::profile::base::lvm

        include ::tripleo::profile::base::cinder::volume


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-mariadb:latest', config_volume: clustercheck,
      puppet_tags: file, step_config: 'include ::tripleo::profile::pacemaker::clustercheck'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-glance-api:latest', config_volume: glance_api,
      puppet_tags: 'glance_api_config,glance_api_paste_ini,glance_swift_config,glance_cache_config',
      step_config: 'include ::tripleo::profile::base::glance::api


        include ::tripleo::profile::base::database::mysql::client'}
    - config_image: 192.168.120.1:8787/rhosp13/openstack-haproxy:latest
      config_volume: haproxy
      puppet_tags: haproxy_config
      step_config: 'exec {''wait-for-settle'': command => ''/bin/true'' }

        class tripleo::firewall(){}; define tripleo::firewall::rule( $port = undef,
        $dport = undef, $sport = undef, $proto = undef, $action = undef, $state =
        undef, $source = undef, $iniface = undef, $chain = undef, $destination = undef,
        $extras = undef){}

        [''pcmk_bundle'', ''pcmk_resource'', ''pcmk_property'', ''pcmk_constraint'',
        ''pcmk_resource_default''].each |String $val| { noop_resource($val) }

        include ::tripleo::profile::pacemaker::haproxy_bundle'
      volumes: ['/etc/ipa/ca.crt:/etc/ipa/ca.crt:ro', '/etc/pki/tls/private/haproxy:/etc/pki/tls/private/haproxy:ro',
        '/etc/pki/tls/certs/haproxy:/etc/pki/tls/certs/haproxy:ro', '/etc/pki/tls/private/overcloud_endpoint.pem:/etc/pki/tls/private/overcloud_endpoint.pem:ro']
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-heat-api:latest', config_volume: heat_api,
      puppet_tags: 'heat_config,file,concat,file_line', step_config: 'include ::tripleo::profile::base::heat::api

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-heat-api-cfn:latest', config_volume: heat_api_cfn,
      puppet_tags: 'heat_config,file,concat,file_line', step_config: 'include ::tripleo::profile::base::heat::api_cfn

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-heat-api:latest', config_volume: heat,
      puppet_tags: 'heat_config,file,concat,file_line', step_config: 'include ::tripleo::profile::base::heat::engine


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-horizon:latest', config_volume: horizon,
      puppet_tags: horizon_config, step_config: 'include ::tripleo::profile::base::horizon

        '}
    - config_image: 192.168.120.1:8787/rhosp13/openstack-iscsid:latest
      config_volume: iscsid
      puppet_tags: iscsid_config
      step_config: include ::tripleo::profile::base::iscsid
      volumes: ['/etc/iscsi:/etc/iscsi']
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-keystone:latest', config_volume: keystone,
      puppet_tags: 'keystone_config,keystone_domain_config', step_config: '[''Keystone_user'',
        ''Keystone_endpoint'', ''Keystone_domain'', ''Keystone_tenant'', ''Keystone_user_role'',
        ''Keystone_role'', ''Keystone_service''].each |String $val| { noop_resource($val)
        }

        include ::tripleo::profile::base::keystone


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-memcached:latest', config_volume: memcached,
      puppet_tags: file, step_config: 'include ::tripleo::profile::base::memcached

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-mariadb:latest', config_volume: mysql,
      puppet_tags: file, step_config: '[''Mysql_datadir'', ''Mysql_user'', ''Mysql_database'',
        ''Mysql_grant'', ''Mysql_plugin''].each |String $val| { noop_resource($val)
        }

        exec {''wait-for-settle'': command => ''/bin/true'' }

        include ::tripleo::profile::pacemaker::database::mysql_bundle'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-neutron-server:latest',
      config_volume: neutron, puppet_tags: 'neutron_config,neutron_api_config', step_config: 'include
        tripleo::profile::base::neutron::server


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-neutron-server:latest',
      config_volume: neutron, puppet_tags: neutron_plugin_ml2, step_config: 'include
        ::tripleo::profile::base::neutron::plugins::ml2

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-neutron-server:latest',
      config_volume: neutron, puppet_tags: 'neutron_config,neutron_dhcp_agent_config',
      step_config: 'include tripleo::profile::base::neutron::dhcp

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-neutron-server:latest',
      config_volume: neutron, puppet_tags: 'neutron_config,neutron_l3_agent_config',
      step_config: 'include tripleo::profile::base::neutron::l3

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-neutron-server:latest',
      config_volume: neutron, puppet_tags: 'neutron_config,neutron_metadata_agent_config',
      step_config: 'include tripleo::profile::base::neutron::metadata

        '}
    - config_image: 192.168.120.1:8787/rhosp13/openstack-neutron-server:latest
      config_volume: neutron
      puppet_tags: neutron_config,neutron_agent_ovs,neutron_plugin_ml2
      step_config: 'include ::tripleo::profile::base::neutron::ovs

        '
      volumes: ['/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch']
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-api:latest', config_volume: nova,
      puppet_tags: nova_config, step_config: '[''Nova_cell_v2''].each |String $val|
        { noop_resource($val) }

        include tripleo::profile::base::nova::api


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-api:latest', config_volume: nova,
      puppet_tags: nova_config, step_config: 'include tripleo::profile::base::nova::conductor


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-api:latest', config_volume: nova,
      puppet_tags: nova_config, step_config: 'include tripleo::profile::base::nova::consoleauth


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-api:latest', config_volume: nova,
      puppet_tags: nova_config, step_config: ''}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-placement-api:latest',
      config_volume: nova_placement, puppet_tags: nova_config, step_config: 'include
        tripleo::profile::base::nova::placement


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-api:latest', config_volume: nova,
      puppet_tags: nova_config, step_config: 'include tripleo::profile::base::nova::scheduler


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-api:latest', config_volume: nova,
      puppet_tags: nova_config, step_config: 'include tripleo::profile::base::nova::vncproxy


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-cron:latest', config_volume: crond,
      step_config: 'include ::tripleo::profile::base::logging::logrotate'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest', config_volume: rabbitmq,
      puppet_tags: file, step_config: '[''Rabbitmq_policy'', ''Rabbitmq_user''].each
        |String $val| { noop_resource($val) }

        include ::tripleo::profile::base::rabbitmq

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-redis:latest', config_volume: redis,
      puppet_tags: exec, step_config: 'include ::tripleo::profile::pacemaker::database::redis_bundle'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-swift-proxy-server:latest',
      config_volume: swift, puppet_tags: 'swift_config,swift_proxy_config,swift_keymaster_config',
      step_config: 'include ::tripleo::profile::base::swift::proxy

        '}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-swift-proxy-server:latest',
      config_volume: swift_ringbuilder, puppet_tags: 'exec,fetch_swift_ring_tarball,extract_swift_ring_tarball,ring_object_device,swift::ringbuilder::create,tripleo::profile::base::swift::add_devices,swift::ringbuilder::rebalance,create_swift_ring_tarball,upload_swift_ring_tarball',
      step_config: 'include ::tripleo::profile::base::swift::ringbuilder'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-swift-proxy-server:latest',
      config_volume: swift, puppet_tags: 'swift_config,swift_container_config,swift_container_sync_realms_config,swift_account_config,swift_object_config,swift_object_expirer_config,rsync::server',
      step_config: 'include ::tripleo::profile::base::swift::storage


        class xinetd() {}'}
    role_data_service_config_settings: {}
    role_data_service_metadata_settings: null
    role_data_service_names: [ca_certs, ceilometer_api_disabled, ceilometer_collector_disabled,
      ceilometer_expirer_disabled, ceph_mgr, ceph_mon, cinder_api, cinder_scheduler,
      cinder_volume, clustercheck, docker, glance_api, glance_registry_disabled, haproxy,
      heat_api, heat_api_cloudwatch_disabled, heat_api_cfn, heat_engine, horizon,
      iscsid, kernel, keystone, memcached, mongodb_disabled, mysql, mysql_client,
      neutron_api, neutron_plugin_ml2, neutron_dhcp, neutron_l3, neutron_metadata,
      neutron_ovs_agent, nova_api, nova_conductor, nova_consoleauth, nova_metadata,
      nova_placement, nova_scheduler, nova_vnc_proxy, ntp, logrotate_crond, pacemaker,
      rabbitmq, redis, snmp, sshd, swift_proxy, swift_ringbuilder, swift_storage,
      timezone, tripleo_firewall, tripleo_packages, tuned]
    role_data_step_config: "# Copyright 2014 Red Hat, Inc.\n# All Rights Reserved.\n\
      #\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n\
      # not use this file except in compliance with the License. You may obtain\n\
      # a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n\
      #\n# Unless required by applicable law or agreed to in writing, software\n#\
      \ distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\
      # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\
      # License for the specific language governing permissions and limitations\n\
      # under the License.\n\n# Common config, from tripleo-heat-templates/puppet/manifests/overcloud_common.pp\n\
      # The content of this file will be used to generate\n# the puppet manifests\
      \ for all roles, the placeholder\n# Controller will be replaced by 'controller',\
      \ 'blockstorage',\n# 'cephstorage' and all the deployed roles.\n\nif hiera('step')\
      \ >= 4 {\n  hiera_include('Controller_classes', [])\n}\n\n$package_manifest_name\
      \ = join(['/var/lib/tripleo/installed-packages/overcloud_Controller', hiera('step')])\n\
      package_manifest{$package_manifest_name: ensure => present}\n\n# End of overcloud_common.pp\n\
      \ninclude ::tripleo::trusted_cas\ninclude ::tripleo::profile::base::docker\n\
      \ninclude ::tripleo::profile::base::kernel\ninclude ::tripleo::profile::base::database::mysql::client\n\
      include ::tripleo::profile::base::time::ntp\ninclude ::tripleo::profile::base::pacemaker\n\
      \ninclude ::tripleo::profile::base::snmp\n\ninclude ::tripleo::profile::base::sshd\n\
      \ninclude ::timezone\ninclude ::tripleo::firewall\n\ninclude ::tripleo::packages\n\
      \ninclude ::tripleo::profile::base::tuned"
    role_data_update_tasks:
    - block:
      - name: Get docker Cinder-Volume image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-cinder-volume:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-cinder-volume:pcmklatest'}
      - {name: Get previous Cinder-Volume image id, register: cinder_volume_image_id,
        shell: 'docker images | awk ''/cinder-volume.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Cinder-Volume image, register: cinder_volume_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{cinder_volume_image_id.stdout}}'''}
        - {name: Remove any container using the same Cinder-Volume image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ cinder_volume_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Cinder-Volume images, shell: 'docker rmi -f {{cinder_volume_image_id.stdout}}'}
        when: [cinder_volume_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Cinder-Volume
          images}
      - {name: Retag pcmklatest to latest Cinder-Volume image, shell: 'docker tag
          {{docker_image}} {{docker_image_latest}}'}
      name: Cinder-Volume fetch and retag container image for pacemaker
      when: step|int == 2
    - block:
      - {failed_when: false, name: Detect if puppet on the docker profile would restart
          the service, register: puppet_docker_noop_output, shell: "puppet apply --noop\
          \ --summarize --detailed-exitcodes --verbose \\\n  --modulepath /etc/puppet/modules:/opt/stack/puppet-modules:/usr/share/openstack-puppet/modules\
          \ \\\n  --color=false -e \"class { 'tripleo::profile::base::docker': step\
          \ => 1, }\" 2>&1 | \\\nawk -F \":\" '/Out of sync:/ { print $2}'\n"}
      - {changed_when: docker_check_update.rc == 100, failed_when: 'docker_check_update.rc
          not in [0, 100]', name: Is docker going to be updated, register: docker_check_update,
        shell: yum check-update docker}
      - {name: Set docker_rpm_needs_update fact, set_fact: 'docker_rpm_needs_update={{
          docker_check_update.rc == 100 }}'}
      - {name: Set puppet_docker_is_outofsync fact, set_fact: 'puppet_docker_is_outofsync={{
          puppet_docker_noop_output.stdout|trim|int >= 1 }}'}
      - {name: Stop all containers, shell: docker ps -q | xargs --no-run-if-empty
          -n1 docker stop, when: puppet_docker_is_outofsync or docker_rpm_needs_update}
      - name: Stop docker
        service: {name: docker, state: stopped}
        when: puppet_docker_is_outofsync or docker_rpm_needs_update
      - {name: Update the docker package, when: docker_rpm_needs_update, yum: name=docker
          state=latest update_cache=yes}
      - {changed_when: puppet_docker_apply.rc == 2, failed_when: 'puppet_docker_apply.rc
          not in [0, 2]', name: Apply puppet which will start the service again, register: puppet_docker_apply,
        shell: "puppet apply --detailed-exitcodes --verbose \\\n  --modulepath  /etc/puppet/modules:/opt/stack/puppet-modules:/usr/share/openstack-puppet/modules\
          \ \\\n  -e \"class { 'tripleo::profile::base::docker': step => 1, }\"\n"}
      when: step|int == 2
    - block:
      - name: Check for haproxy Kolla configuration
        register: haproxy_kolla_config
        stat: {path: /var/lib/config-data/puppet-generated/haproxy}
      - name: Check if haproxy is already containerized
        set_fact: {haproxy_containerized: '{{haproxy_kolla_config.stat.isdir | default(false)}}'}
      - {command: hiera -c /etc/puppet/hiera.yaml bootstrap_nodeid, name: get bootstrap
          nodeid, register: bootstrap_node, tags: common}
      - {name: set is_bootstrap_node fact, set_fact: 'is_bootstrap_node={{bootstrap_node.stdout|lower
          == ansible_hostname|lower}}', tags: common}
      name: Set HAProxy upgrade facts
    - block:
      - {command: 'cibadmin --query --xpath "//storage-mapping[@id=''haproxy-cert'']"',
        ignore_errors: true, name: Check haproxy public certificate configuration
          in pacemaker, register: haproxy_cert_mounted}
      - name: Disable the haproxy cluster resource
        pacemaker_resource: {resource: haproxy-bundle, state: disable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
        when: haproxy_cert_mounted.rc == 6
      - name: Set HAProxy public cert volume mount fact
        set_fact: {haproxy_public_cert_path: /etc/pki/tls/private/overcloud_endpoint.pem,
          haproxy_public_tls_enabled: false}
      - {command: 'pcs resource bundle update haproxy-bundle storage-map add id=haproxy-cert
          source-dir={{ haproxy_public_cert_path }} target-dir=/var/lib/kolla/config_files/src-tls/{{
          haproxy_public_cert_path }} options=ro', name: Add a bind mount for public
          certificate in the haproxy bundle, when: haproxy_cert_mounted.rc == 6 and
          haproxy_public_tls_enabled|bool}
      - name: Enable the haproxy cluster resource
        pacemaker_resource: {resource: haproxy-bundle, state: enable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
        when: haproxy_cert_mounted.rc == 6
      name: Mount TLS cert if needed
      when: [step|int == 1, haproxy_containerized|bool, is_bootstrap_node]
    - block:
      - name: Get docker Haproxy image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-haproxy:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-haproxy:pcmklatest'}
      - {name: Get previous Haproxy image id, register: haproxy_image_id, shell: 'docker
          images | awk ''/haproxy.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Haproxy image, register: haproxy_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{haproxy_image_id.stdout}}'''}
        - {name: Remove any container using the same Haproxy image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ haproxy_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Haproxy images, shell: 'docker rmi -f {{haproxy_image_id.stdout}}'}
        when: [haproxy_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Haproxy images}
      - {name: Retag pcmklatest to latest Haproxy image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Haproxy fetch and retag container image for pacemaker
      when: step|int == 2
    - block:
      - name: Get docker Mariadb image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-mariadb:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-mariadb:pcmklatest'}
      - {name: Get previous Mariadb image id, register: mariadb_image_id, shell: 'docker
          images | awk ''/mariadb.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Mariadb image, register: mariadb_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{mariadb_image_id.stdout}}'''}
        - {name: Remove any container using the same Mariadb image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ mariadb_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Mariadb images, shell: 'docker rmi -f {{mariadb_image_id.stdout}}'}
        when: [mariadb_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Mariadb images}
      - {name: Retag pcmklatest to latest Mariadb image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Mariadb fetch and retag container image for pacemaker
      when: step|int == 2
    - {lineinfile: dest=/etc/sysconfig/iptables regexp=".*neutron-" state=absent,
      name: Remove IPv4 iptables rules created by Neutron that are persistent, when: step|int
        == 5}
    - {lineinfile: dest=/etc/sysconfig/ip6tables regexp=".*neutron-" state=absent,
      name: Remove IPv6 iptables rules created by Neutron that are persistent, when: step|int
        == 5}
    - {async: 30, name: Check pacemaker cluster running before the minor update, pacemaker_cluster: state=online
        check_and_fail=true, poll: 4, when: step|int == 0}
    - {name: Stop pacemaker cluster, pacemaker_cluster: state=offline, when: step|int
        == 1}
    - {name: Start pacemaker cluster, pacemaker_cluster: state=online, when: step|int
        == 4}
    - block:
      - name: Get docker Rabbitmq image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-rabbitmq:pcmklatest'}
      - {name: Get previous Rabbitmq image id, register: rabbitmq_image_id, shell: 'docker
          images | awk ''/rabbitmq.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Rabbitmq image, register: rabbitmq_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{rabbitmq_image_id.stdout}}'''}
        - {name: Remove any container using the same Rabbitmq image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ rabbitmq_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Rabbitmq images, shell: 'docker rmi -f {{rabbitmq_image_id.stdout}}'}
        when: [rabbitmq_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Rabbitmq images}
      - {name: Retag pcmklatest to latest Rabbitmq image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Rabbit fetch and retag container image for pacemaker
      when: step|int == 2
    - block:
      - name: Get docker Redis image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-redis:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-redis:pcmklatest'}
      - {name: Get previous Redis image id, register: redis_image_id, shell: 'docker
          images | awk ''/redis.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Redis image, register: redis_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{redis_image_id.stdout}}'''}
        - {name: Remove any container using the same Redis image, shell: 'docker rm
            -fv {{item}}', with_items: '{{ redis_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Redis images, shell: 'docker rmi -f {{redis_image_id.stdout}}'}
        when: [redis_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Redis images}
      - {name: Retag pcmklatest to latest Redis image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Redis fetch and retag container image for pacemaker
      when: step|int == 2
    - file: {path: /var/run/rsyncd.pid, state: absent}
      name: Ensure rsyncd pid file is absent
    - {name: Check for existing yum.pid, register: yum_pid_file, stat: path=/var/run/yum.pid,
      when: step|int == 0 or step|int == 3}
    - {fail: msg="ERROR existing yum.pid detected - can't continue! Please ensure
        there is no other package update process for the duration of the minor update
        worfklow. Exiting.", name: Exit if existing yum process, when: (step|int ==
        0 or step|int == 3) and yum_pid_file.stat.exists}
    - {name: Update all packages, when: step == "3", yum: name=* state=latest update_cache=yes}
    role_data_upgrade_batch_tasks: []
    role_data_upgrade_tasks:
    - {command: systemctl is-enabled openstack-cinder-api, ignore_errors: true, name: Check
        is cinder_api is deployed, register: cinder_api_enabled, tags: common}
    - name: 'PreUpgrade step0,validation: Check service openstack-cinder-api is running'
      shell: systemctl is-active --quiet openstack-cinder-api
      tags: validation
      when: [step|int == 0, cinder_api_enabled.rc == 0]
    - name: Stop and disable cinder_api service (pre-upgrade not under httpd)
      service: name=openstack-cinder-api state=stopped enabled=no
      when: [step|int == 2, cinder_api_enabled.rc == 0]
    - {ignore_errors: true, name: check for cinder_api running under apache (post
        upgrade), register: cinder_api_apache, shell: httpd -t -D DUMP_VHOSTS | grep
        -q cinder, when: step|int == 2}
    - name: Stop and disable cinder_api service
      service: name=httpd state=stopped enabled=no
      when: [step|int == 2, cinder_api_apache.rc == 0]
    - file: {path: /var/spool/cron/cinder, state: absent}
      name: remove old cinder cron jobs
      when: step|int == 2
    - name: Set fact for removal of httpd package
      set_fact: {remove_httpd_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove httpd package if operator requests it
      when: [step|int == 2, remove_httpd_package|bool]
      yum: name=httpd state=removed
    - {command: systemctl is-enabled openstack-cinder-scheduler, ignore_errors: true,
      name: Check if cinder_scheduler is deployed, register: cinder_scheduler_enabled,
      tags: common}
    - name: 'PreUpgrade step0,validation: Check service openstack-cinder-scheduler
        is running'
      shell: systemctl is-active --quiet openstack-cinder-scheduler
      tags: validation
      when: [step|int == 0, cinder_scheduler_enabled.rc == 0]
    - name: Stop and disable cinder_scheduler service
      service: name=openstack-cinder-scheduler state=stopped enabled=no
      when: [step|int == 2, cinder_scheduler_enabled.rc == 0]
    - name: Set fact for removal of openstack-cinder package
      set_fact: {remove_cinder_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-cinder package if operator requests it
      when: [step|int == 2, remove_cinder_package|bool]
      yum: name=openstack-cinder state=removed
    - name: Get docker Cinder-Volume image
      set_fact: {cinder_volume_docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-cinder-volume:pcmklatest'}
    - {changed_when: false, command: 'grep ''^volume_driver[ \t]*='' /var/lib/config-data/puppet-generated/cinder/etc/cinder/cinder.conf',
      ignore_errors: true, name: Check for Cinder-Volume Kolla configuration, register: cinder_volume_kolla_config}
    - name: Check if Cinder-Volume is already containerized
      set_fact: {cinder_volume_containerized: '{{cinder_volume_kolla_config|succeeded}}'}
    - block:
      - {command: hiera -c /etc/puppet/hiera.yaml bootstrap_nodeid, name: get bootstrap
          nodeid, register: bootstrap_node, tags: common}
      - {name: set is_bootstrap_node fact, set_fact: 'is_bootstrap_node={{bootstrap_node.stdout|lower
          == ansible_hostname|lower}}', tags: common}
      - ignore_errors: true
        name: Check cluster resource status
        pacemaker_resource: {check_mode: false, resource: openstack-cinder-volume,
          state: show}
        register: cinder_volume_res
      - block:
        - name: Disable the openstack-cinder-volume cluster resource
          pacemaker_resource: {resource: openstack-cinder-volume, state: disable,
            wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        - name: Delete the stopped openstack-cinder-volume cluster resource.
          pacemaker_resource: {resource: openstack-cinder-volume, state: delete, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        when: (is_bootstrap_node) and (cinder_volume_res|succeeded)
      - {name: Disable cinder_volume service from boot, service: name=openstack-cinder-volume
          enabled=no}
      name: Cinder-Volume baremetal to container upgrade tasks
      when: [step|int == 1, not cinder_volume_containerized|bool]
    - block:
      - {name: Get cinder_volume image id currently used by pacemaker, register: cinder_volume_current_pcmklatest_id,
        shell: 'docker images | awk ''/cinder-volume.* pcmklatest/{print $3}'' | uniq'}
      - {name: Temporarily tag the current cinder_volume image id with the upgraded
          image name, shell: 'docker tag {{cinder_volume_current_pcmklatest_id.stdout}}
          {{cinder_volume_docker_image_latest}}'}
      name: Prepare the switch to new cinder_volume container image name in pacemaker
      when: [step|int == 0, cinder_volume_containerized|bool]
    - ignore_errors: true
      name: Check openstack-cinder-volume cluster resource status
      pacemaker_resource: {check_mode: false, resource: openstack-cinder-volume, state: show}
      register: cinder_volume_pcs_res
    - block:
      - name: Disable the cinder_volume cluster resource before container upgrade
        pacemaker_resource: {resource: openstack-cinder-volume, state: disable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      - {command: 'pcs resource bundle update openstack-cinder-volume container image={{cinder_volume_docker_image_latest}}',
        name: pcs resource bundle update cinder_volume for new container image name}
      - name: Enable the cinder_volume cluster resource
        pacemaker_resource: {resource: openstack-cinder-volume, state: enable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
        when: null
      name: Update cinder_volume pcs resource bundle for new container image
      when: [step|int == 1, cinder_volume_containerized|bool, is_bootstrap_node, cinder_volume_pcs_res|succeeded]
    - block:
      - name: Get docker Cinder-Volume image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-cinder-volume:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-cinder-volume:pcmklatest'}
      - {name: Get previous Cinder-Volume image id, register: cinder_volume_image_id,
        shell: 'docker images | awk ''/cinder-volume.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Cinder-Volume image, register: cinder_volume_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{cinder_volume_image_id.stdout}}'''}
        - {name: Remove any container using the same Cinder-Volume image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ cinder_volume_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Cinder-Volume images, shell: 'docker rmi -f {{cinder_volume_image_id.stdout}}'}
        when: [cinder_volume_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Cinder-Volume
          images}
      - {name: Retag pcmklatest to latest Cinder-Volume image, shell: 'docker tag
          {{docker_image}} {{docker_image_latest}}'}
      name: Retag the pacemaker image if containerized
      when: [step|int == 3, cinder_volume_containerized|bool]
    - {name: Install docker packages on upgrade if missing, when: step|int == 3, yum: name=docker
        state=latest}
    - {command: systemctl is-enabled --quiet openstack-glance-api, ignore_errors: true,
      name: Check if glance_api is deployed, register: glance_api_enabled, tags: common}
    - command: systemctl is-active --quiet openstack-glance-api
      name: 'PreUpgrade step0,validation: Check service openstack-glance-api is running'
      tags: validation
      when: [step|int == 0, glance_api_enabled.rc == 0]
    - name: Stop and disable glance_api service
      service: name=openstack-glance-api state=stopped enabled=no
      when: [step|int == 2, glance_api_enabled.rc == 0]
    - name: Set fact for removal of openstack-glance package
      set_fact: {remove_glance_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-glance package if operator requests it
      when: [step|int == 2, remove_glance_package|bool]
      yum: name=openstack-glance state=removed
    - {name: Stop and disable glance_registry service on upgrade, service: name=openstack-glance-registry
        state=stopped enabled=no, when: step|int == 1}
    - name: Get docker haproxy image
      set_fact: {haproxy_docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-haproxy:pcmklatest'}
    - block:
      - name: Check for haproxy Kolla configuration
        register: haproxy_kolla_config
        stat: {path: /var/lib/config-data/puppet-generated/haproxy}
      - name: Check if haproxy is already containerized
        set_fact: {haproxy_containerized: '{{haproxy_kolla_config.stat.isdir | default(false)}}'}
      - {command: hiera -c /etc/puppet/hiera.yaml bootstrap_nodeid, name: get bootstrap
          nodeid, register: bootstrap_node, tags: common}
      - {name: set is_bootstrap_node fact, set_fact: 'is_bootstrap_node={{bootstrap_node.stdout|lower
          == ansible_hostname|lower}}', tags: common}
      name: Set HAProxy upgrade facts
    - block:
      - ignore_errors: true
        name: Check cluster resource status
        pacemaker_resource: {check_mode: true, resource: haproxy, state: started}
        register: haproxy_res
      - block:
        - name: Disable the haproxy cluster resource.
          pacemaker_resource: {resource: haproxy, state: disable, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        - name: Delete the stopped haproxy cluster resource.
          pacemaker_resource: {resource: haproxy, state: delete, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        when: (is_bootstrap_node) and (haproxy_res|succeeded)
      name: haproxy baremetal to container upgrade tasks
      when: [step|int == 1, not haproxy_containerized|bool]
    - block:
      - {name: Get haproxy image id currently used by pacemaker, register: haproxy_current_pcmklatest_id,
        shell: 'docker images | awk ''/haproxy.* pcmklatest/{print $3}'' | uniq'}
      - {name: Temporarily tag the current haproxy image id with the upgraded image
          name, shell: 'docker tag {{haproxy_current_pcmklatest_id.stdout}} {{haproxy_docker_image_latest}}'}
      name: Prepare the switch to new haproxy container image name in pacemaker
      when: [step|int == 0, haproxy_containerized|bool]
    - ignore_errors: true
      name: Check haproxy-bundle cluster resource status
      pacemaker_resource: {check_mode: false, resource: haproxy-bundle, state: show}
      register: haproxy_pcs_res
    - block:
      - name: Disable the haproxy cluster resource before container upgrade
        pacemaker_resource: {resource: haproxy-bundle, state: disable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      - block:
        - {command: 'cibadmin --query --xpath "//storage-mapping[@id=''haproxy-var-lib'']"',
          ignore_errors: true, name: Check haproxy stats socket configuration in pacemaker,
          register: haproxy_stats_exposed}
        - {command: 'cibadmin --query --xpath "//storage-mapping[@id=''haproxy-cert'']"',
          ignore_errors: true, name: Check haproxy public certificate configuration
            in pacemaker, register: haproxy_cert_mounted}
        - {command: pcs resource bundle update haproxy-bundle storage-map add id=haproxy-var-lib
            source-dir=/var/lib/haproxy target-dir=/var/lib/haproxy options=rw, name: Add
            a bind mount for stats socket in the haproxy bundle, when: haproxy_stats_exposed.rc
            == 6}
        - name: Set HAProxy public cert volume mount fact
          set_fact: {haproxy_public_cert_path: /etc/pki/tls/private/overcloud_endpoint.pem,
            haproxy_public_tls_enabled: false}
        - command: pcs resource bundle update haproxy-bundle storage-map add id=haproxy-cert
            source-dir={{ haproxy_public_cert_path }} target-dir=/var/lib/kolla/config_files/src-tls/{{
            haproxy_public_cert_path }} options=ro
          name: Add a bind mount for public certificate in the haproxy bundle
          when: [haproxy_cert_mounted.rc == 6, haproxy_public_tls_enabled|bool]
        name: Expose HAProxy stats socket on the host and mount TLS cert if needed
      - {command: 'pcs resource bundle update haproxy-bundle container image={{haproxy_docker_image_latest}}',
        name: Update the haproxy bundle to use the new container image name}
      - name: Enable the haproxy cluster resource
        pacemaker_resource: {resource: haproxy-bundle, state: enable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      name: Update haproxy pcs resource bundle for new container image
      when: [step|int == 1, haproxy_containerized|bool, is_bootstrap_node, haproxy_pcs_res|succeeded]
    - block:
      - name: Get docker Haproxy image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-haproxy:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-haproxy:pcmklatest'}
      - {name: Get previous Haproxy image id, register: haproxy_image_id, shell: 'docker
          images | awk ''/haproxy.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Haproxy image, register: haproxy_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{haproxy_image_id.stdout}}'''}
        - {name: Remove any container using the same Haproxy image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ haproxy_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Haproxy images, shell: 'docker rmi -f {{haproxy_image_id.stdout}}'}
        when: [haproxy_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Haproxy images}
      - {name: Retag pcmklatest to latest Haproxy image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Retag the pacemaker image if containerized
      when: [step|int == 3, haproxy_containerized|bool]
    - {command: systemctl is-enabled --quiet openstack-heat-api, ignore_errors: true,
      name: Check if heat_api is deployed, register: heat_api_enabled, tags: common}
    - {ignore_errors: true, name: Check for heat_api running under apache, register: httpd_enabled,
      shell: httpd -t -D DUMP_VHOSTS | grep -q heat_api_wsgi, tags: common}
    - command: systemctl is-active --quiet openstack-heat-api
      name: 'PreUpgrade step0,validation: Check service openstack-heat-api is running'
      tags: validation
      when: [step|int == 0, heat_api_enabled.rc == 0, httpd_enabled.rc != 0]
    - name: Stop and disable heat_api service (pre-upgrade not under httpd)
      service: name=openstack-heat-api state=stopped enabled=no
      when: [step|int == 2, heat_api_enabled.rc == 0, httpd_enabled.rc != 0]
    - {command: systemctl is-active --quiet httpd, ignore_errors: true, name: Check
        if httpd is running, register: httpd_running, tags: common}
    - name: 'PreUpgrade step0,validation: Check if heat_api_wsgi is running'
      shell: systemctl status 'httpd' | grep -q heat_api_wsgi
      tags: validation
      when: [step|int == 0, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - name: Stop heat_api service (running under httpd)
      service: name=httpd state=stopped
      when: [step|int == 2, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - file: {path: /var/spool/cron/heat, state: absent}
      name: remove old heat cron jobs
      when: step|int == 2
    - {command: systemctl is-enabled openstack-heat-api-cloudwatch, ignore_errors: true,
      name: Check if heat_api_cloudwatch is deployed, register: heat_api_cloudwatch_enabled,
      when: step|int == 1}
    - name: Stop and disable heat_api_cloudwatch service (pre-upgrade not under httpd)
      service: name=openstack-heat-api-cloudwatch state=stopped enabled=no
      when: [step|int == 1, heat_api_cloudwatch_enabled.rc == 0]
    - {command: systemctl is-enabled --quiet openstack-heat-api-cfn, ignore_errors: true,
      name: Check if heat_api_cfn is deployed, register: heat_api_cfn_enabled, tags: common}
    - {ignore_errors: true, name: Check for heat_api_cfn running under apache, register: httpd_enabled,
      shell: httpd -t -D DUMP_VHOSTS | grep -q heat_api_cfn_wsgi, tags: common}
    - command: systemctl is-active --quiet openstack-heat-api-cfn
      name: 'PreUpgrade step0,validation: Check service openstack-heat-api-cfn is
        running'
      tags: validation
      when: [step|int == 0, heat_api_cfn_enabled.rc == 0, httpd_enabled.rc != 0]
    - name: Stop and disable heat_api_cfn service (pre-upgrade not under httpd)
      service: name=openstack-heat-api-cfn state=stopped enabled=no
      when: [step|int == 2, heat_api_cfn_enabled.rc == 0, httpd_enabled.rc != 0]
    - {command: systemctl is-active --quiet httpd, ignore_errors: true, name: Check
        if httpd service is running, register: httpd_running, tags: common}
    - name: 'PreUpgrade step0,validation: Check if heat_api_cfn_wsgi is running'
      shell: systemctl status 'httpd' | grep -q heat_api_cfn_wsgi
      tags: validation
      when: [step|int == 0, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - name: Stop heat_api_cfn service (running under httpd)
      service: name=httpd state=stopped
      when: [step|int == 2, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - {command: systemctl is-enabled --quiet openstack-heat-engine, ignore_errors: true,
      name: Check if heat_engine is deployed, register: heat_engine_enabled, tags: common}
    - command: systemctl is-active --quiet openstack-heat-engine
      name: 'PreUpgrade step0,validation: Check service openstack-heat-engine is running'
      tags: validation
      when: [step|int == 0, heat_engine_enabled.rc == 0]
    - name: Stop and disable heat_engine service
      service: name=openstack-heat-engine state=stopped enabled=no
      when: [step|int == 2, heat_engine_enabled.rc == 0]
    - {ignore_errors: true, name: Check for horizon running under apache, register: httpd_enabled,
      shell: httpd -t -D DUMP_VHOSTS | grep -q horizon_vhost, tags: common}
    - name: 'PreUpgrade step0,validation: Check if horizon is running'
      shell: systemctl is-active --quiet httpd
      tags: validation
      when: [step|int == 0, httpd_enabled.rc == 0]
    - name: Stop and disable horizon service (running under httpd)
      service: name=httpd state=stopped enabled=no
      when: [step|int == 2, httpd_enabled.rc == 0]
    - {command: systemctl is-enabled --quiet iscsid, ignore_errors: true, name: Check
        if iscsid service is deployed, register: iscsid_enabled, tags: common}
    - command: systemctl is-active --quiet iscsid
      name: 'PreUpgrade step0,validation: Check if iscsid is running'
      tags: validation
      when: [step|int == 0, iscsid_enabled.rc == 0]
    - name: Stop and disable iscsid service
      service: name=iscsid state=stopped enabled=no
      when: [step|int == 2, iscsid_enabled.rc == 0]
    - {command: systemctl is-enabled --quiet iscsid.socket, ignore_errors: true, name: Check
        if iscsid.socket service is deployed, register: iscsid_socket_enabled, tags: common}
    - command: systemctl is-active --quiet iscsid.socket
      name: 'PreUpgrade step0,validation: Check if iscsid.socket is running'
      tags: validation
      when: [step|int == 0, iscsid_socket_enabled.rc == 0]
    - name: Stop and disable iscsid.socket service
      service: name=iscsid.socket state=stopped enabled=no
      when: [step|int == 2, iscsid_socket_enabled.rc == 0]
    - {ignore_errors: true, name: Check for keystone running under apache, register: httpd_enabled,
      shell: httpd -t -D DUMP_VHOSTS | grep -q keystone_wsgi, tags: common}
    - name: 'PreUpgrade step0,validation: Check if keystone_wsgi is running under
        httpd'
      shell: systemctl status 'httpd' | grep -q keystone
      tags: validation
      when: [step|int == 0, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - name: Stop and disable keystone service (running under httpd)
      service: name=httpd state=stopped enabled=no
      when: [step|int == 2, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - file: {path: /var/spool/cron/keystone, state: absent}
      name: remove old keystone cron jobs
      when: step|int == 2
    - {command: systemctl is-enabled --quiet memcached, ignore_errors: true, name: Check
        if memcached is deployed, register: memcached_enabled, tags: common}
    - command: systemctl is-active --quiet memcached
      name: 'PreUpgrade step0,validation: Check service memcached is running'
      tags: validation
      when: [step|int == 0, memcached_enabled.rc == 0]
    - name: Stop and disable memcached service
      service: name=memcached state=stopped enabled=no
      when: [step|int == 2, memcached_enabled.rc == 0]
    - {name: Check for mongodb service, register: mongod_service, stat: path=/usr/lib/systemd/system/mongod.service,
      tags: common}
    - name: Stop and disable mongodb service on upgrade
      service: name=mongod state=stopped enabled=no
      when: [step|int == 1, mongod_service.stat.exists]
    - name: Get docker Mysql image
      set_fact: {mysql_docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-mariadb:pcmklatest'}
    - name: Check for Mysql Kolla configuration
      register: mysql_kolla_config
      stat: {path: /var/lib/config-data/puppet-generated/mysql}
    - name: Check if Mysql is already containerized
      set_fact: {mysql_containerized: '{{mysql_kolla_config.stat.isdir | default(false)}}'}
    - {command: hiera -c /etc/puppet/hiera.yaml bootstrap_nodeid, name: get bootstrap
        nodeid, register: bootstrap_node, tags: common}
    - {name: set is_bootstrap_node fact, set_fact: 'is_bootstrap_node={{bootstrap_node.stdout|lower
        == ansible_hostname|lower}}', tags: common}
    - block:
      - ignore_errors: true
        name: Check cluster resource status
        pacemaker_resource: {check_mode: true, resource: galera, state: master}
        register: galera_res
      - block:
        - name: Disable the galera cluster resource
          pacemaker_resource: {resource: galera, state: disable, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        - name: Delete the stopped galera cluster resource.
          pacemaker_resource: {resource: galera, state: delete, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        when: (is_bootstrap_node) and (galera_res|succeeded)
      - {name: Disable mysql service, service: name=mariadb enabled=no}
      - {file: state=absent path=/etc/xinetd.d/galera-monitor, name: Remove clustercheck
          service from xinetd}
      - {name: Restart xinetd service after clustercheck removal, service: name=xinetd
          state=restarted}
      name: Mysql baremetal to container upgrade tasks
      when: [step|int == 1, not mysql_containerized|bool]
    - block:
      - {name: Get galera image id currently used by pacemaker, register: galera_current_pcmklatest_id,
        shell: 'docker images | awk ''/mariadb.* pcmklatest/{print $3}'' | uniq'}
      - {name: Temporarily tag the current galera image id with the upgraded image
          name, shell: 'docker tag {{galera_current_pcmklatest_id.stdout}} {{mysql_docker_image_latest}}'}
      name: Prepare the switch to new galera container image name in pacemaker
      when: [step|int == 0, mysql_containerized|bool]
    - ignore_errors: true
      name: Check galera cluster resource status
      pacemaker_resource: {check_mode: false, resource: galera, state: show}
      register: galera_pcs_res
    - block:
      - name: Disable the galera cluster resource before container upgrade
        pacemaker_resource: {resource: galera, state: disable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      - block:
        - {command: 'cibadmin --query --xpath "//storage-mapping[@id=''mysql-log'']"',
          ignore_errors: true, name: Check Mysql logging configuration in pacemaker,
          register: mysql_logs_moved}
        - block:
          - {command: pcs resource bundle update galera-bundle storage-map add id=mysql-log
              source-dir=/var/log/containers/mysql target-dir=/var/log/mysql options=rw,
            name: Add a bind mount for logging in the galera bundle}
          - {command: pcs resource update galera log=/var/log/mysql/mysqld.log, name: Reconfigure
              Mysql log file in the galera resource agent}
          name: Change Mysql logging configuration in pacemaker
          when: mysql_logs_moved.rc == 6
        name: Move Mysql logging to /var/log/containers
      - {command: 'pcs resource bundle update galera-bundle container image={{mysql_docker_image_latest}}',
        name: Update the galera bundle to use the new container image name}
      - name: Enable the galera cluster resource
        pacemaker_resource: {resource: galera, state: enable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      name: Update galera pcs resource bundle for new container image
      when: [step|int == 1, mysql_containerized|bool, is_bootstrap_node, galera_pcs_res|succeeded]
    - block:
      - name: Get docker Mariadb image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-mariadb:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-mariadb:pcmklatest'}
      - {name: Get previous Mariadb image id, register: mariadb_image_id, shell: 'docker
          images | awk ''/mariadb.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Mariadb image, register: mariadb_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{mariadb_image_id.stdout}}'''}
        - {name: Remove any container using the same Mariadb image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ mariadb_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Mariadb images, shell: 'docker rmi -f {{mariadb_image_id.stdout}}'}
        when: [mariadb_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Mariadb images}
      - {name: Retag pcmklatest to latest Mariadb image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Retag the pacemaker image if containerized
      when: [step|int == 3, mysql_containerized|bool]
    - block:
      - {name: Update host mariadb packages, when: step|int == 3, yum: name=mariadb-server-galera
          state=latest}
      - name: Mysql upgrade script
        set_fact: {mysql_upgrade_script: '{% if mysql_containerized %}kolla_set_configs;
            {% endif %} chown -R mysql:mysql /var/lib/mysql; mysqld_safe --user=mysql
            --wsrep-provider=none --skip-networking --wsrep-on=off & timeout 60 sh
            -c ''while ! mysqladmin ping --silent; do sleep 1; done''; mysql_upgrade;
            mysqladmin shutdown'}
      - name: Bind mounts for temporary container
        set_fact:
          mysql_upgrade_db_bind_mounts: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/mysql.json:/var/lib/kolla/config_files/config.json',
            '/var/lib/config-data/puppet-generated/mysql/:/var/lib/kolla/config_files/src:ro',
            '/var/lib/mysql:/var/lib/mysql']
      - {name: Upgrade Mysql database from a temporary container, shell: '/usr/bin/docker
          run --rm --log-driver=syslog -u root --net=host -e "KOLLA_CONFIG_STRATEGY=COPY_ALWAYS"
          -v {{ mysql_upgrade_db_bind_mounts | union([''/tmp/mariadb-upgrade:/var/log/mariadb:rw''])
          | join('' -v '')}} "192.168.120.1:8787/rhosp13/openstack-mariadb:pcmklatest"
          /bin/bash -ecx "{{mysql_upgrade_script}}"', when: mysql_containerized|bool}
      - {name: Upgrade Mysql database from the host, shell: '/bin/bash -ecx "{{mysql_upgrade_script}}"',
        when: not mysql_containerized|bool}
      name: Check and upgrade Mysql database after major version upgrade
      when: step|int == 3
    - {command: systemctl is-enabled --quiet neutron-server, ignore_errors: true,
      name: Check if neutron_server is deployed, register: neutron_server_enabled,
      tags: common}
    - command: systemctl is-active --quiet neutron-server
      name: 'PreUpgrade step0,validation: Check service neutron-server is running'
      tags: validation
      when: [step|int == 0, neutron_server_enabled.rc == 0]
    - name: Stop and disable neutron_api service
      service: name=neutron-server state=stopped enabled=no
      when: [step|int == 2, neutron_server_enabled.rc == 0]
    - name: Set fact for removal of openstack-neutron package
      set_fact: {remove_neutron_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-neutron package if operator requests it
      when: [step|int == 2, remove_neutron_package|bool]
      yum: name=openstack-neutron state=removed
    - {command: systemctl is-enabled --quiet neutron-dhcp-agent, ignore_errors: true,
      name: Check if neutron_dhcp_agent is deployed, register: neutron_dhcp_agent_enabled,
      tags: common}
    - command: systemctl is-active --quiet neutron-dhcp-agent
      name: 'PreUpgrade step0,validation: Check service neutron-dhcp-agent is running'
      tags: validation
      when: [step|int == 0, neutron_dhcp_agent_enabled.rc == 0]
    - name: Stop and disable neutron_dhcp service
      service: name=neutron-dhcp-agent state=stopped enabled=no
      when: [step|int == 2, neutron_dhcp_agent_enabled.rc == 0]
    - {command: systemctl is-enabled --quiet neutron-l3-agent, ignore_errors: true,
      name: Check if neutron_l3_agent is deployed, register: neutron_l3_agent_enabled,
      tags: common}
    - command: systemctl is-active --quiet neutron-l3-agent
      name: 'PreUpgrade step0,validation: Check service neutron-l3-agent is running'
      tags: validation
      when: [step|int == 0, neutron_l3_agent_enabled.rc == 0]
    - name: Stop and disable neutron_l3 service
      service: name=neutron-l3-agent state=stopped enabled=no
      when: [step|int == 2, neutron_l3_agent_enabled.rc == 0]
    - {command: systemctl is-enabled --quiet neutron-metadata-agent, ignore_errors: true,
      name: Check if neutron_metadata_agent is deployed, register: neutron_metadata_agent_enabled,
      tags: common}
    - command: systemctl is-active --quiet neutron-metadata-agent
      name: 'PreUpgrade step0,validation: Check service neutron-metadata-agent is
        running'
      tags: validation
      when: [step|int == 0, neutron_metadata_agent_enabled.rc == 0]
    - name: Stop and disable neutron_metadata service
      service: name=neutron-metadata-agent state=stopped enabled=no
      when: [step|int == 2, neutron_metadata_agent_enabled.rc == 0]
    - {ignore_errors: true, name: Check openvswitch version., register: ovs_version,
      shell: 'rpm -qa | awk -F- ''/^openvswitch-2/{print $2 "-" $3}''', when: step|int
        == 2}
    - {ignore_errors: true, name: Check openvswitch packaging., register: ovs_packaging_issue,
      shell: 'rpm -q --scripts openvswitch | awk ''/postuninstall/,/*/'' | grep -q
        "systemctl.*try-restart"', when: step|int == 2}
    - block:
      - file: {path: /root/OVS_UPGRADE, state: absent}
        name: 'Ensure empty directory: emptying.'
      - file: {group: root, mode: 488, owner: root, path: /root/OVS_UPGRADE, state: directory}
        name: 'Ensure empty directory: creating.'
      - {command: yum makecache, name: Make yum cache.}
      - {command: yumdownloader --destdir /root/OVS_UPGRADE --resolve openvswitch,
        name: Download OVS packages.}
      - {name: Get rpm list for manual upgrade of OVS., register: ovs_list_of_rpms,
        shell: ls -1 /root/OVS_UPGRADE/*.rpm}
      - args: {chdir: /root/OVS_UPGRADE}
        name: Manual upgrade of OVS
        shell: 'rpm -U --test {{item}} 2>&1 | grep "already installed" || \

          rpm -U --replacepkgs --notriggerun --nopostun {{item}};

          '
        with_items: ['{{ovs_list_of_rpms.stdout_lines}}']
      when: [step|int == 2, '''2.5.0-14'' in ovs_version.stdout|default('''') or ovs_packaging_issue|default(false)|succeeded']
    - {command: systemctl is-enabled --quiet neutron-openvswitch-agent, ignore_errors: true,
      name: Check if neutron_ovs_agent is deployed, register: neutron_ovs_agent_enabled,
      tags: common}
    - command: systemctl is-active --quiet neutron-openvswitch-agent
      name: 'PreUpgrade step0,validation: Check service neutron-openvswitch-agent
        is running'
      tags: validation
      when: [step|int == 0, neutron_ovs_agent_enabled.rc == 0]
    - name: Stop and disable neutron_ovs_agent service
      service: name=neutron-openvswitch-agent state=stopped enabled=no
      when: [step|int == 2, neutron_ovs_agent_enabled.rc == 0]
    - name: Set fact for removal of openstack-neutron-openvswitch package
      set_fact: {remove_neutron_openvswitch_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-neutron-openvswitch package if operator requests it
      when: [step|int == 2, remove_neutron_openvswitch_package|bool]
      yum: name=openstack-neutron-openvswitch state=removed
    - {command: systemctl is-enabled --quiet openstack-nova-api, ignore_errors: true,
      name: Check if nova_api is deployed, register: nova_api_enabled, tags: common}
    - {ignore_errors: true, name: Check for nova-api running under apache, register: httpd_enabled,
      shell: httpd -t -D DUMP_VHOSTS | grep -q 'nova', tags: common}
    - command: systemctl is-active --quiet openstack-nova-api
      name: 'PreUpgrade step0,validation: Check service openstack-nova-api is running'
      tags: validation
      when: [step|int == 0, nova_api_enabled.rc == 0, httpd_enabled.rc != 0]
    - name: Stop and disable nova_api service
      service: name=openstack-nova-api state=stopped enabled=no
      when: [step|int == 2, nova_api_enabled.rc == 0, httpd_enabled.rc != 0]
    - name: 'PreUpgrade step0,validation: Check if nova_wsgi is running'
      shell: systemctl status 'httpd' | grep -q 'nova'
      tags: validation
      when: [step|int == 0, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - name: Stop nova_api service (running under httpd)
      service: name=httpd state=stopped
      when: [step|int == 2, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - name: Set fact for removal of openstack-nova-api package
      set_fact: {remove_nova_api_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-nova-api package if operator requests it
      when: [step|int == 2, remove_nova_api_package|bool]
      yum: name=openstack-nova-api state=removed
    - file: {path: /var/spool/cron/nova, state: absent}
      name: remove old nova cron jobs
      when: step|int == 2
    - {command: systemctl is-enabled --quiet openstack-nova-conductor, ignore_errors: true,
      name: Check if nova_conductor is deployed, register: nova_conductor_enabled,
      tags: common}
    - {ini_file: dest=/etc/nova/nova.conf section=upgrade_levels option=compute value=,
      name: Set compute upgrade level to auto, when: step|int == 1}
    - command: systemctl is-active --quiet openstack-nova-conductor
      name: 'PreUpgrade step0,validation: Check service openstack-nova-conductor is
        running'
      tags: validation
      when: [step|int == 0, nova_conductor_enabled.rc == 0]
    - name: Stop and disable nova_conductor service
      service: name=openstack-nova-conductor state=stopped enabled=no
      when: [step|int == 2, nova_conductor_enabled.rc == 0]
    - name: Set fact for removal of openstack-nova-conductor package
      set_fact: {remove_nova_conductor_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-nova-conductor package if operator requests it
      when: [step|int == 2, remove_nova_conductor_package|bool]
      yum: name=openstack-nova-conductor state=removed
    - {command: systemctl is-enabled --quiet openstack-nova-consoleauth, ignore_errors: true,
      name: Check if nova_consoleauth is deployed, register: nova_consoleauth_enabled,
      tags: common}
    - command: systemctl is-active --quiet openstack-nova-consoleauth
      name: 'PreUpgrade step0,validation: Check service openstack-nova-consoleauth
        is running'
      tags: validation
      when: [step|int == 0, nova_consoleauth_enabled.rc == 0]
    - name: Stop and disable nova_consoleauth service
      service: name=openstack-nova-consoleauth state=stopped enabled=no
      when: [step|int == 2, nova_consoleauth_enabled.rc == 0]
    - name: Set fact for removal of openstack-nova-console package
      set_fact: {remove_nova_console_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-nova-console package if operator requests it
      when: [step|int == 2, remove_nova_console_package|bool]
      yum: name=openstack-nova-console state=removed
    - {command: systemctl is-enabled --quiet openstack-nova-api, ignore_errors: true,
      name: Check if nova_api_metadata is deployed, register: nova_metadata_enabled,
      tags: common}
    - command: systemctl is-active --quiet openstack-nova-api
      name: 'PreUpgrade step0,validation: Check service openstack-nova-api is running'
      tags: validation
      when: [step|int == 0, nova_metadata_enabled.rc == 0]
    - name: Stop and disable nova_api service
      service: name=openstack-nova-api state=stopped enabled=no
      when: [step|int == 2, nova_metadata_enabled.rc == 0]
    - {ignore_errors: true, name: Check for nova placement running under apache, register: httpd_enabled,
      shell: httpd -t -D DUMP_VHOSTS | grep -q placement_wsgi, tags: common}
    - name: 'PreUpgrade step0,validation: Check if placement_wsgi is running'
      shell: systemctl status 'httpd' | grep -q placement_wsgi
      tags: validation
      when: [step|int == 0, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - name: Stop and disable nova_placement service (running under httpd)
      service: name=httpd state=stopped enabled=no
      when: [step|int == 2, httpd_enabled.rc == 0, httpd_running.rc == 0]
    - {command: systemctl is-enabled --quiet openstack-nova-scheduler, ignore_errors: true,
      name: Check if nova_scheduler is deployed, register: nova_scheduler_enabled,
      tags: common}
    - command: systemctl is-active --quiet openstack-nova-scheduler
      name: 'PreUpgrade step0,validation: Check service openstack-nova-scheduler is
        running'
      tags: validation
      when: [step|int == 0, nova_scheduler_enabled.rc == 0]
    - name: Stop and disable nova_scheduler service
      service: name=openstack-nova-scheduler state=stopped enabled=no
      when: [step|int == 2, nova_scheduler_enabled.rc == 0]
    - name: Set fact for removal of openstack-nova-scheduler package
      set_fact: {remove_nova_scheduler_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-nova-scheduler package if operator requests it
      when: [step|int == 2, remove_nova_scheduler_package|bool]
      yum: name=openstack-nova-scheduler state=removed
    - {command: systemctl is-enabled --quiet openstack-nova-novncproxy, ignore_errors: true,
      name: Check if nova vncproxy is deployed, register: nova_vncproxy_enabled, tags: common}
    - command: systemctl is-active --quiet openstack-nova-novncproxy
      name: 'PreUpgrade step0,validation: Check service openstack-nova-novncproxy
        is running'
      tags: validation
      when: [step|int == 0, nova_vncproxy_enabled.rc == 0]
    - name: Stop and disable nova_vnc_proxy service
      service: name=openstack-nova-novncproxy state=stopped enabled=no
      when: [step|int == 2, nova_vncproxy_enabled.rc == 0]
    - name: Set fact for removal of openstack-nova-novncproxy package
      set_fact: {remove_nova_novncproxy_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-nova-novncproxy package if operator requests it
      when: [step|int == 2, remove_nova_novncproxy_package|bool]
      yum: name=openstack-nova-novncproxy state=removed
    - {async: 30, name: Check pacemaker cluster running before upgrade, pacemaker_cluster: state=online
        check_and_fail=true, poll: 4, tags: validation, when: step|int == 0}
    - {name: Stop pacemaker cluster, pacemaker_cluster: state=offline, when: step|int
        == 2}
    - {name: Start pacemaker cluster, pacemaker_cluster: state=online, when: step|int
        == 4}
    - name: Get docker Rabbitmq image
      set_fact: {rabbitmq_docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-rabbitmq:pcmklatest'}
    - name: Check for Rabbitmq Kolla configuration
      register: rabbit_kolla_config
      stat: {path: /var/lib/config-data/puppet-generated/rabbitmq}
    - name: Check if Rabbitmq is already containerized
      set_fact: {rabbit_containerized: '{{rabbit_kolla_config.stat.isdir | default(false)}}'}
    - {command: hiera -c /etc/puppet/hiera.yaml bootstrap_nodeid, name: get bootstrap
        nodeid, register: bootstrap_node}
    - {name: set is_bootstrap_node fact, set_fact: 'is_bootstrap_node={{bootstrap_node.stdout|lower
        == ansible_hostname|lower}}'}
    - block:
      - ignore_errors: true
        name: Check cluster resource status of rabbitmq
        pacemaker_resource: {check_mode: false, resource: rabbitmq, state: show}
        register: rabbitmq_res
      - block:
        - name: Disable the rabbitmq cluster resource.
          pacemaker_resource: {resource: rabbitmq, state: disable, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        - name: Delete the stopped rabbitmq cluster resource.
          pacemaker_resource: {resource: rabbitmq, state: delete, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        when: (is_bootstrap_node) and (rabbitmq_res|succeeded)
      - {name: Disable rabbitmq service, service: name=rabbitmq-server enabled=no}
      name: Rabbitmq baremetal to container upgrade tasks
      when: [step|int == 1, not rabbit_containerized|bool]
    - block:
      - {name: Get rabbitmq image id currently used by pacemaker, register: rabbitmq_current_pcmklatest_id,
        shell: 'docker images | awk ''/rabbitmq.* pcmklatest/{print $3}'' | uniq'}
      - {name: Temporarily tag the current rabbitmq image id with the upgraded image
          name, shell: 'docker tag {{rabbitmq_current_pcmklatest_id.stdout}} {{rabbitmq_docker_image_latest}}'}
      name: Prepare the switch to new rabbitmq container image name in pacemaker
      when: [step|int == 0, rabbit_containerized|bool]
    - ignore_errors: true
      name: Check rabbitmq-bundle cluster resource status
      pacemaker_resource: {check_mode: false, resource: rabbitmq-bundle, state: show}
      register: rabbit_pcs_res
    - block:
      - name: Disable the rabbitmq cluster resource before container upgrade
        pacemaker_resource: {resource: rabbitmq-bundle, state: disable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      - block:
        - {command: 'cibadmin --query --xpath "//storage-mapping[@id=''rabbitmq-log'']"',
          ignore_errors: true, name: Check rabbitmq logging configuration in pacemaker,
          register: rabbitmq_logs_moved}
        - {command: pcs resource bundle update rabbitmq-bundle storage-map add id=rabbitmq-log
            source-dir=/var/log/containers/rabbitmq target-dir=/var/log/rabbitmq options=rw,
          name: Add a bind mount for logging in the rabbitmq bundle, when: rabbitmq_logs_moved.rc
            == 6}
        name: Move rabbitmq logging to /var/log/containers
      - {command: 'pcs resource bundle update rabbitmq-bundle container image={{rabbitmq_docker_image_latest}}',
        name: Update the rabbitmq bundle to use the new container image name}
      - name: Enable the rabbitmq cluster resource
        pacemaker_resource: {resource: rabbitmq-bundle, state: enable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      name: Update rabbitmq-bundle pcs resource bundle for new container image
      when: [step|int == 1, rabbit_containerized|bool, is_bootstrap_node, rabbit_pcs_res|succeeded]
    - block:
      - name: Get docker Rabbitmq image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-rabbitmq:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-rabbitmq:pcmklatest'}
      - {name: Get previous Rabbitmq image id, register: rabbitmq_image_id, shell: 'docker
          images | awk ''/rabbitmq.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Rabbitmq image, register: rabbitmq_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{rabbitmq_image_id.stdout}}'''}
        - {name: Remove any container using the same Rabbitmq image, shell: 'docker
            rm -fv {{item}}', with_items: '{{ rabbitmq_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Rabbitmq images, shell: 'docker rmi -f {{rabbitmq_image_id.stdout}}'}
        when: [rabbitmq_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Rabbitmq images}
      - {name: Retag pcmklatest to latest Rabbitmq image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Retag the pacemaker image if containerized
      when: [step|int == 3, rabbit_containerized|bool]
    - name: Get docker redis image
      set_fact: {redis_docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-redis:pcmklatest'}
    - name: Check for redis Kolla configuration
      register: redis_kolla_config
      stat: {path: /var/lib/config-data/puppet-generated/redis}
    - name: Check if redis is already containerized
      set_fact: {redis_containerized: '{{redis_kolla_config.stat.isdir | default(false)}}'}
    - block:
      - ignore_errors: true
        name: Check cluster resource status of redis
        pacemaker_resource: {check_mode: false, resource: redis, state: show}
        register: redis_res
      - block:
        - name: Disable the redis cluster resource
          pacemaker_resource: {resource: redis, state: disable, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        - name: Delete the stopped redis cluster resource.
          pacemaker_resource: {resource: redis, state: delete, wait_for_resource: true}
          register: output
          retries: 5
          until: output.rc == 0
        when: (is_bootstrap_node) and (redis_res|succeeded)
      - {name: Disable redis service, service: name=redis enabled=no}
      name: redis baremetal to container upgrade tasks
      when: [step|int == 1, not redis_containerized|bool]
    - block:
      - {name: Get redis image id currently used by pacemaker, register: redis_current_pcmklatest_id,
        shell: 'docker images | awk ''/redis.* pcmklatest/{print $3}'' | uniq'}
      - {name: Temporarily tag the current redis image id with the upgraded image
          name, shell: 'docker tag {{redis_current_pcmklatest_id.stdout}} {{redis_docker_image_latest}}'}
      name: Prepare the switch to new redis container image name in pacemaker
      when: [step|int == 0, redis_containerized|bool]
    - ignore_errors: true
      name: Check redis-bundle cluster resource status
      pacemaker_resource: {check_mode: false, resource: redis-bundle, state: show}
      register: redis_pcs_res
    - block:
      - name: Disable the redis cluster resource before container upgrade
        pacemaker_resource: {resource: redis-bundle, state: disable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      - block:
        - {command: 'cibadmin --query --xpath "//storage-mapping[@id=''redis-log''
            and @source-dir=''/var/log/containers/redis'']"', ignore_errors: true,
          name: Check redis logging configuration in pacemaker, register: redis_logs_moved}
        - block:
          - {command: pcs resource bundle update redis-bundle storage-map remove redis-log,
            name: Remove old bind mount for logging in the redis bundle}
          - {command: pcs resource bundle update redis-bundle storage-map add id=redis-log
              source-dir=/var/log/containers/redis target-dir=/var/log/redis options=rw,
            name: Add a bind mount for logging in the redis bundle}
          name: Change redis logging configuration in pacemaker
          when: redis_logs_moved.rc == 6
        name: Move redis logging to /var/log/containers
      - {command: 'pcs resource bundle update redis-bundle container image={{redis_docker_image_latest}}',
        name: Update the redis bundle to use the new container image name}
      - name: Enable the redis cluster resource
        pacemaker_resource: {resource: redis-bundle, state: enable, wait_for_resource: true}
        register: output
        retries: 5
        until: output.rc == 0
      name: Update redis-bundle pcs resource bundle for new container image
      when: [step|int == 1, redis_containerized|bool, is_bootstrap_node, redis_pcs_res|succeeded]
    - block:
      - name: Get docker Redis image
        set_fact: {docker_image: '192.168.120.1:8787/rhosp13/openstack-redis:latest',
          docker_image_latest: '192.168.120.1:8787/rhosp13/openstack-redis:pcmklatest'}
      - {name: Get previous Redis image id, register: redis_image_id, shell: 'docker
          images | awk ''/redis.* pcmklatest/{print $3}'' | uniq'}
      - block:
        - {name: Get a list of container using Redis image, register: redis_containers_to_destroy,
          shell: 'docker ps -a -q -f ''ancestor={{redis_image_id.stdout}}'''}
        - {name: Remove any container using the same Redis image, shell: 'docker rm
            -fv {{item}}', with_items: '{{ redis_containers_to_destroy.stdout_lines
            }}'}
        - {name: Remove previous Redis images, shell: 'docker rmi -f {{redis_image_id.stdout}}'}
        when: [redis_image_id.stdout != '']
      - {command: 'docker pull {{docker_image}}', name: Pull latest Redis images}
      - {name: Retag pcmklatest to latest Redis image, shell: 'docker tag {{docker_image}}
          {{docker_image_latest}}'}
      name: Retag the pacemaker image if containerized
      when: [step|int == 3, redis_containerized|bool]
    - {name: Stop snmp service, service: name=snmpd state=stopped, when: step|int
        == 1}
    - command: systemctl is-enabled --quiet "{{ item }}"
      ignore_errors: true
      name: Check if swift-proxy or swift-object-expirer are deployed
      register: swift_proxy_services_enabled
      tags: common
      with_items: [openstack-swift-proxy, openstack-swift-object-expirer]
    - command: systemctl is-active --quiet "{{ item.item }}"
      name: 'PreUpgrade step0,validation: Check service openstack-swift-proxy and
        openstack-swift-object-expirer are running'
      tags: validation
      when: [step|int == 0, item.rc == 0]
      with_items: '{{ swift_proxy_services_enabled.results }}'
    - name: Stop and disable swift-proxy and swift-object-expirer services
      service: name={{ item.item }} state=stopped enabled=no
      when: [step|int == 2, item.rc == 0]
      with_items: '{{ swift_proxy_services_enabled.results }}'
    - name: Set fact for removal of openstack-swift-proxy package
      set_fact: {remove_swift_proxy_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-swift-proxy package if operator requests it
      when: [step|int == 2, remove_swift_proxy_package|bool]
      yum: name=openstack-swift-proxy state=removed
    - command: systemctl is-enabled --quiet "{{ item }}"
      ignore_errors: true
      name: Check if swift storage services are deployed
      register: swift_services_enabled
      tags: common
      with_items: [openstack-swift-account-auditor, openstack-swift-account-reaper,
        openstack-swift-account-replicator, openstack-swift-account, openstack-swift-container-auditor,
        openstack-swift-container-replicator, openstack-swift-container-updater, openstack-swift-container,
        openstack-swift-object-auditor, openstack-swift-object-replicator, openstack-swift-object-updater,
        openstack-swift-object]
    - command: systemctl is-active --quiet "{{ item.item }}"
      name: 'PreUpgrade step0,validation: Check swift storage services are running'
      tags: validation
      when: [step|int == 0, item.rc == 0]
      with_items: '{{ swift_services_enabled.results }}'
    - name: Stop and disable swift storage services
      service: name={{ item.item }} state=stopped enabled=no
      when: [step|int == 2, item.rc == 0]
      with_items: '{{ swift_services_enabled.results }}'
    - name: Set fact for removal of openstack-swift-container,object,account package
      set_fact: {remove_swift_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-swift-container,object,account packages if operator requests
        it
      when: [step|int == 2, remove_swift_package|bool]
      with_items: [openstack-swift-container, openstack-swift-object, openstack-swift-account]
      yum: name={{ item }} state=removed
    - {file: state=absent path=/etc/xinetd.d/rsync, name: Remove rsync service from
        xinetd, register: rsync_service_removed, when: step|int == 2}
    - name: Restart xinetd service after rsync removal
      service: name=xinetd state=restarted
      when: [step|int == 2, rsync_service_removed|changed]
    - args: {creates: /etc/sysconfig/ip6tables.n-o-upgrade}
      name: blank ipv6 rule before activating ipv6 firewall.
      shell: cat /etc/sysconfig/ip6tables > /etc/sysconfig/ip6tables.n-o-upgrade;
        cat</dev/null>/etc/sysconfig/ip6tables
      when: step|int == 3
    - {name: Check yum for rpm-python present, register: rpm_python_check, when: step|int
        == 0, yum: name=rpm-python state=present}
    - fail: msg="rpm-python package was not present before this run! Check environment
        before re-running"
      name: Fail when rpm-python wasn't present
      when: [step|int == 0, rpm_python_check.changed != false]
    - {name: Check for os-net-config upgrade, register: os_net_config_need_upgrade,
      shell: 'yum check-upgrade | awk ''/os-net-config/{print}''', when: step|int
        == 3}
    - {ignore_errors: true, name: Check that os-net-config has configuration, register: os_net_config_has_config,
      shell: test -s /etc/os-net-config/config.json, when: step|int == 3}
    - block:
      - {name: Upgrade os-net-config, yum: name=os-net-config state=latest}
      - {changed_when: os_net_config_upgrade.rc == 2, command: os-net-config --no-activate
          -c /etc/os-net-config/config.json -v --detailed-exit-codes, failed_when: 'os_net_config_upgrade.rc
          not in [0,2]', name: take new os-net-config parameters into account now,
        register: os_net_config_upgrade}
      when: [step|int == 3, os_net_config_need_upgrade.stdout, os_net_config_has_config.rc
          == 0]
    - {name: Update all packages, when: step|int == 3, yum: name=* state=latest}
    role_data_workflow_tasks:
      step2:
      - input:
          ansible_env_variables: {}
          ansible_playbook_verbosity: 1
          ansible_skip_tags: package-install,with_pkg
          ceph_ansible_extra_vars: {ceph_osd_docker_cpu_limit: 1, ceph_osd_docker_memory_limit: 3g,
            osd_max_backfills: 1, osd_objectstore: filestore, osd_recovery_max_active: 3,
            osd_recovery_op_priority: 3}
          ceph_ansible_playbook: [/usr/share/ceph-ansible/site-docker.yml.sample]
          node_data_lookup: '{}'
        name: ceph_base_ansible_workflow
        workflow: tripleo.storage.v1.ceph-install
    role_name: Controller
osd-compute-0:
  hosts:
    192.168.120.14: {}
  vars:
    cephmetrics_ip: 172.17.5.221
    ctlplane_ip: 192.168.120.14
    deploy_server_id: 1f1cf36a-899e-4d1f-80cf-3b4303a8f5eb
    enabled_networks: [cephmetrics, management, storage, ctlplane, external, internal_api,
      storage_mgmt, tenant]
    external_ip: 192.168.120.14
    internal_api_ip: 172.17.1.221
    management_ip: 192.168.120.14
    storage_ip: 172.17.3.221
    storage_mgmt_ip: 172.17.4.221
    tenant_ip: 172.17.2.221
osd-compute-1:
  hosts:
    192.168.120.7: {}
  vars:
    cephmetrics_ip: 172.17.5.222
    ctlplane_ip: 192.168.120.7
    deploy_server_id: ac4e1a8b-b841-4275-aceb-53d893b4d107
    enabled_networks: [cephmetrics, management, storage, ctlplane, external, internal_api,
      storage_mgmt, tenant]
    external_ip: 192.168.120.7
    internal_api_ip: 172.17.1.222
    management_ip: 192.168.120.7
    storage_ip: 172.17.3.222
    storage_mgmt_ip: 172.17.4.222
    tenant_ip: 172.17.2.222
osd-compute-2:
  hosts:
    192.168.120.12: {}
  vars:
    cephmetrics_ip: 172.17.5.223
    ctlplane_ip: 192.168.120.12
    deploy_server_id: 6085fc04-9acc-4f32-ae74-ef2b7ef8de16
    enabled_networks: [cephmetrics, management, storage, ctlplane, external, internal_api,
      storage_mgmt, tenant]
    external_ip: 192.168.120.12
    internal_api_ip: 172.17.1.223
    management_ip: 192.168.120.12
    storage_ip: 172.17.3.223
    storage_mgmt_ip: 172.17.4.223
    tenant_ip: 172.17.2.223
osd-compute-3:
  hosts:
    192.168.120.17: {}
  vars:
    cephmetrics_ip: 172.17.5.224
    ctlplane_ip: 192.168.120.17
    deploy_server_id: 1aeff0f0-67e1-4623-93fa-db3ee9059503
    enabled_networks: [cephmetrics, management, storage, ctlplane, external, internal_api,
      storage_mgmt, tenant]
    external_ip: 192.168.120.17
    internal_api_ip: 172.17.1.224
    management_ip: 192.168.120.17
    storage_ip: 172.17.3.224
    storage_mgmt_ip: 172.17.4.224
    tenant_ip: 172.17.2.224
osd-compute-4:
  hosts:
    192.168.120.18: {}
  vars:
    cephmetrics_ip: 172.17.5.225
    ctlplane_ip: 192.168.120.18
    deploy_server_id: 0da2d6cc-1646-4963-a81c-d39f700738c6
    enabled_networks: [cephmetrics, management, storage, ctlplane, external, internal_api,
      storage_mgmt, tenant]
    external_ip: 192.168.120.18
    internal_api_ip: 172.17.1.225
    management_ip: 192.168.120.18
    storage_ip: 172.17.3.225
    storage_mgmt_ip: 172.17.4.225
    tenant_ip: 172.17.2.225
ComputeHCI:
  children:
    osd-compute-0: {}
    osd-compute-1: {}
    osd-compute-2: {}
    osd-compute-3: {}
    osd-compute-4: {}
  vars:
    ansible_ssh_user: heat-admin
    bootstrap_server_id: 641e1af0-6cc5-4c1b-9a3b-230233242e8b
    role_data_cellv2_discovery: true
    role_data_config_settings: {}
    role_data_deploy_steps_tasks: []
    role_data_docker_config:
      step_3:
        iscsid:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-iscsid:latest
          net: host
          privileged: true
          restart: always
          start_order: 2
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/iscsid.json:/var/lib/kolla/config_files/config.json:ro',
            '/dev/:/dev/', '/run/:/run/', '/sys:/sys', '/lib/modules:/lib/modules:ro',
            '/etc/iscsi:/var/lib/kolla/config_files/src-iscsid:ro']
        neutron_ovs_bridge:
          command: [puppet, apply, --modulepath, '/etc/puppet/modules:/usr/share/openstack-puppet/modules',
            --tags, 'file,file_line,concat,augeas,neutron::plugins::ovs::bridge,vs_config',
            -v, -e, 'include neutron::agents::ml2::ovs']
          detach: false
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-server:latest
          net: host
          pid: host
          privileged: true
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/neutron_ovs_agent.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro',
            '/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch', '/etc/puppet:/etc/puppet:ro',
            '/usr/share/openstack-puppet/modules/:/usr/share/openstack-puppet/modules/:ro',
            '/var/run/openvswitch/:/var/run/openvswitch/']
        nova_libvirt:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-nova-libvirt:latest
          net: host
          pid: host
          privileged: true
          restart: always
          start_order: 1
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/nova_libvirt.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova_libvirt/:/var/lib/kolla/config_files/src:ro',
            '/etc/ceph:/var/lib/kolla/config_files/src-ceph:ro', '/lib/modules:/lib/modules:ro',
            '/dev:/dev', '/run:/run', '/sys/fs/cgroup:/sys/fs/cgroup', '/var/lib/nova:/var/lib/nova:shared',
            '/etc/libvirt:/etc/libvirt', '/var/run/libvirt:/var/run/libvirt', '/var/lib/libvirt:/var/lib/libvirt',
            '/var/log/containers/libvirt:/var/log/libvirt', '/var/log/libvirt/qemu:/var/log/libvirt/qemu:ro',
            '/var/lib/vhost_sockets:/var/lib/vhost_sockets', '/sys/fs/selinux:/sys/fs/selinux']
        nova_virtlogd:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-nova-libvirt:latest
          net: host
          pid: host
          privileged: true
          restart: always
          start_order: 0
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/nova_virtlogd.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova_libvirt/:/var/lib/kolla/config_files/src:ro',
            '/lib/modules:/lib/modules:ro', '/dev:/dev', '/run:/run', '/sys/fs/cgroup:/sys/fs/cgroup',
            '/var/lib/nova:/var/lib/nova:shared', '/var/run/libvirt:/var/run/libvirt',
            '/var/lib/libvirt:/var/lib/libvirt', '/etc/libvirt/qemu:/etc/libvirt/qemu:ro',
            '/var/log/libvirt/qemu:/var/log/libvirt/qemu']
      step_4:
        logrotate_crond:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-cron:latest
          net: none
          pid: host
          privileged: true
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/logrotate-crond.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/crond/:/var/lib/kolla/config_files/src:ro',
            '/var/log/containers:/var/log/containers']
        neutron_ovs_agent:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-neutron-openvswitch-agent:latest
          net: host
          pid: host
          privileged: true
          restart: always
          start_order: 10
          ulimit: [nofile=1024]
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/neutron:/var/log/neutron', '/var/lib/kolla/config_files/neutron_ovs_agent.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/neutron/:/var/lib/kolla/config_files/src:ro',
            '/var/lib/docker-config-scripts/neutron_ovs_agent_launcher.sh:/neutron_ovs_agent_launcher.sh:ro',
            '/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch']
        nova_compute:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          healthcheck: {test: /openstack/healthcheck}
          image: 192.168.120.1:8787/rhosp13/openstack-nova-compute:latest
          ipc: host
          net: host
          privileged: true
          restart: always
          ulimit: [nofile=1024]
          user: nova
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/log/containers/nova:/var/log/nova', '/var/lib/kolla/config_files/nova_compute.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova_libvirt/:/var/lib/kolla/config_files/src:ro',
            '/etc/iscsi:/var/lib/kolla/config_files/src-iscsid:ro', '/etc/ceph:/var/lib/kolla/config_files/src-ceph:ro',
            '/dev:/dev', '/lib/modules:/lib/modules:ro', '/run:/run', '/var/lib/nova:/var/lib/nova:shared',
            '/var/lib/libvirt:/var/lib/libvirt', '/sys/class/net:/sys/class/net',
            '/sys/bus/pci:/sys/bus/pci']
        nova_libvirt_init_secret:
          command: [/bin/bash, -c, /usr/bin/virsh secret-define --file /etc/nova/secret.xml
              && /usr/bin/virsh secret-set-value --secret '1ed62898-b2ad-11e8-916e-2047478ccfaa'
              --base64 'AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==']
          detach: false
          image: 192.168.120.1:8787/rhosp13/openstack-nova-libvirt:latest
          privileged: false
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/config-data/puppet-generated/nova_libvirt/etc/nova:/etc/nova:ro',
            '/etc/libvirt:/etc/libvirt', '/var/run/libvirt:/var/run/libvirt', '/var/lib/libvirt:/var/lib/libvirt']
        nova_migration_target:
          environment: [KOLLA_CONFIG_STRATEGY=COPY_ALWAYS]
          image: 192.168.120.1:8787/rhosp13/openstack-nova-compute:latest
          net: host
          privileged: true
          restart: always
          user: root
          volumes: ['/etc/hosts:/etc/hosts:ro', '/etc/localtime:/etc/localtime:ro',
            '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',
            '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',
            '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro', '/dev/log:/dev/log',
            '/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro', '/etc/puppet:/etc/puppet:ro',
            '/var/lib/kolla/config_files/nova-migration-target.json:/var/lib/kolla/config_files/config.json:ro',
            '/var/lib/config-data/puppet-generated/nova_libvirt/:/var/lib/kolla/config_files/src:ro',
            '/etc/ssh/:/host-ssh/:ro', '/run:/run', '/var/lib/nova:/var/lib/nova:shared']
    role_data_docker_config_scripts:
      neutron_ovs_agent_launcher.sh: {content: '#!/bin/bash

          set -xe

          /usr/bin/python -m neutron.cmd.destroy_patch_ports --config-file /usr/share/neutron/neutron-dist.conf
          --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/openvswitch_agent.ini
          --config-dir /etc/neutron/conf.d/common --config-dir /etc/neutron/conf.d/neutron-openvswitch-agent

          /usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf
          --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/openvswitch_agent.ini
          --config-file /etc/neutron/plugins/ml2/ml2_conf.ini --config-dir /etc/neutron/conf.d/common
          --log-file=/var/log/neutron/openvswitch-agent.log

          ', mode: '0755'}
    role_data_docker_puppet_tasks: {}
    role_data_external_deploy_tasks: []
    role_data_external_post_deploy_tasks: []
    role_data_fast_forward_post_upgrade_tasks:
    - name: Register repo type and args
      set_fact:
        fast_forward_repo_args:
          tripleo_repos: {ocata: -b ocata current, pike: -b pike current, queens: -b
              queens current}
        fast_forward_repo_type: custom-script
    - debug: {msg: 'fast_forward_repo_type: {{ fast_forward_repo_type }} fast_forward_repo_args:
          {{ fast_forward_repo_args }}'}
    - block:
      - git: {dest: /home/stack/tripleo-repos/, repo: 'https://github.com/openstack/tripleo-repos.git'}
        name: clone tripleo-repos
      - args: {chdir: /home/stack/tripleo-repos/}
        command: python setup.py install
        name: install tripleo-repos
      - {command: 'tripleo-repos {{ fast_forward_repo_args.tripleo_repos[release]
          }}', name: Enable tripleo-repos}
      when: [ffu_packages_apply|bool, fast_forward_repo_type == 'tripleo-repos']
    - block:
      - copy: {content: "#!/bin/bash\nset -e\necho \"If you use FastForwardRepoType\
            \ 'custom-script' you have to provide the upgrade repo script content.\"\
            \necho \"It will be installed as /root/ffu_upgrade_repo.sh on the node\"\
            \necho \"and passed the upstream name (ocata, pike, queens) of the release\
            \ as first argument\"\ncase $1 in\n  ocata)\n    subscription-manager\
            \ repos --disable=rhel-7-server-openstack-10-rpms\n    subscription-manager\
            \ repos --enable=rhel-7-server-openstack-11-rpms\n    ;;\n  pike)\n  \
            \  subscription-manager repos --disable=rhel-7-server-openstack-11-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-12-rpms\n\
            \    ;;\n  queens)\n    subscription-manager repos --disable=rhel-7-server-openstack-12-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-13-rpms\n\
            \    ;;\n  *)\n    echo \"unknown release $1\" >&2\n    exit 1\nesac\n",
          dest: /root/ffu_update_repo.sh, mode: 448}
        name: Create custom Script for upgrading repo.
      - {name: Execute custom script for upgrading repo., shell: '/root/ffu_update_repo.sh
          {{release}}'}
      when: [ffu_packages_apply|bool, fast_forward_repo_type == 'custom-script']
    role_data_fast_forward_upgrade_tasks:
    - command: systemctl is-enabled --quiet neutron-openvswitch-agent
      ignore_errors: true
      name: Check if neutron_ovs_agent is deployed
      register: neutron_ovs_agent_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact neutron_ovs_agent_enabled
      set_fact: {neutron_ovs_agent_enabled: '{{ neutron_ovs_agent_enabled_result.rc
          == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop neutron_openvswitch_agent
      service: name=neutron-openvswitch-agent state=stopped enabled=no
      when: [step|int == 1, release == 'ocata', neutron_ovs_agent_enabled|bool]
    - command: systemctl is-enabled --quiet openstack-nova-compute
      ignore_errors: true
      name: Check if nova-compute is deployed
      register: nova_compute_enabled_result
      when: [step|int == 0, release == 'ocata']
    - name: Set fact nova_compute_enabled
      set_fact: {nova_compute_enabled: '{{ nova_compute_enabled_result.rc == 0 }}'}
      when: [step|int == 0, release == 'ocata']
    - name: Stop and disable nova-compute service
      service: name=openstack-nova-compute state=stopped
      when: [step|int == 1, nova_compute_enabled|bool, release == 'ocata']
    - name: Register repo type and args
      set_fact:
        fast_forward_repo_args:
          tripleo_repos: {ocata: -b ocata current, pike: -b pike current, queens: -b
              queens current}
        fast_forward_repo_type: custom-script
      when: step|int == 3
    - debug: {msg: 'fast_forward_repo_type: {{ fast_forward_repo_type }} fast_forward_repo_args:
          {{ fast_forward_repo_args }}'}
      when: step|int == 3
    - block:
      - git: {dest: /home/stack/tripleo-repos/, repo: 'https://github.com/openstack/tripleo-repos.git'}
        name: clone tripleo-repos
      - args: {chdir: /home/stack/tripleo-repos/}
        command: python setup.py install
        name: install tripleo-repos
      - {command: 'tripleo-repos {{ fast_forward_repo_args.tripleo_repos[release]
          }}', name: Enable tripleo-repos}
      when: [step|int == 3, ffu_packages_apply|bool, fast_forward_repo_type == 'tripleo-repos']
    - block:
      - copy: {content: "#!/bin/bash\nset -e\necho \"If you use FastForwardRepoType\
            \ 'custom-script' you have to provide the upgrade repo script content.\"\
            \necho \"It will be installed as /root/ffu_upgrade_repo.sh on the node\"\
            \necho \"and passed the upstream name (ocata, pike, queens) of the release\
            \ as first argument\"\ncase $1 in\n  ocata)\n    subscription-manager\
            \ repos --disable=rhel-7-server-openstack-10-rpms\n    subscription-manager\
            \ repos --enable=rhel-7-server-openstack-11-rpms\n    ;;\n  pike)\n  \
            \  subscription-manager repos --disable=rhel-7-server-openstack-11-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-12-rpms\n\
            \    ;;\n  queens)\n    subscription-manager repos --disable=rhel-7-server-openstack-12-rpms\n\
            \    subscription-manager repos --enable=rhel-7-server-openstack-13-rpms\n\
            \    ;;\n  *)\n    echo \"unknown release $1\" >&2\n    exit 1\nesac\n",
          dest: /root/ffu_update_repo.sh, mode: 448}
        name: Create custom Script for upgrading repo.
      - {name: Execute custom script for upgrading repo., shell: '/root/ffu_update_repo.sh
          {{release}}'}
      when: [step|int == 3, ffu_packages_apply|bool, fast_forward_repo_type == 'custom-script']
    role_data_global_config_settings: {}
    role_data_host_prep_tasks:
    - file: {path: '{{ item }}', state: directory}
      name: create persistent logs directory
      with_items: [/var/log/containers/neutron]
    - copy: {content: 'Log files from neutron containers can be found under

          /var/log/containers/neutron and /var/log/containers/httpd/neutron-api.

          ', dest: /var/log/neutron/readme.txt}
      ignore_errors: true
      name: neutron logs readme
    - {name: stat /lib/systemd/system/iscsid.socket, register: stat_iscsid_socket,
      stat: path=/lib/systemd/system/iscsid.socket}
    - {name: Stop and disable iscsid.socket service, service: name=iscsid.socket state=stopped
        enabled=no, when: stat_iscsid_socket.stat.exists}
    - file: {path: /var/log/containers/nova, state: directory}
      name: create persistent logs directory
    - copy: {content: 'Log files from nova containers can be found under

          /var/log/containers/nova and /var/log/containers/httpd/nova-*.

          ', dest: /var/log/nova/readme.txt}
      ignore_errors: true
      name: nova logs readme
    - file: {path: '{{ item }}', state: directory}
      name: create persistent directories
      with_items: [/var/lib/nova, /var/lib/libvirt]
    - file: {path: /etc/ceph, state: directory}
      name: ensure ceph configurations exist
    - name: is Instance HA enabled
      set_fact: {instance_ha_enabled: false}
    - block:
      - file: {path: /var/lib/nova/instanceha, state: directory}
        name: prepare Instance HA script directory
      - copy: {content: "#!/bin/python -utt\n\nimport os\nimport sys\nimport time\n\
            import inspect\nimport logging\nimport argparse\nimport oslo_config.cfg\n\
            import requests.exceptions\n\ndef is_forced_down(connection, hostname):\n\
            \    services = connection.services.list(host=hostname, binary=\"nova-compute\"\
            )\n    for service in services:\n        if service.forced_down:\n   \
            \         return True\n    return False\n\ndef evacuations_done(connection,\
            \ hostname):\n    # Get a list of migrations.\n    #  :param host: (optional)\
            \ filter migrations by host name.\n    #  :param status: (optional) filter\
            \ migrations by status.\n    #  :param cell_name: (optional) filter migrations\
            \ for a cell.\n    #\n    migrations = connection.migrations.list(host=hostname)\n\
            \n    print(\"Checking %d migrations\" % len(migrations))\n    for migration\
            \ in migrations:\n        # print migration.to_dict()\n        #\n   \
            \     # {\n        # u'status': u'error',\n        # u'dest_host': None,\n\
            \        # u'new_instance_type_id': 2,\n        # u'old_instance_type_id':\
            \ 2,\n        # u'updated_at': u'2018-04-22T20:55:29.000000',\n      \
            \  # u'dest_compute':\n        #   u'overcloud-novacompute-2.localdomain',\n\
            \        # u'migration_type': u'live-migration',\n        # u'source_node':\n\
            \        #   u'overcloud-novacompute-0.localdomain',\n        # u'id':\
            \ 8,\n        # u'created_at': u'2018-04-22T20:52:58.000000',\n      \
            \  # u'instance_uuid':\n        #   u'd1c82ce8-3dc5-48db-b59f-854b3b984ef1',\n\
            \        # u'dest_node':\n        #   u'overcloud-novacompute-2.localdomain',\n\
            \        # u'source_compute':\n        #   u'overcloud-novacompute-0.localdomain'\n\
            \        # }\n        # Acceptable: done, completed, failed\n        if\
            \ migration.status in [\"running\", \"accepted\", \"pre-migrating\"]:\n\
            \            return False\n    return True\n\ndef safe_to_start(connection,\
            \ hostname):\n    if is_forced_down(connection, hostname):\n        print(\"\
            Waiting for fence-down flag to be cleared\")\n        return False\n \
            \   if not evacuations_done(connection, hostname):\n        print(\"Waiting\
            \ for evacuations to complete or fail\")\n        return False\n    return\
            \ True\n\ndef create_nova_connection(options):\n    try:\n        from\
            \ novaclient import client\n        from novaclient.exceptions import\
            \ NotAcceptable\n    except ImportError:\n        print(\"Nova not found\
            \ or not accessible\")\n        sys.exit(1)\n\n    from keystoneauth1\
            \ import loading\n    from keystoneauth1 import session\n    from keystoneclient\
            \ import discover\n\n    # Prefer the oldest and strip the leading 'v'\n\
            \    keystone_versions = discover.available_versions(options[\"auth_url\"\
            ][0])\n    keystone_version = keystone_versions[0]['id'][1:]\n    kwargs\
            \ = dict(\n        auth_url=options[\"auth_url\"][0],\n        username=options[\"\
            username\"][0],\n        password=options[\"password\"][0]\n        )\n\
            \n    if discover.version_match(\"2\", keystone_version):\n        kwargs[\"\
            tenant_name\"] = options[\"tenant_name\"][0]\n\n    elif discover.version_match(\"\
            3\", keystone_version):\n        kwargs[\"project_name\"] = options[\"\
            project_name\"][0]\n        kwargs[\"user_domain_name\"] = options[\"\
            user_domain_name\"][0]\n        kwargs[\"project_domain_name\"] = options[\"\
            project_domain_name\"][0]\n\n    loader = loading.get_plugin_loader('password')\n\
            \    keystone_auth = loader.load_from_options(**kwargs)\n    keystone_session\
            \ = session.Session(auth=keystone_auth, verify=(not options[\"insecure\"\
            ]))\n\n    nova_versions = [ \"2.23\", \"2\" ]\n    for version in nova_versions:\n\
            \        clientargs = inspect.getargspec(client.Client).varargs\n    \
            \    # Some versions of Openstack prior to Ocata only\n        # supported\
            \ positional arguments for username,\n        # password, and tenant.\n\
            \        #\n        # Versions since Ocata only support named arguments.\n\
            \        #\n        # So we need to use introspection to figure out how\
            \ to\n        # create a Nova client.\n        #\n        # Happy days\n\
            \        #\n        if clientargs:\n            # OSP < Ocata\n      \
            \      # ArgSpec(args=['version', 'username', 'password', 'project_id',\
            \ 'auth_url'],\n            #         varargs=None,\n            #   \
            \      keywords='kwargs', defaults=(None, None, None, None))\n       \
            \     nova = client.Client(version,\n                                \
            \ None, # User\n                                 None, # Password\n  \
            \                               None, # Tenant\n                     \
            \            None, # Auth URL\n                                 insecure=options[\"\
            insecure\"],\n                                 region_name=options[\"\
            os_region_name\"][0],\n                                 session=keystone_session,\
            \ auth=keystone_auth,\n                                 http_log_debug=options.has_key(\"\
            verbose\"))\n        else:\n            # OSP >= Ocata\n            #\
            \ ArgSpec(args=['version'], varargs='args', keywords='kwargs', defaults=None)\n\
            \            nova = client.Client(version,\n                         \
            \        region_name=options[\"os_region_name\"][0],\n               \
            \                  session=keystone_session, auth=keystone_auth,\n   \
            \                              http_log_debug=options.has_key(\"verbose\"\
            ))\n\n        try:\n            nova.hypervisors.list()\n            return\
            \ nova\n\n        except NotAcceptable as e:\n            logging.warning(e)\n\
            \n        except Exception as e:\n            logging.warning(\"Nova connection\
            \ failed. %s: %s\" % (e.__class__.__name__, e))\n\n    print(\"Couldn't\
            \ obtain a supported connection to nova, tried: %s\\n\" % repr(nova_versions))\n\
            \    return None\n\n\nparser = argparse.ArgumentParser(description='Process\
            \ some integers.')\nparser.add_argument('--config-file', dest='nova_config',\
            \ action='store',\n                    default=\"/etc/nova/nova.conf\"\
            ,\n                    help='path to nova configuration (default: /etc/nova/nova.conf)')\n\
            parser.add_argument('--nova-binary', dest='nova_binary', action='store',\n\
            \                    default=\"/usr/bin/nova-compute\",\n            \
            \        help='path to nova compute binary (default: /usr/bin/nova-compute)')\n\
            parser.add_argument('--enable-file', dest='enable_file', action='store',\n\
            \                    default=\"/var/lib/nova/instanceha/enabled\",\n \
            \                   help='file exists if instance HA is enabled on this\
            \ host '\\\n                    '(default: /var/lib/nova/instanceha/enabled)')\n\
            \n\nsections = {}\n(args, remaining) = parser.parse_known_args(sys.argv)\n\
            \nconfig = oslo_config.cfg.ConfigParser(args.nova_config, sections)\n\
            config.parse()\nconfig.sections[\"placement\"][\"insecure\"] = 0\nconfig.sections[\"\
            placement\"][\"verbose\"] = 1\n\nif os.path.isfile(args.enable_file):\n\
            \    connection = None\n    while not connection:\n        # Loop in case\
            \ the control plane is recovering when we run\n        connection = create_nova_connection(config.sections[\"\
            placement\"])\n        if not connection:\n            time.sleep(10)\n\
            \n    while not safe_to_start(connection, config.sections[\"DEFAULT\"\
            ][\"host\"][0]):\n        time.sleep(10)\n\nreal_args = [args.nova_binary,\
            \ '--config-file', args.nova_config]\nreal_args.extend(remaining[1:])\n\
            os.execv(args.nova_binary, real_args)\n", dest: /var/lib/nova/instanceha/check-run-nova-compute,
          mode: 493}
        name: install Instance HA script that runs nova-compute
      - {command: hiera -c /etc/puppet/hiera.yaml compute_instanceha_short_node_names,
        name: Get list of instance HA compute nodes, register: iha_nodes}
      - {file: path=/var/lib/nova/instanceha/enabled state=touch, name: If instance
          HA is enabled on the node activate the evacuation completed check, when: iha_nodes.stdout|lower
          | search('"'+ansible_hostname|lower+'"')}
      name: install Instance HA recovery script
      when: instance_ha_enabled|bool
    - file: {path: '{{ item }}', state: directory}
      name: create libvirt persistent data directories
      with_items: [/etc/libvirt, /etc/libvirt/secrets, /etc/libvirt/qemu, /var/lib/libvirt,
        /var/log/containers/libvirt]
    - group: {gid: 107, name: qemu, state: present}
      name: ensure qemu group is present on the host
    - name: ensure qemu user is present on the host
      user: {comment: qemu user, group: qemu, name: qemu, shell: /sbin/nologin, state: present,
        uid: 107}
    - file: {group: qemu, owner: qemu, path: /var/lib/vhost_sockets, setype: virt_cache_t,
        seuser: system_u, state: directory}
      name: create directory for vhost-user sockets with qemu ownership
    - {command: /usr/bin/rpm -q libvirt-daemon, failed_when: false, name: check if
        libvirt is installed, register: libvirt_installed}
    - name: make sure libvirt services are disabled
      service: {enabled: false, name: '{{ item }}', state: stopped}
      when: libvirt_installed.rc == 0
      with_items: [libvirtd.service, virtlogd.socket]
    role_data_kolla_config:
      /var/lib/kolla/config_files/iscsid.json:
        command: /usr/sbin/iscsid -f
        config_files:
        - {dest: /etc/iscsi/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-iscsid/*}
      /var/lib/kolla/config_files/logrotate-crond.json:
        command: /usr/sbin/crond -s -n
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
      /var/lib/kolla/config_files/neutron_ovs_agent.json:
        command: /neutron_ovs_agent_launcher.sh
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        permissions:
        - {owner: 'neutron:neutron', path: /var/log/neutron, recurse: true}
      /var/lib/kolla/config_files/nova-migration-target.json:
        command: /usr/sbin/sshd -D -p 2022
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /etc/ssh/, owner: root, perm: '0600', source: /host-ssh/ssh_host_*_key}
      /var/lib/kolla/config_files/nova_compute.json:
        command: '/usr/bin/nova-compute '
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /etc/iscsi/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-iscsid/*}
        - {dest: /etc/ceph/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-ceph/}
        permissions:
        - {owner: 'nova:nova', path: /var/log/nova, recurse: true}
        - {owner: 'nova:nova', path: /var/lib/nova, recurse: true}
        - {owner: 'nova:nova', path: /etc/ceph/ceph.client.openstack.keyring, perm: '0600'}
      /var/lib/kolla/config_files/nova_libvirt.json:
        command: /usr/sbin/libvirtd
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
        - {dest: /etc/ceph/, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src-ceph/}
        permissions:
        - {owner: 'nova:nova', path: /etc/ceph/ceph.client.openstack.keyring, perm: '0600'}
      /var/lib/kolla/config_files/nova_virtlogd.json:
        command: /usr/sbin/virtlogd --config /etc/libvirt/virtlogd.conf
        config_files:
        - {dest: /, merge: true, preserve_properties: true, source: /var/lib/kolla/config_files/src/*}
    role_data_logging_groups: [root]
    role_data_logging_sources: []
    role_data_merged_config_settings:
      ceph_osd_ansible_vars:
        ceph_conf_overrides:
          global: {max_open_files: 131072, mon_max_pg_per_osd: 1000, mon_osd_full_ratio: 90,
            osd_journal_size: 10240, osd_pool_default_pg_num: 32, osd_pool_default_pgp_num: 32,
            osd_pool_default_size: 3, rgw_keystone_accepted_roles: 'Member, admin',
            rgw_keystone_admin_domain: default, rgw_keystone_admin_password: pFavNu2xcWbJFKHgBatfFpn9Z,
            rgw_keystone_admin_project: service, rgw_keystone_admin_user: swift, rgw_keystone_api_version: 3,
            rgw_keystone_implicit_tenants: 'true', rgw_keystone_revocation_interval: '0',
            rgw_keystone_url: 'http://172.17.1.13:5000', rgw_s3_auth_use_keystone: 'true'}
        ceph_docker_image: rhceph/rhceph-3-rhel7
        ceph_docker_image_tag: latest
        ceph_docker_registry: 192.168.120.1:8787
        ceph_origin: distro
        ceph_stable: true
        cluster: ceph
        cluster_network: 172.17.4.0/24
        containerized_deployment: true
        dedicated_devices: [/dev/nvme0n1, /dev/nvme0n1, /dev/nvme0n1, /dev/nvme0n1,
          /dev/nvme0n1, /dev/nvme0n1, /dev/nvme0n1, /dev/nvme0n1, /dev/nvme0n1, /dev/nvme0n1,
          /dev/nvme0n1, /dev/nvme0n1]
        devices: [/dev/sdb, /dev/sdc, /dev/sdd, /dev/sde, /dev/sdf, /dev/sdg, /dev/sdh,
          /dev/sdi, /dev/sdj, /dev/sdk, /dev/sdl, /dev/sdm]
        docker: true
        fsid: 1ed62898-b2ad-11e8-916e-2047478ccfaa
        generate_fsid: false
        ip_version: ipv4
        ireallymeanit: 'yes'
        journal_size: 10240
        keys:
        - {key: AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==, mgr_cap: allow *, mode: '0600',
          mon_cap: profile rbd, name: client.openstack, osd_cap: 'profile rbd pool=volumes,
            profile rbd pool=backups, profile rbd pool=vms, profile rbd pool=images,
            profile rbd pool=metrics'}
        - {key: AQDNj5JbAAAAABAAkqBgz7ALcbRkZuO5m5jr2A==, mds_cap: allow *, mgr_cap: allow
            *, mode: '0600', mon_cap: 'allow r, allow command \"auth del\", allow
            command \"auth caps\", allow command \"auth get\", allow command \"auth
            get-or-create\"', name: client.manila, osd_cap: allow rw}
        - {key: AQDNj5JbAAAAABAAZXB7q7DHSABFHtBxlhIvWw==, mgr_cap: allow *, mode: '0600',
          mon_cap: allow rw, name: client.radosgw, osd_cap: allow rwx}
        monitor_address_block: 172.17.3.0/24
        ntp_service_enabled: false
        openstack_config: true
        openstack_keys:
        - {key: AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==, mgr_cap: allow *, mode: '0600',
          mon_cap: profile rbd, name: client.openstack, osd_cap: 'profile rbd pool=volumes,
            profile rbd pool=backups, profile rbd pool=vms, profile rbd pool=images,
            profile rbd pool=metrics'}
        - {key: AQDNj5JbAAAAABAAkqBgz7ALcbRkZuO5m5jr2A==, mds_cap: allow *, mgr_cap: allow
            *, mode: '0600', mon_cap: 'allow r, allow command \"auth del\", allow
            command \"auth caps\", allow command \"auth get\", allow command \"auth
            get-or-create\"', name: client.manila, osd_cap: allow rw}
        - {key: AQDNj5JbAAAAABAAZXB7q7DHSABFHtBxlhIvWw==, mgr_cap: allow *, mode: '0600',
          mon_cap: allow rw, name: client.radosgw, osd_cap: allow rwx}
        openstack_pools:
        - {name: images, pg_num: 32, rule_name: ''}
        - {name: metrics, pg_num: 32, rule_name: ''}
        - {name: backups, pg_num: 32, rule_name: ''}
        - {name: vms, pg_num: 32, rule_name: ''}
        - {name: volumes, pg_num: 32, rule_name: ''}
        osd_objectstore: filestore
        osd_scenario: non-collocated
        pools: []
        public_network: 172.17.3.0/24
        user_config: true
      cold_migration_ssh_inbound_addr: internal_api
      kernel_modules:
        nf_conntrack: {}
        nf_conntrack_proto_sctp: {}
      live_migration_ssh_inbound_addr: internal_api
      neutron::agents::ml2::ovs::arp_responder: false
      neutron::agents::ml2::ovs::bridge_mappings: ['datacentre:br-ex', 'floating:br-floating']
      neutron::agents::ml2::ovs::enable_distributed_routing: false
      neutron::agents::ml2::ovs::extensions: [qos]
      neutron::agents::ml2::ovs::l2_population: 'False'
      neutron::agents::ml2::ovs::local_ip: tenant
      neutron::agents::ml2::ovs::tunnel_types: [vxlan]
      neutron::allow_overlapping_ips: true
      neutron::core_plugin: ml2
      neutron::db::database_db_max_retries: -1
      neutron::db::database_max_retries: -1
      neutron::db::sync::db_sync_timeout: 300
      neutron::db::sync::extra_params: ''
      neutron::debug: true
      neutron::dhcp_agent_notification: true
      neutron::dns_domain: openstacklocal
      neutron::global_physnet_mtu: 1500
      neutron::host: '%{::fqdn}'
      neutron::notification_driver: noop
      neutron::plugins::ml2::extension_drivers: [qos, port_security]
      neutron::plugins::ml2::firewall_driver: iptables_hybrid
      neutron::plugins::ml2::flat_networks: [datacentre, floating]
      neutron::plugins::ml2::mechanism_drivers: [openvswitch]
      neutron::plugins::ml2::network_vlan_ranges: ['datacentre:1:1000']
      neutron::plugins::ml2::overlay_ip_version: 4
      neutron::plugins::ml2::tenant_network_types: [vxlan]
      neutron::plugins::ml2::tunnel_id_ranges: ['1:4094']
      neutron::plugins::ml2::type_drivers: [vxlan, vlan, flat, gre]
      neutron::plugins::ml2::vni_ranges: ['1:4094']
      neutron::purge_config: false
      neutron::rabbit_heartbeat_timeout_threshold: 60
      neutron::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      neutron::rabbit_port: 5672
      neutron::rabbit_use_ssl: 'False'
      neutron::rabbit_user: guest
      neutron::service_plugins: [router, qos, trunk]
      nova::api_database_connection: mysql+pymysql://nova_api:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova_api?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::cell0_database_connection: mysql+pymysql://nova:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova_cell0?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::cinder_catalog_info: volumev3:cinderv3:internalURL
      nova::compute::instance_usage_audit: true
      nova::compute::instance_usage_audit_period: hour
      nova::compute::libvirt::libvirt_enabled_perf_events: []
      nova::compute::libvirt::libvirt_virt_type: kvm
      nova::compute::libvirt::manage_libvirt_services: false
      nova::compute::libvirt::migration_support: false
      nova::compute::libvirt::qemu::configure_qemu: true
      nova::compute::libvirt::qemu::max_files: 32768
      nova::compute::libvirt::qemu::max_processes: 131072
      nova::compute::libvirt::services::libvirt_virt_type: kvm
      nova::compute::libvirt::vncserver_listen: internal_api
      nova::compute::neutron::libvirt_vif_driver: ''
      nova::compute::pci::passthrough: ''
      nova::compute::rbd::ephemeral_storage: true
      nova::compute::rbd::libvirt_images_rbd_ceph_conf: /etc/ceph/ceph.conf
      nova::compute::rbd::libvirt_images_rbd_pool: vms
      nova::compute::rbd::libvirt_rbd_secret_key: AQDNj5JbAAAAABAAZeYg7vyIgf8wUm/h1UREZw==
      nova::compute::rbd::libvirt_rbd_secret_uuid: 1ed62898-b2ad-11e8-916e-2047478ccfaa
      nova::compute::rbd::libvirt_rbd_user: openstack
      nova::compute::rbd::rbd_keyring: client.openstack
      nova::compute::reserved_host_memory: 37500
      nova::compute::vcpu_pin_set: []
      nova::compute::verify_glance_signatures: false
      nova::compute::vncproxy_host: 172.21.1.153
      nova::compute::vncserver_proxyclient_address: internal_api
      nova::cpu_allocation_ratio: 0.5
      nova::cron::archive_deleted_rows::destination: /var/log/nova/nova-rowsflush.log
      nova::cron::archive_deleted_rows::hour: '0'
      nova::cron::archive_deleted_rows::max_rows: '100'
      nova::cron::archive_deleted_rows::minute: '1'
      nova::cron::archive_deleted_rows::month: '*'
      nova::cron::archive_deleted_rows::monthday: '*'
      nova::cron::archive_deleted_rows::until_complete: false
      nova::cron::archive_deleted_rows::user: nova
      nova::cron::archive_deleted_rows::weekday: '*'
      nova::database_connection: mysql+pymysql://nova:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::db::database_db_max_retries: -1
      nova::db::database_max_retries: -1
      nova::db::sync::db_sync_timeout: 300
      nova::db::sync_api::db_sync_timeout: 300
      nova::debug: true
      nova::glance_api_servers: http://172.17.1.13:9292
      nova::host: '%{::fqdn}'
      nova::migration::live_migration_tunnelled: true
      nova::my_ip: internal_api
      nova::network::neutron::dhcp_domain: ''
      nova::network::neutron::neutron_auth_type: v3password
      nova::network::neutron::neutron_auth_url: http://192.168.120.10:35357/v3
      nova::network::neutron::neutron_ovs_bridge: br-int
      nova::network::neutron::neutron_password: G8Hfuesb4Zp2TBAwDxAdyrQyY
      nova::network::neutron::neutron_project_name: service
      nova::network::neutron::neutron_region_name: regionOne
      nova::network::neutron::neutron_url: http://172.17.1.13:9696
      nova::network::neutron::neutron_username: neutron
      nova::notification_driver: noop
      nova::notification_format: unversioned
      nova::notify_on_state_change: vm_and_task_state
      nova::placement::auth_url: http://172.17.1.13:5000
      nova::placement::os_interface: internal
      nova::placement::os_region_name: regionOne
      nova::placement::password: dcYx3yVe7heXKRR9QnAEDRA9z
      nova::placement::project_name: service
      nova::placement_database_connection: mysql+pymysql://nova_placement:dcYx3yVe7heXKRR9QnAEDRA9z@172.17.1.13/nova_placement?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf
      nova::purge_config: false
      nova::rabbit_heartbeat_timeout_threshold: 60
      nova::rabbit_password: BXxHGhfV3KNrVQDkDKYDzhvr2
      nova::rabbit_port: 5672
      nova::rabbit_use_ssl: 'False'
      nova::rabbit_userid: guest
      nova::use_ipv6: false
      nova::vncproxy::common::vncproxy_host: 172.21.1.153
      nova::vncproxy::common::vncproxy_port: '6080'
      nova::vncproxy::common::vncproxy_protocol: http
      ntp::iburst_enable: true
      'ntp::maxpoll:': 10
      'ntp::minpoll:': 6
      ntp::servers: [pool.ntp.org]
      rbd_persistent_storage: true
      snmp::agentaddress: ['udp:161', 'udp6:[::1]:161']
      snmp::snmpd_options: -LS0-5d
      snmpd_network: internal_api_subnet
      sysctl_settings:
        fs.inotify.max_user_instances: {value: 1024}
        fs.suid_dumpable: {value: 0}
        kernel.dmesg_restrict: {value: 1}
        kernel.pid_max: {value: 1048576}
        net.core.netdev_max_backlog: {value: 10000}
        net.ipv4.conf.all.arp_accept: {value: 1}
        net.ipv4.conf.all.log_martians: {value: 1}
        net.ipv4.conf.all.secure_redirects: {value: 0}
        net.ipv4.conf.all.send_redirects: {value: 0}
        net.ipv4.conf.default.accept_redirects: {value: 0}
        net.ipv4.conf.default.log_martians: {value: 1}
        net.ipv4.conf.default.secure_redirects: {value: 0}
        net.ipv4.conf.default.send_redirects: {value: 0}
        net.ipv4.ip_forward: {value: 1}
        net.ipv4.neigh.default.gc_thresh1: {value: 1024}
        net.ipv4.neigh.default.gc_thresh2: {value: 2048}
        net.ipv4.neigh.default.gc_thresh3: {value: 4096}
        net.ipv4.tcp_keepalive_intvl: {value: 1}
        net.ipv4.tcp_keepalive_probes: {value: 5}
        net.ipv4.tcp_keepalive_time: {value: 5}
        net.ipv6.conf.all.accept_ra: {value: 0}
        net.ipv6.conf.all.accept_redirects: {value: 0}
        net.ipv6.conf.all.autoconf: {value: 0}
        net.ipv6.conf.all.disable_ipv6: {value: 0}
        net.ipv6.conf.default.accept_ra: {value: 0}
        net.ipv6.conf.default.accept_redirects: {value: 0}
        net.ipv6.conf.default.autoconf: {value: 0}
        net.ipv6.conf.default.disable_ipv6: {value: 0}
        net.netfilter.nf_conntrack_max: {value: 500000}
        net.nf_conntrack_max: {value: 500000}
      timezone::timezone: UTC
      tripleo.ceph_osd.firewall_rules:
        111 ceph_osd:
          dport: [6800-7300]
      tripleo.neutron_ovs_agent.firewall_rules:
        118 neutron vxlan networks: {dport: 4789, proto: udp}
        136 neutron gre networks: {proto: gre}
      tripleo.nova_libvirt.firewall_rules:
        200 nova_libvirt:
          dport: [16514, 49152-49215, 5900-6923]
      tripleo.nova_migration_target.firewall_rules:
        113 nova_migration_target:
          dport: [2022]
      tripleo.ntp.firewall_rules:
        105 ntp: {dport: 123, proto: udp}
      tripleo.snmp.firewall_rules:
        124 snmp: {dport: 161, proto: udp, source: '%{hiera(''snmpd_network'')}'}
      tripleo::firewall::manage_firewall: true
      tripleo::firewall::purge_firewall_rules: false
      tripleo::packages::enable_install: false
      tripleo::profile::base::certmonger_user::libvirt_postsave_cmd: 'true'
      tripleo::profile::base::database::mysql::client::enable_ssl: false
      tripleo::profile::base::database::mysql::client::mysql_client_bind_address: internal_api
      tripleo::profile::base::database::mysql::client::ssl_ca: /etc/ipa/ca.crt
      tripleo::profile::base::docker::additional_sockets: [/var/lib/openstack/docker.sock]
      tripleo::profile::base::docker::configure_network: true
      tripleo::profile::base::docker::debug: true
      tripleo::profile::base::docker::docker_options: --log-driver=journald --signature-verification=false
        --iptables=false --live-restore
      tripleo::profile::base::docker::insecure_registries: ['192.168.120.1:8787']
      tripleo::profile::base::docker::network_options: --bip=172.31.0.1/24
      tripleo::profile::base::nova::compute::cinder_nfs_backend: false
      tripleo::profile::base::nova::migration::client::libvirt_enabled: true
      tripleo::profile::base::nova::migration::client::nova_compute_enabled: true
      tripleo::profile::base::nova::migration::client::ssh_port: 2022
      tripleo::profile::base::nova::migration::client::ssh_private_key: '-----BEGIN
        RSA PRIVATE KEY-----

        MIIEpAIBAAKCAQEAy1JU+0Mgs0I3LhT82BCCyFjpK9SgDNxC78xI0PrLZJQeo6tc

        fTQlp/XEFu3MjIx5SYIzl1s1dDd+3rdLnX9AExoNCx+j2cZKuLm8oEes5QvVrGzu

        dDIVeckihaOOGKumZGj5iyw8QyByPI3Y2AMLHIKziBaUnF/JvXDhUkij4KpivJe0

        pCh7ikebugh7jjq6Lg1AKi7S2onlmySccHsEXf7c6V2ZAAXDreW9yJ6MyZD0+o4D

        uQ/fy7AS5/R+GbCgrBIgmJ3jc9zD+69jSv9+NWlxI6ABYSqMMubM/df5ZCnzPk7M

        MkGf8Yky3t4pc4JWkUxqI5z8GFL4W0n/7tnVBQIDAQABAoIBAQCPl1nFCadycfUe

        alwXC7IRZBlohwVxj60GRbO3QyeZpLHuzPrIwadGc19FAjRPnnWZ4IYnApdBuF+1

        UqXoWRIba26CyJmjF1N2GlaIq5YDPNRurY/KKATauZ3T26pi6fbdOvXhwB/fGG8V

        LOhTxfJ29CdumOPjfBKx46J9xidWZjv4kRla3Zj9i9PWoGGuLV29drLBDxknNEWt

        C3UEdLvl4iJHtyfXnWCtM/QRZcdt1Cl9uWy8/D3Y5+cGAUEMz+XiCSm15K1woNi8

        38zvEjHOYlSY3u06O6cDkRVuvpSiZpSmCBQ+qeA2UhSlCW1OXw51J/N1QO5xACvm

        U4BS+4URAoGBAPJAq39fIWRqz97tW3WGbWuRRj5PgbshTchc0RFvJu9eQFeGbG3N

        VyN7ZqPiXEp/9WXfHLfKaBhiKfX/CbGzJR2mEp62/WSwqWr1IB6bC1klReEQ9nV0

        nBKRnEUcX2Gjr1HeAa8+dx7oJ1WJ9K/MWLPQalW2HCJ+j5bUWupQUv+XAoGBANbc

        F0uo27SWDKBP3yIi4yqUFIHjg/7jLLBRgHdK/hrp8MlXj+9bze4VIlO5jrzssB3B

        DHwbCAD+zw6bZGYQee4jc2VNUJBYFZVCsMEkglrSY6mWK+96FKbmj8venfEmkJU6

        AAT/qPhJEL2WlmhgAktRL7uMQQBDYCYLEDvsvKPDAoGAWfHy+wbKMiuKZL4CQD+a

        Lt8wkoAYre8unx7/LRb1ANkg0J6Dcb1VBkKSGHU6SRr7dUiCtCa9wZfo9oIsCkN2

        m2yG61EMAi+pp6G9vD9a1k2FhYwzxeZ28FoKCIsBSxJoTOA+BeAOgHY37IWdSQx5

        PsI6SPqltZUCIUFVRDxwpJ8CgYBvMs0jrSq4FYhHV88FncSv/p92cceYnP496pBw

        LPHCAB1MNvMZUAwOEgYICtri0CR94bGScXL9AOMsjiOx5wkTxGkunor5iXzyGOfd

        rgWA96340O3ALRCPKHh3bKmVav4dHT1xWNecWEQ/urXxIu9NB6GKkRX7CuOyDLvE

        J8uhfQKBgQCtRdeWIC9KdEbzHc/UyGHYvPJcAph8PPUwb3ow9hUbSH11fXkVO9rC

        45ljfQ0A5MdThXlnsOtvCZAk11ZczDSEGjyY0Wk6qQTtfgj0hKl93h1AMqe0RsA9

        m0Txo2Z6WTukxXiFK9Zn4/gN79dcAnXgWs/HRo+e2d1pzmoj1tQexg==

        -----END RSA PRIVATE KEY-----

        '
      tripleo::profile::base::nova::migration::target::ssh_authorized_keys: [ssh-rsa
          AAAAB3NzaC1yc2EAAAADAQABAAABAQDLUlT7QyCzQjcuFPzYEILIWOkr1KAM3ELvzEjQ+stklB6jq1x9NCWn9cQW7cyMjHlJgjOXWzV0N37et0udf0ATGg0LH6PZxkq4ubygR6zlC9WsbO50MhV5ySKFo44Yq6ZkaPmLLDxDIHI8jdjYAwscgrOIFpScX8m9cOFSSKPgqmK8l7SkKHuKR5u6CHuOOrouDUAqLtLaieWbJJxwewRd/tzpXZkABcOt5b3InozJkPT6jgO5D9/LsBLn9H4ZsKCsEiCYneNz3MP7r2NK/341aXEjoAFhKowy5sz91/lkKfM+TswyQZ/xiTLe3ilzglaRTGojnPwYUvhbSf/u2dUF
          Generated by TripleO]
      tripleo::profile::base::nova::migration::target::ssh_localaddrs: ['%{hiera(''cold_migration_ssh_inbound_addr'')}',
        '%{hiera(''live_migration_ssh_inbound_addr'')}']
      tripleo::profile::base::snmp::snmpd_password: dabc299cc94cdf8efe32e6f57c20a0da685fc850
      tripleo::profile::base::snmp::snmpd_user: ro_snmp_user
      tripleo::profile::base::sshd::bannertext: ''
      tripleo::profile::base::sshd::motd: ''
      tripleo::profile::base::sshd::options:
        AcceptEnv: [LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES,
          LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT, LC_IDENTIFICATION
            LC_ALL LANGUAGE, XMODIFIERS]
        AuthorizedKeysFile: .ssh/authorized_keys
        ChallengeResponseAuthentication: 'no'
        GSSAPIAuthentication: 'yes'
        GSSAPICleanupCredentials: 'no'
        HostKey: [/etc/ssh/ssh_host_rsa_key, /etc/ssh/ssh_host_ecdsa_key, /etc/ssh/ssh_host_ed25519_key]
        PasswordAuthentication: 'no'
        Subsystem: sftp  /usr/libexec/openssh/sftp-server
        SyslogFacility: AUTHPRIV
        UseDNS: 'no'
        UsePAM: 'yes'
        UsePrivilegeSeparation: sandbox
        X11Forwarding: 'yes'
      tripleo::profile::base::sshd::port: 22
      tripleo::profile::base::tuned::profile: ''
      tripleo::trusted_cas::ca_map: {}
      vswitch::ovs::enable_hw_offload: false
    role_data_monitoring_subscriptions: []
    role_data_post_update_tasks: []
    role_data_post_upgrade_tasks: []
    role_data_pre_upgrade_rolling_tasks: []
    role_data_puppet_config:
    - {config_image: '', config_volume: '', step_config: ''}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-neutron-server:latest',
      config_volume: neutron, puppet_tags: neutron_plugin_ml2, step_config: 'include
        ::tripleo::profile::base::neutron::plugins::ml2

        '}
    - config_image: 192.168.120.1:8787/rhosp13/openstack-neutron-server:latest
      config_volume: neutron
      puppet_tags: neutron_config,neutron_agent_ovs,neutron_plugin_ml2
      step_config: 'include ::tripleo::profile::base::neutron::ovs

        '
      volumes: ['/lib/modules:/lib/modules:ro', '/run/openvswitch:/run/openvswitch']
    - config_image: 192.168.120.1:8787/rhosp13/openstack-iscsid:latest
      config_volume: iscsid
      puppet_tags: iscsid_config
      step_config: include ::tripleo::profile::base::iscsid
      volumes: ['/etc/iscsi:/etc/iscsi']
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-compute:latest', config_volume: nova_libvirt,
      puppet_tags: 'nova_config,nova_paste_api_ini', step_config: '# TODO(emilien):
        figure how to deal with libvirt profile.

        # We''ll probably treat it like we do with Neutron plugins.

        # Until then, just include it in the default nova-compute role.

        include tripleo::profile::base::nova::compute::libvirt


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-compute:latest', config_volume: nova_libvirt,
      puppet_tags: 'libvirtd_config,nova_config,file,libvirt_tls_password', step_config: 'include
        tripleo::profile::base::nova::libvirt


        include ::tripleo::profile::base::database::mysql::client'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-nova-compute:latest', config_volume: nova_libvirt,
      step_config: 'include ::tripleo::profile::base::sshd

        include tripleo::profile::base::nova::migration::target'}
    - {config_image: '192.168.120.1:8787/rhosp13/openstack-cron:latest', config_volume: crond,
      step_config: 'include ::tripleo::profile::base::logging::logrotate'}
    role_data_service_config_settings: {}
    role_data_service_metadata_settings: null
    role_data_service_names: [ca_certs, ceph_client, ceph_osd, neutron_plugin_ml2,
      neutron_ovs_agent, docker, iscsid, kernel, mysql_client, nova_compute, nova_libvirt,
      nova_migration_target, ntp, logrotate_crond, snmp, sshd, timezone, tripleo_firewall,
      tripleo_packages, tuned]
    role_data_step_config: "# Copyright 2014 Red Hat, Inc.\n# All Rights Reserved.\n\
      #\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n\
      # not use this file except in compliance with the License. You may obtain\n\
      # a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n\
      #\n# Unless required by applicable law or agreed to in writing, software\n#\
      \ distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\
      # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\
      # License for the specific language governing permissions and limitations\n\
      # under the License.\n\n# Common config, from tripleo-heat-templates/puppet/manifests/overcloud_common.pp\n\
      # The content of this file will be used to generate\n# the puppet manifests\
      \ for all roles, the placeholder\n# ComputeHCI will be replaced by 'controller',\
      \ 'blockstorage',\n# 'cephstorage' and all the deployed roles.\n\nif hiera('step')\
      \ >= 4 {\n  hiera_include('ComputeHCI_classes', [])\n}\n\n$package_manifest_name\
      \ = join(['/var/lib/tripleo/installed-packages/overcloud_ComputeHCI', hiera('step')])\n\
      package_manifest{$package_manifest_name: ensure => present}\n\n# End of overcloud_common.pp\n\
      \ninclude ::tripleo::trusted_cas\ninclude ::tripleo::profile::base::docker\n\
      \ninclude ::tripleo::profile::base::kernel\ninclude ::tripleo::profile::base::database::mysql::client\n\
      include ::tripleo::profile::base::time::ntp\ninclude ::tripleo::profile::base::snmp\n\
      \ninclude ::tripleo::profile::base::sshd\n\ninclude ::timezone\ninclude ::tripleo::firewall\n\
      \ninclude ::tripleo::packages\n\ninclude ::tripleo::profile::base::tuned"
    role_data_update_tasks:
    - {lineinfile: dest=/etc/sysconfig/iptables regexp=".*neutron-" state=absent,
      name: Remove IPv4 iptables rules created by Neutron that are persistent, when: step|int
        == 5}
    - {lineinfile: dest=/etc/sysconfig/ip6tables regexp=".*neutron-" state=absent,
      name: Remove IPv6 iptables rules created by Neutron that are persistent, when: step|int
        == 5}
    - block:
      - {failed_when: false, name: Detect if puppet on the docker profile would restart
          the service, register: puppet_docker_noop_output, shell: "puppet apply --noop\
          \ --summarize --detailed-exitcodes --verbose \\\n  --modulepath /etc/puppet/modules:/opt/stack/puppet-modules:/usr/share/openstack-puppet/modules\
          \ \\\n  --color=false -e \"class { 'tripleo::profile::base::docker': step\
          \ => 1, }\" 2>&1 | \\\nawk -F \":\" '/Out of sync:/ { print $2}'\n"}
      - {changed_when: docker_check_update.rc == 100, failed_when: 'docker_check_update.rc
          not in [0, 100]', name: Is docker going to be updated, register: docker_check_update,
        shell: yum check-update docker}
      - {name: Set docker_rpm_needs_update fact, set_fact: 'docker_rpm_needs_update={{
          docker_check_update.rc == 100 }}'}
      - {name: Set puppet_docker_is_outofsync fact, set_fact: 'puppet_docker_is_outofsync={{
          puppet_docker_noop_output.stdout|trim|int >= 1 }}'}
      - {name: Stop all containers, shell: docker ps -q | xargs --no-run-if-empty
          -n1 docker stop, when: puppet_docker_is_outofsync or docker_rpm_needs_update}
      - name: Stop docker
        service: {name: docker, state: stopped}
        when: puppet_docker_is_outofsync or docker_rpm_needs_update
      - {name: Update the docker package, when: docker_rpm_needs_update, yum: name=docker
          state=latest update_cache=yes}
      - {changed_when: puppet_docker_apply.rc == 2, failed_when: 'puppet_docker_apply.rc
          not in [0, 2]', name: Apply puppet which will start the service again, register: puppet_docker_apply,
        shell: "puppet apply --detailed-exitcodes --verbose \\\n  --modulepath  /etc/puppet/modules:/opt/stack/puppet-modules:/usr/share/openstack-puppet/modules\
          \ \\\n  -e \"class { 'tripleo::profile::base::docker': step => 1, }\"\n"}
      when: step|int == 2
    - {name: Check for existing yum.pid, register: yum_pid_file, stat: path=/var/run/yum.pid,
      when: step|int == 0 or step|int == 3}
    - {fail: msg="ERROR existing yum.pid detected - can't continue! Please ensure
        there is no other package update process for the duration of the minor update
        worfklow. Exiting.", name: Exit if existing yum process, when: (step|int ==
        0 or step|int == 3) and yum_pid_file.stat.exists}
    - {name: Update all packages, when: step == "3", yum: name=* state=latest update_cache=yes}
    role_data_upgrade_batch_tasks: []
    role_data_upgrade_tasks:
    - {name: Check legacy Ceph hieradata, shell: 'test "nil" == "$(hiera -c /etc/puppet/hiera.yaml
        ceph::profile::params::osds)"', tags: validation, when: step|int == 0}
    - {ignore_errors: true, name: Check openvswitch version., register: ovs_version,
      shell: 'rpm -qa | awk -F- ''/^openvswitch-2/{print $2 "-" $3}''', when: step|int
        == 2}
    - {ignore_errors: true, name: Check openvswitch packaging., register: ovs_packaging_issue,
      shell: 'rpm -q --scripts openvswitch | awk ''/postuninstall/,/*/'' | grep -q
        "systemctl.*try-restart"', when: step|int == 2}
    - block:
      - file: {path: /root/OVS_UPGRADE, state: absent}
        name: 'Ensure empty directory: emptying.'
      - file: {group: root, mode: 488, owner: root, path: /root/OVS_UPGRADE, state: directory}
        name: 'Ensure empty directory: creating.'
      - {command: yum makecache, name: Make yum cache.}
      - {command: yumdownloader --destdir /root/OVS_UPGRADE --resolve openvswitch,
        name: Download OVS packages.}
      - {name: Get rpm list for manual upgrade of OVS., register: ovs_list_of_rpms,
        shell: ls -1 /root/OVS_UPGRADE/*.rpm}
      - args: {chdir: /root/OVS_UPGRADE}
        name: Manual upgrade of OVS
        shell: 'rpm -U --test {{item}} 2>&1 | grep "already installed" || \

          rpm -U --replacepkgs --notriggerun --nopostun {{item}};

          '
        with_items: ['{{ovs_list_of_rpms.stdout_lines}}']
      when: [step|int == 2, '''2.5.0-14'' in ovs_version.stdout|default('''') or ovs_packaging_issue|default(false)|succeeded']
    - {command: systemctl is-enabled --quiet neutron-openvswitch-agent, ignore_errors: true,
      name: Check if neutron_ovs_agent is deployed, register: neutron_ovs_agent_enabled,
      tags: common}
    - command: systemctl is-active --quiet neutron-openvswitch-agent
      name: 'PreUpgrade step0,validation: Check service neutron-openvswitch-agent
        is running'
      tags: validation
      when: [step|int == 0, neutron_ovs_agent_enabled.rc == 0]
    - name: Stop and disable neutron_ovs_agent service
      service: name=neutron-openvswitch-agent state=stopped enabled=no
      when: [step|int == 2, neutron_ovs_agent_enabled.rc == 0]
    - name: Set fact for removal of openstack-neutron-openvswitch package
      set_fact: {remove_neutron_openvswitch_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-neutron-openvswitch package if operator requests it
      when: [step|int == 2, remove_neutron_openvswitch_package|bool]
      yum: name=openstack-neutron-openvswitch state=removed
    - {name: Install docker packages on upgrade if missing, when: step|int == 3, yum: name=docker
        state=latest}
    - {command: systemctl is-enabled --quiet iscsid, ignore_errors: true, name: Check
        if iscsid service is deployed, register: iscsid_enabled, tags: common}
    - command: systemctl is-active --quiet iscsid
      name: 'PreUpgrade step0,validation: Check if iscsid is running'
      tags: validation
      when: [step|int == 0, iscsid_enabled.rc == 0]
    - name: Stop and disable iscsid service
      service: name=iscsid state=stopped enabled=no
      when: [step|int == 2, iscsid_enabled.rc == 0]
    - {command: systemctl is-enabled --quiet iscsid.socket, ignore_errors: true, name: Check
        if iscsid.socket service is deployed, register: iscsid_socket_enabled, tags: common}
    - command: systemctl is-active --quiet iscsid.socket
      name: 'PreUpgrade step0,validation: Check if iscsid.socket is running'
      tags: validation
      when: [step|int == 0, iscsid_socket_enabled.rc == 0]
    - name: Stop and disable iscsid.socket service
      service: name=iscsid.socket state=stopped enabled=no
      when: [step|int == 2, iscsid_socket_enabled.rc == 0]
    - {command: systemctl is-enabled --quiet openstack-nova-compute, ignore_errors: true,
      name: Check if nova_compute is deployed, register: nova_compute_enabled, tags: common}
    - {ini_file: dest=/etc/nova/nova.conf section=upgrade_levels option=compute value=,
      name: Set compute upgrade level to auto, when: step|int == 1}
    - command: systemctl is-active --quiet openstack-nova-compute
      name: 'PreUpgrade step0,validation: Check service openstack-nova-compute is
        running'
      tags: validation
      when: [step|int == 0, nova_compute_enabled.rc == 0]
    - name: Stop and disable nova-compute service
      service: name=openstack-nova-compute state=stopped enabled=no
      when: [step|int == 2, nova_compute_enabled.rc == 0]
    - name: Set fact for removal of openstack-nova-compute package
      set_fact: {remove_nova_compute_package: false}
      when: step|int == 2
    - ignore_errors: true
      name: Remove openstack-nova-compute package if operator requests it
      when: [step|int == 2, remove_nova_compute_package|bool]
      yum: name=openstack-nova-compute state=removed
    - {command: systemctl is-enabled --quiet libvirtd, ignore_errors: true, name: Check
        if nova_libvirt is deployed, register: nova_libvirt_enabled, tags: common}
    - command: systemctl is-active --quiet libvirtd
      name: 'PreUpgrade step0,validation: Check service libvirtd is running'
      tags: validation
      when: [step|int == 0, nova_libvirt_enabled.rc == 0]
    - name: Stop and disable libvirtd service
      service: name=libvirtd state=stopped enabled=no
      when: [step|int == 2, nova_libvirt_enabled.rc == 0]
    - {name: Stop snmp service, service: name=snmpd state=stopped, when: step|int
        == 1}
    - args: {creates: /etc/sysconfig/ip6tables.n-o-upgrade}
      name: blank ipv6 rule before activating ipv6 firewall.
      shell: cat /etc/sysconfig/ip6tables > /etc/sysconfig/ip6tables.n-o-upgrade;
        cat</dev/null>/etc/sysconfig/ip6tables
      when: step|int == 3
    - {name: Check yum for rpm-python present, register: rpm_python_check, when: step|int
        == 0, yum: name=rpm-python state=present}
    - fail: msg="rpm-python package was not present before this run! Check environment
        before re-running"
      name: Fail when rpm-python wasn't present
      when: [step|int == 0, rpm_python_check.changed != false]
    - {name: Check for os-net-config upgrade, register: os_net_config_need_upgrade,
      shell: 'yum check-upgrade | awk ''/os-net-config/{print}''', when: step|int
        == 3}
    - {ignore_errors: true, name: Check that os-net-config has configuration, register: os_net_config_has_config,
      shell: test -s /etc/os-net-config/config.json, when: step|int == 3}
    - block:
      - {name: Upgrade os-net-config, yum: name=os-net-config state=latest}
      - {changed_when: os_net_config_upgrade.rc == 2, command: os-net-config --no-activate
          -c /etc/os-net-config/config.json -v --detailed-exit-codes, failed_when: 'os_net_config_upgrade.rc
          not in [0,2]', name: take new os-net-config parameters into account now,
        register: os_net_config_upgrade}
      when: [step|int == 3, os_net_config_need_upgrade.stdout, os_net_config_has_config.rc
          == 0]
    - {name: Update all packages, when: step|int == 3, yum: name=* state=latest}
    role_data_workflow_tasks:
      step2:
      - input:
          ansible_env_variables: {}
          ansible_playbook_verbosity: 1
          ansible_skip_tags: package-install,with_pkg
          ceph_ansible_extra_vars: {ceph_osd_docker_cpu_limit: 1, ceph_osd_docker_memory_limit: 3g,
            osd_max_backfills: 1, osd_objectstore: filestore, osd_recovery_max_active: 3,
            osd_recovery_op_priority: 3}
          ceph_ansible_playbook: [/usr/share/ceph-ansible/site-docker.yml.sample]
          node_data_lookup: '{}'
        name: ceph_base_ansible_workflow
        workflow: tripleo.storage.v1.ceph-install
    role_name: ComputeHCI
overcloud:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {cephmetrics_vip: 172.17.5.4, ctlplane_vip: 192.168.120.10, external_vip: 172.21.1.153,
    internal_api_vip: 172.17.1.13, redis_vip: 172.17.1.16, storage_mgmt_vip: 172.17.4.17,
    storage_vip: 172.17.3.12}
nova_placement:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
kernel:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
neutron_metadata:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
pacemaker:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
redis:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
mysql:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
cinder_api:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
ceph_client:
  children:
    ComputeHCI: {}
  vars: {ansible_ssh_user: heat-admin}
ceph_mon:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
mongodb_disabled:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
swift_ringbuilder:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
neutron_dhcp:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
heat_api:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
timezone:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
heat_api_cloudwatch_disabled:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
tripleo_firewall:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
clustercheck:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
snmp:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
iscsid:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_conductor:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
heat_engine:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_consoleauth:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
glance_api:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
keystone:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
cinder_volume:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
ceilometer_collector_disabled:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
memcached:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
haproxy:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_vnc_proxy:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
neutron_plugin_ml2:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_api:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_metadata:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
mysql_client:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
ntp:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
ceilometer_expirer_disabled:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
ceilometer_api_disabled:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_migration_target:
  children:
    ComputeHCI: {}
  vars: {ansible_ssh_user: heat-admin}
cinder_scheduler:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
glance_registry_disabled:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_scheduler:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_compute:
  children:
    ComputeHCI: {}
  vars: {ansible_ssh_user: heat-admin}
ceph_osd:
  children:
    ComputeHCI: {}
  vars: {ansible_ssh_user: heat-admin}
neutron_ovs_agent:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
swift_proxy:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
sshd:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
neutron_l3:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
nova_libvirt:
  children:
    ComputeHCI: {}
  vars: {ansible_ssh_user: heat-admin}
rabbitmq:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
tuned:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
horizon:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
neutron_api:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
ca_certs:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
heat_api_cfn:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
docker:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
ceph_mgr:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
swift_storage:
  children:
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
logrotate_crond:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
tripleo_packages:
  children:
    ComputeHCI: {}
    Controller: {}
  vars: {ansible_ssh_user: heat-admin}
_meta:
  hostvars: {}
